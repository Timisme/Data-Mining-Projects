# è³‡æ–™æ¢å‹˜ Mid term 

###### tags: `è³‡æ–™æ¢å‹˜`

## Association Rule 

### FP Growth

:::info
Frequent Pattern Growth è¦å‰‡

Let a be a frequent itemset in DB, B be a's conditional
pattern base, and b be an itemset in B. Then a è¯é›† b is a
frequent itemset in DB iff b is frequent in B.
:::

#### why FP growth the winner?

1. Divide and Conquer (æ ¹æ“šç›®å‰å·²çŸ¥freq itemsetsç´°åˆ†å¾Œæ‰¾å‡ºæ‰€æœ‰å­é›†freq ) 
2. ä¸ç”¨æ‰¾candidate?
3. Compressed database?
4. ä¸ç”¨é‡è¤‡æƒææ•´å€‹database
5. æ‰¾å‡ºlocal freq itemsï¼Œå»ºç«‹sub fp treeï¼Œæ²’æœ‰pattern search and matching(ç¬¬äºŒæ¬¡æƒDBæ™‚å·²ç¶“å°‡å®Œæ•´treeå»ºç«‹å®Œæˆ)

#### ä»¥ä¸‹é‡é»(why?)

:::info
1. ç‚ºç”šéº¼è¦sort 1-itemset (by support)?
2. descent orderæ–¹å¼å»ºç«‹fp growth?
    * ç•¶itemsçš„countç›¸åŒï¼Œå¦‚ä½•æ’åº?   
:::

## Multi-level Association Rule 

:::info
high level çš„å•é¡Œ
ç›¸åŒçš„supportå€¼æœƒç”¢ç”Ÿå¾ˆå¤šçš„frequent itemsets(ç”¢ç”Ÿå¾ˆå¤šæ²’æœ‰å¾ˆé‡è¦çš„é—œè¯)
æ„ˆé«˜levelçš„itemæ„ˆå®¹æ˜“æ»¿è¶³min support?

uniform support æœƒé‡åˆ°å…©å€‹å•é¡Œ
1. è¨­å¤ªé«˜ -> åªæœ‰high levelæœƒç•™ä¸‹
2. è¨­å¤ªä½ -> å¤ªå¤šfreq itemsets

Reduced Support : 4 strategies 
:::

    A set A is a superset of another set B if all elements of the set B are elements of the set A.

![](https://i.imgur.com/ZqQ5nyC.png =50%x)


:::info

Max-patterns
freq patterns without frequent super patternã€‚
å¦‚BCDE is max-patternï¼Œbut BCD not(even frequent as well)

Closed frequent itemsets
An itemset is closed in a data set if there exists no superset that has the same support count as this original itemset.(è¼ƒå¯¬é¬†ï¼Œå³ä¾¿supersetæœ‰è¶…émin supportä½†ä¸åŠoriginal setï¼Œå°±æ˜¯closed)
:::

==max patterns å’Œ closed frequent itemsetå·®åœ¨å“ª?==
    
    Frequent item set ğ‘‹âˆˆğ¹ is maximal if it does not have any frequent supersets.
    Frequent item set X âˆˆ F is closed if it has no superset with the same frequency

![](https://i.imgur.com/BxXRA1F.png =50%x)

==ç”¨closed freq itemsetæ‰¾å‡ºçš„ruleæ›´æœ‰ä»£è¡¨æ€§ã€‚==
![](https://i.imgur.com/bAhDPsR.png =50%x)

---

## Quantitative é—œè¯æ³•å‰‡

![](https://i.imgur.com/eAeIoG6.png =50%x)

:::info
å•é¡Œ:

ç•¶attributeè¢«åˆ‡çš„å¾ˆå¤šï¼Œè³‡æ–™æœ¬èº«å„itemçš„ support valueå¾ˆä½ï¼Œconfidenceå¾ˆå®¹æ˜“å°±å¾ˆé«˜(attributeçš„ support valueä½) 
:::

----

# Text Analysis 

:::info
Inverted index:
çµ¦å®šæ–‡å­—ï¼Œè¼¸å‡ºoutputç‚ºæ–‡ç« idåŠåœ¨æ–‡ç« å…§ä½ç½®
:::

### Lexical processing 

1. tokenization
2. stemming (è¤‡æ•¸ å­—æ ¹ å»é™¤ç­‰ã€‚)
3. removing stop words é™ä½size reduction 

:::info
TF-IDF 
IDFj = log(total documents in the set / docus which contain the term W)
:::

![](https://i.imgur.com/rYKVK1t.png)

### BM25

    TF-IDF çš„è¤‡é›œç‰ˆã€‚ç®—å…©å€‹å‘é‡çš„SCORE

#### LSA & LSI example 

    svd -> ç„¡æ³•é‹ç®—å¤§é‡æ–‡æœ¬

#### word embedding 

:::info
å•é¡Œ:
é‡åˆ°æ²’çœ‹éçš„å­—è©(out of bag)ï¼Œæ²’æœ‰åˆ†è¾¨åŠé æ¸¬åŠ›
:::
    
#### information extraction 

:::info
workflow 
1. æ–·å­—å’Œè¾­æ„åˆ†ç³»(lexical analysis)
2. paper name idenfication 
3. shallow parsing? (syntactic analysis)
4. building relations 
5. inferencing? 
:::

![](https://i.imgur.com/suDMHRW.png)


---

# Sequence Pattern

![](https://i.imgur.com/KWIqB9m.png =50%x)

    elementæ˜¯æ™‚é–“tçš„å¤§å–®ä½ï¼Œä¸€å€‹elementç´°åˆ†ç‚ºå¤šå€‹items

## Subsequence 

![](https://i.imgur.com/fQtvM5M.png =50%x)

## Sequential pattern mining ç›®æ¨™ç‚º?
    
    çµ¦å®šä¸€çµ„åºåˆ—ï¼Œæ‰¾å‡ºæ‰€æœ‰å…¶frequent subsequences

![](https://i.imgur.com/G4MJ3ZC.png =50%x)

## Challenge 

    1. è¨ˆç®—é‡å¤§ 2. many scan of databases 3. é•·åºåˆ—æº–åº¦å•é¡Œ

![](https://i.imgur.com/UAuDGeX.png =50%x)

## algorithm 

    ç‰¹æ®Šæƒ…æ³ å°‡items åšmappingæ™‚å°‡åŒå€‹elementä¸­å¤§æ–¼å…©å€‹freq item
    çš†åšçµ„åˆ
    
![](https://i.imgur.com/KuHFv1t.png)
 
:::info
æ³¨æ„:

(3)(5)æ˜¯å…©å€‹ä¸åŒæ™‚é–“çš„patternï¼Œä¸æ˜¯(3 5)çš„å­é›†
:::
![](https://i.imgur.com/OB8bbof.png)

### FreeSpan 

é‹ç”¨æ¦‚å¿µ ==pattern projected== 

:::info
1. å°‡å„åºåˆ—ä¾ç…§itemåˆ†åˆ¥æ˜ å°„(project)åˆ°æ›´å°çš„projected database
2. æ ¹æ“šprojected databaseç¹¼çºŒå¾€ä¸‹é•·subsequence
3. divide and conquerä½œæ³•
4. å¯ä»¥å°‡å®Œæ•´çš„åºåˆ—è³‡æ–™åˆ†æˆå„ç¨®subsetã€‚
:::

![](https://i.imgur.com/ThL7Qoy.png =50%x)

![](https://i.imgur.com/2QWkJue.png =50%x)

![](https://i.imgur.com/INaUSfE.png =50%x)

### ==Prefix Span== 

:::info
å„ªå‹¢:
1. no candidate subsets to be generated 
2. projected DBs keep shrinking 
:::

![](https://i.imgur.com/ARQCuXn.png =50%x)

    æ¯æ¬¡é‡å°itemå»ºç«‹projected DB æ™‚å¯ä»¥æ‰¾åˆ°subset

![](https://i.imgur.com/JoQFynH.png =50%x)
![](https://i.imgur.com/B2lHdpq.png =50%x)

    prefix span ç²¾ç¥:
    å…ˆç”¨prefixåˆ†åˆ¥æ‰¾projection db -> divide and conquer
    å¾dbæ‰¾Sequential pattern -> å’Œprefix çµ„åˆä¹Ÿæ˜¯sp
    
    å…ˆæŠŠç­”æ¡ˆæ•´ç†å¥½ï¼Œä¸€å€‹å€‹å¾€ä¸‹åšï¼Œå’Œå…¶ä»–æ¢ç¨ç«‹ï¼Œå¾ˆå¿«æ”¶æ–‚ï¼Œé€Ÿåº¦å¿«ã€‚

-----

# Machine Learning 

## æ±ºç­–æ¨¹

:::info

1. hunt's algo : éš¨æ©Ÿé¸æ“‡featureå»åˆ†é¡ ---> overfitting
2. Greedy Strategy 
    split the records based on an attribute test that optimizes certain criterion
    å°±æ˜¯æ‰¾åˆ°ä¸€å€‹æœ€ä½³çš„attributeå¯ä»¥ä½¿å¾—ç›®æ¨™è¢«æœ€å¤§æ»¿è¶³(min | max)
    (åœ¨é€™æ™‚é–“é»æœ€å¥½çš„è§£)
==nodes with homogeneous class distribution are preferred==
åˆ©ç”¨node impurityè¨ˆç®—ä¸ç´”åº¦
:::

![](https://i.imgur.com/YtKDALG.png =50%x)

### å¸¸ç”¨è¨ˆç®—node impurityç®—æ³•
![](https://i.imgur.com/0MuNpRR.png =50%x)
![](https://i.imgur.com/P7ADzWW.png =50%x)

    æ±ºç­–æ¨¹è¨“ç·´çš„ç›®æ¨™å‡½æ•¸ç‚º
    information gain = parent node entropy - weighted sum entropy(é¸æ“‡èƒ½å°‡info gainæœ€å¤§åŒ–çš„featureå»split)

:::info
gini å’Œ entropyè¨ˆç®—æ–¹å¼çš†prefer splits that result in large num of partitions, each being small but pureã€‚
:::

:::info
leaf node (stop) criterion 
1. ç•¶åŠƒåˆ†å¾Œæ¯ç­†è³‡æ–™éƒ½æ˜¯åŒå€‹é¡åˆ¥
2. ç•¶åŠƒåˆ†å¾Œæ¯ç­†è³‡æ–™éƒ½æœ‰ç›¸åŒçš„features
3. early stopping -> reduce overfitting
::: 

    å„ªé»ï¼š
    1. è¨ˆç®—å¿«é€Ÿ
    2. å¯ä»¥å¾ˆç°¡å–®çš„è§£é‡‹data
    3. è¡¨ç¾å’Œå…¶ä»–åˆ†é¡æ¨¡å‹ä¸æœƒå·®å¾ˆå¤š
    4. å°æ–¼symbolic featureè¡¨ç¾ç‰¹å¥½ã€‚

    å•é¡Œ:
    1. æœ‰ç¼ºå€¼å°treeçš„è¨“ç·´å½±éŸ¿å¾ˆå¤§ã€‚
    2. nodesæ¬¡æ•¸è¶Šå¤šï¼Œæ„ˆå®¹æ˜“overfittingã€‚
    3. å¦‚æœfeatureäº¤äº’ä½œç”¨æ‰å°çµæœæœ‰å½±éŸ¿ï¼Œæ±ºç­–æ¨¹æ²’è¾¦æ³•åˆ†é¡ã€‚
    4. ä»£è¡¨DTåƒ…èƒ½æ‰¾å‡ºå–®ä¸€featureå°çµæœçš„å½±éŸ¿ã€‚
    5. å°noise å¾ˆsensitive
    
    è§£æ±ºoverfitting
    1. pre-pruning
        ç”¨æ›´åš´è¬¹çš„æ–¹å¼è¨­å®šåœæé»
    2. ...
    
## KNN

![](https://i.imgur.com/5Iz9Fkb.png =50%x)

:::info
Kå€¼é¸å–tricks

1. å¦‚æœk å¤ªå°ï¼Œå‰‡å¾ˆæœ‰å¯èƒ½æœƒå› ç‚ºé„°è¿‘ç‚ºnoise dataç”¢ç”ŸéŒ¯èª¤åˆ†é¡
2. kå¤ªå¤§ä¹Ÿå¯èƒ½å› ç‚ºé¸åˆ°è·é›¢å¤ªé çš„feature(èˆ‡è‡ªå·±å¤ªä¸åƒäº†é‚„è¦é¸)
:::

## è²æ°

:::info
ç›´æ¥å‡è¨­å„featureä¹‹é–“æ¢ä»¶ç¨ç«‹ã€‚
:::
![](https://i.imgur.com/BgyPrfr.png =50%x)

![](https://i.imgur.com/ONNwOVA.png =50%x)

:::info
å„ªé»:
1. robust to noise
2. èƒ½è™•ç†missing value(è¨ˆç®—æ™‚å¾Œç•¶ä½œ1e-6ç­‰ï¼Ÿ)
:::
    