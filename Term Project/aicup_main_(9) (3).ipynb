{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "aicup_main (9).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk66K6AWaW1q"
      },
      "source": [
        "# !cp drive/MyDrive/python檔/aicup/run/dataset.py .\r\n",
        "# !cp drive/MyDrive/python檔/aicup/run "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsnrmjRWrSmN"
      },
      "source": [
        "# loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GHLkq1bcEox",
        "outputId": "d1f5f7a9-d194-4a50-b7ec-b350667dcc86"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59rSMkbocYfI"
      },
      "source": [
        "import sys\r\n",
        "sys.path.insert(0,\"/content/drive/My Drive/python檔/aicup/run\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3D3ktlwcq1p",
        "outputId": "53124bcc-05b0-44a8-a937-81402147ee8d"
      },
      "source": [
        "pip install transformers==3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.1.94)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3) (20.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6RsrpxBWe1H",
        "outputId": "d934aa40-0b98-4674-92e9-c12db9b0ad1c"
      },
      "source": [
        "pip install pytorch-crf"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.6/dist-packages (0.7.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6eL0_ORc3lj",
        "outputId": "4d39b747-4816-4ada-ec77-6949e7c91d20"
      },
      "source": [
        "pip install pytorch_warmup"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_warmup in /usr/local/lib/python3.6/dist-packages (0.0.4)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_warmup) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNw4yCiwa1e2"
      },
      "source": [
        "# from dataset import bert_stc_dataset\r\n",
        "# from model2 import model_crf\r\n",
        "from train import train\r\n",
        "# from txt_preprocess2 import preprocess2\r\n",
        "import re\r\n",
        "import time\r\n",
        "\r\n",
        "from transformers import BertModel, BertTokenizer, get_cosine_schedule_with_warmup\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchsummary import summary\r\n",
        "from torchcrf import CRF\r\n",
        "import pytorch_warmup as warmup\r\n",
        "# from torch.autograd import Variable\r\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Qnou0tbJfb",
        "outputId": "870eb7b2-b2d7-45cf-d405-30571d950ff3"
      },
      "source": [
        "file_path = '/content/drive/My Drive/python檔/aicup/run/data/train2_input.data'\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "print('{} is being used'.format(device))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda is being used\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6sIQ8wbXGnc"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo2E2d2TU7dG"
      },
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "import torch\r\n",
        "\r\n",
        "class bert_stc_dataset(Dataset):\r\n",
        "    \r\n",
        "    def __init__(self, stcs, labels, tokenizer, max_length):\r\n",
        "        \r\n",
        "        self.stcs = stcs\r\n",
        "        self.labels = labels\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.max_length = max_length\r\n",
        "        self.pad_labels = []\r\n",
        "\r\n",
        "        # 已經在preprocess2 做完label 0 padding\r\n",
        "        for i in range(len(labels)):\r\n",
        "            temp_label = [0]*max_length\r\n",
        "            temp_label[:len(labels[i])] = labels[i]\r\n",
        "            self.pad_labels.append(temp_label)\r\n",
        "            \r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.stcs)\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        \r\n",
        "        txt = str(self.stcs[idx])\r\n",
        "        \r\n",
        "        txt = ' '.join(list(txt)) #中間要有空格，數字才會分類準確\r\n",
        "        # print(txt)\r\n",
        "        \r\n",
        "        encoding = self.tokenizer.encode_plus(\r\n",
        "            txt,\r\n",
        "#             truncation= True,\r\n",
        "            max_length= self.max_length,\r\n",
        "            padding = 'max_length',\r\n",
        "            add_special_tokens=False,\r\n",
        "#             pad_to_multiple_of=True,\r\n",
        "            return_attention_mask= True,\r\n",
        "            return_token_type_ids= False,\r\n",
        "            return_tensors='pt')\r\n",
        "        \r\n",
        "        return {\r\n",
        "            'input_ids': encoding['input_ids'].flatten(),\r\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\r\n",
        "            'labels' : torch.tensor(self.pad_labels[idx], dtype= torch.long)\r\n",
        "        }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfh42ir6I9FM"
      },
      "source": [
        "# preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr4UxdsTI8Y-"
      },
      "source": [
        "class preprocess2():\r\n",
        "  def __init__(self, data):\r\n",
        "    self.data = data\r\n",
        "    self.data_list= list()\r\n",
        "    data_list_tmp = list()\r\n",
        "\r\n",
        "    for row in data:\r\n",
        "      data_tuple = tuple()\r\n",
        "      if row == '\\n':\r\n",
        "        self.data_list.append(data_list_tmp)\r\n",
        "        data_list_tmp = []\r\n",
        "\r\n",
        "      else:\r\n",
        "        row = row.strip('\\n').split(' ')\r\n",
        "\r\n",
        "        if (row[0] == '，') & (len(data_list_tmp) >= 40):\r\n",
        "          self.data_list.append(data_list_tmp)\r\n",
        "          data_list_tmp= []\r\n",
        "\r\n",
        "        elif row[0] in ['。', '？','！','～','：']:\r\n",
        "          self.data_list.append(data_list_tmp)\r\n",
        "          data_list_tmp= []\r\n",
        "        \r\n",
        "        elif row[0] in ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']:\r\n",
        "          data_tuple = (row[0].lower(), row[1])\r\n",
        "          data_list_tmp.append(data_tuple)\r\n",
        "\r\n",
        "        elif row[0] not in ['摁','嗯','啦','喔','欸','啊','齁','嘿','嘛','…','...']:\r\n",
        "          data_tuple = (row[0], row[1])\r\n",
        "          data_list_tmp.append(data_tuple)\r\n",
        "        #data_list_tmp 儲存暫時的data_tuple(token,label)\r\n",
        "    if len(data_list_tmp) != 0:\r\n",
        "      self.data_list.append(data_list_tmp)\r\n",
        "\r\n",
        "  def get_stc_label(self):\r\n",
        "    all_stcs = list()\r\n",
        "    all_labels = list()\r\n",
        "\r\n",
        "    for article_txt_tuple in self.data_list:\r\n",
        "\r\n",
        "      txt_len = len(article_txt_tuple) #(文章數，每個文章對應的總字數) (word, label)\r\n",
        "      stc = str() #存字數= max_stc_len的字串\r\n",
        "      labels = []\r\n",
        "\r\n",
        "      for idx, (word, label) in enumerate(article_txt_tuple):\r\n",
        "\r\n",
        "        stc += word\r\n",
        "        labels.append(label)\r\n",
        "\r\n",
        "      if stc not in all_stcs: #確定沒有重複的句子\r\n",
        "        all_stcs.append(stc)\r\n",
        "        all_labels.append(labels)\r\n",
        "\r\n",
        "    all_stcs_clean = []\r\n",
        "    all_labels_clean = []\r\n",
        "\r\n",
        "    idx = 0\r\n",
        "    for stc, label in zip(all_stcs,all_labels): #前處理 & downsampling\r\n",
        "      \r\n",
        "      stc_clean = re.sub(r'(醫師)|(個管師)|(民眾)|(家屬)|(護理師)', '', stc)\r\n",
        "      # print(stc, stc_clean, label)\r\n",
        "      if (len(stc_clean)> 1) & (len(set(label))> 2):  \r\n",
        "        # print(stc_clean, stc)\r\n",
        "        all_stcs_clean.append(stc)\r\n",
        "        all_labels_clean.append(label)\r\n",
        "    \r\n",
        "      elif len(stc_clean) > 1 : # & (((idx+1) % 2) == 0)\r\n",
        "        all_stcs_clean.append(stc)\r\n",
        "        all_labels_clean.append(label)\r\n",
        "    #   idx += 1\r\n",
        "\r\n",
        "    return all_stcs_clean, all_labels_clean\r\n",
        "\r\n",
        "  def tag2id(self, stcs_label):\r\n",
        "\r\n",
        "    all_label = list()\r\n",
        "    for stc_label in stcs_label:\r\n",
        "      for label in stc_label:\r\n",
        "        all_label.append(label)\r\n",
        "\r\n",
        "    labels_set = sorted(set(all_label))\r\n",
        "    tag2id_dict = {}\r\n",
        "\r\n",
        "    for idx, label in enumerate(labels_set):\r\n",
        "      tag2id_dict[label] = idx\r\n",
        "\r\n",
        "    return tag2id_dict\r\n",
        "\r\n",
        "  def label_to_ids(self, tag_to_id, raw_labels):\r\n",
        "\r\n",
        "    label2id = []\r\n",
        "    for stc_labels in raw_labels:\r\n",
        "      stc_label_ids = [tag_to_id[label] for label in stc_labels]\r\n",
        "      label2id.append(stc_label_ids)\r\n",
        "    return label2id\r\n",
        "\r\n",
        "  def get_stcs_label2ids(self):\r\n",
        "\r\n",
        "    stcs, labels = self.get_stc_label()\r\n",
        "    tag2id = self.tag2id(stcs_label= labels)\r\n",
        "    labels_ids= self.label_to_ids(tag_to_id= tag2id, raw_labels= labels)\r\n",
        "\r\n",
        "    return stcs, labels_ids"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpu1QT-5tYsi"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48kSvtOFtQRg"
      },
      "source": [
        "class model_crf(nn.Module):\r\n",
        "\tdef __init__(self, n_tags, hidden_dim=768, batchsize= 32, num_layers= 1, lstm_dropout= 0, fc_dropout= 0.2):\r\n",
        "\t\tsuper(model_crf, self).__init__()\r\n",
        "\t\tself.num_layers = num_layers\r\n",
        "\t\tself.n_tags = n_tags\r\n",
        "\t\tself.lstm =  nn.LSTM(bidirectional=True, num_layers=num_layers, input_size=768, hidden_size=hidden_dim//2, dropout= lstm_dropout, batch_first=True)\t\t\r\n",
        "\t\tself.hidden_dim = hidden_dim\r\n",
        "\t\tself.fc = nn.Linear(hidden_dim, self.n_tags)\r\n",
        "\t\tself.bert = BertModel.from_pretrained('bert-base-chinese')\r\n",
        "\r\n",
        "\t\t# for param in self.bert.parameters():\r\n",
        "\t\t# \tparam.requires_grad = False\r\n",
        "\t\t# self.bert.eval()  # 知用来取bert embedding\r\n",
        "\r\n",
        "\t\tself.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "\t\tself.CRF = CRF(n_tags, batch_first= True)\r\n",
        "\t\tself.dropout = nn.Dropout(p= fc_dropout)\r\n",
        "\t\tself.hidden = self.init_hidden(batchsize)\r\n",
        "\r\n",
        "\tdef init_hidden(self, batch_size):\r\n",
        "\t\treturn (torch.randn(2*self.num_layers, batch_size, self.hidden_dim // 2).to(self.device),\r\n",
        "\t\t\t\ttorch.randn(2*self.num_layers, batch_size, self.hidden_dim // 2).to(self.device))\r\n",
        "\r\n",
        "\tdef forward(self, input_ids, attention_mask, tags):\r\n",
        "\r\n",
        "\t\tbatch_size = input_ids.size(0)\r\n",
        "\t\tmax_seq_len = input_ids.size(1)\r\n",
        "\t\tbert_output, _  = self.bert(input_ids.long(), attention_mask)\r\n",
        "\t\tseq_len = torch.sum(attention_mask, dim= 1).cpu().int()\r\n",
        "\t\t# print(seq_len)\r\n",
        "\t\tpack_input = pack_padded_sequence(input= bert_output, lengths= seq_len, batch_first= True, enforce_sorted= False)\r\n",
        "\t\tpacked_lstm_out, _ = self.lstm(pack_input,self.init_hidden(batch_size= batch_size))\r\n",
        "\t\tlstm_enc, _=  pad_packed_sequence(packed_lstm_out, batch_first=True)\r\n",
        "\t\t# print(lstm_enc.size())\r\n",
        "\t\tlstm_enc = self.dropout(lstm_enc)\r\n",
        "\t\tlstm_feats = self.fc(lstm_enc)\r\n",
        "\r\n",
        "\t\tlstm_max_seq_len = lstm_feats.size(1)\r\n",
        "\t\tpad = torch.zeros(size=(batch_size, max_seq_len-lstm_max_seq_len, self.n_tags), dtype= torch.float).to(self.device)\r\n",
        "\t\tlstm_feats= torch.cat((lstm_feats, pad), dim= 1)\r\n",
        "  \r\n",
        "\t\t# lstm_feats[:,:,:4] = lstm_feats[:,:,:5]*100\r\n",
        "\t\t# lstm_feats[:,:,5:9] = lstm_feats[:,:,5:9]*10\r\n",
        "\t\t# lstm_feats[:,:,9:11] = lstm_feats[:,:,9:11]*100\r\n",
        "\t\t# lstm_feats[:,:,11] = lstm_feats[:,:,11]*100\r\n",
        "\t\t# lstm_feats[:,:,12:17] = lstm_feats[:,:,12:17]*100\r\n",
        "\t\t# lstm_feats[:,:,17:21] = lstm_feats[:,:,17:21]*10\r\n",
        "\t\t# lstm_feats[:,:,21:23] = lstm_feats[:,:,21:23]*100\r\n",
        "\t\t# lstm_feats[:,:,23] = lstm_feats[:,:,23]*1\r\n",
        "\r\n",
        "\t\tlstm_feats[:,:,:23] = lstm_feats[:,:,:23]*100\r\n",
        "\t\tlstm_feats[:,:,23] = lstm_feats[:,:,23]*10\r\n",
        "\r\n",
        "\t\tloss = -self.CRF(lstm_feats, tags, attention_mask.bool(), reduction= 'token_mean')\r\n",
        "\t\tpred_seqs = self.CRF.decode(emissions= lstm_feats, mask= attention_mask.bool())\r\n",
        "  \r\n",
        "\t\treturn loss, pred_seqs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYEi_mFctfCR"
      },
      "source": [
        "# 載入stcs, tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWEzS06NtqJZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "outputId": "9cdb4db0-3ce0-4f65-8dad-dba005f37714"
      },
      "source": [
        "# ---------------前處理---------------\r\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\r\n",
        "\tdata=f.readlines()#.encode('utf-8').decode('utf-8-sig')\r\n",
        "\r\n",
        "preprocessor = preprocess2(data)\r\n",
        "\r\n",
        "stcs, original_labels= preprocessor.get_stc_label()\r\n",
        "stcs, labels = preprocessor.get_stcs_label2ids()\r\n",
        "tag2id_dict = preprocessor.tag2id(original_labels)\r\n",
        "n_tags = len(tag2id_dict)\r\n",
        "print(tag2id_dict)\r\n",
        "print('tags數: {}'.format(n_tags))\r\n",
        "\r\n",
        "gt_tags = [tag for label in labels for tag in label]\r\n",
        "\r\n",
        "for tag in set(gt_tags):\r\n",
        "  print('{}|{}'.format(tag, gt_tags.count(tag)/len(gt_tags)))\r\n",
        "# plt.hist(gt_tags)\r\n",
        "plt.hist([len(stc) for stc in stcs])\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\r\n",
        "max_length = len(max(stcs, key=len)) \r\n",
        "print('max_length', max_length)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-ID': 0, 'B-clinical_event': 1, 'B-contact': 2, 'B-education': 3, 'B-family': 4, 'B-location': 5, 'B-med_exam': 6, 'B-money': 7, 'B-name': 8, 'B-organization': 9, 'B-profession': 10, 'B-time': 11, 'I-ID': 12, 'I-clinical_event': 13, 'I-contact': 14, 'I-education': 15, 'I-family': 16, 'I-location': 17, 'I-med_exam': 18, 'I-money': 19, 'I-name': 20, 'I-organization': 21, 'I-profession': 22, 'I-time': 23, 'O': 24}\n",
            "tags數: 25\n",
            "0|4.500272829040261e-05\n",
            "1|2.8126705181501627e-05\n",
            "2|0.00010125613865340586\n",
            "3|1.6876023108900978e-05\n",
            "4|0.00013500818487120783\n",
            "5|0.0008944292247717517\n",
            "6|0.00120944832280457\n",
            "7|0.0004387766008314254\n",
            "8|0.0009394319530621544\n",
            "9|5.625341036300326e-06\n",
            "10|7.312943347190423e-05\n",
            "11|0.00795423222532866\n",
            "12|8.438011554450489e-05\n",
            "13|8.438011554450489e-05\n",
            "14|0.0004162752366862241\n",
            "15|1.6876023108900978e-05\n",
            "16|0.0001518842079801088\n",
            "17|0.0013388311666394776\n",
            "18|0.0021263789117215233\n",
            "19|0.0010913161610422632\n",
            "20|0.0015750954901640913\n",
            "21|1.1250682072600652e-05\n",
            "22|0.00022501364145201302\n",
            "23|0.01743293187149471\n",
            "24|0.9636040434951368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "max_length 74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F72vtVN9tbhD"
      },
      "source": [
        "# 參數設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTvRlmOldMGf",
        "outputId": "4cc59dd3-6fc9-4fe7-a78e-52b459244e99"
      },
      "source": [
        "# --------------- 現在model只有單層 ---------------\r\n",
        "batchsize= 64\r\n",
        "hidden_dim= 256\r\n",
        "num_epochs= 30\r\n",
        "lr = 1e-4\r\n",
        "weight_decay = 1e-4\r\n",
        "warmup_ratio = 0.4\r\n",
        "lstm_dropout = 0\r\n",
        "fc_dropout = 0.5\r\n",
        "num_layers = 1\r\n",
        "\r\n",
        "model = model_crf(n_tags= n_tags, hidden_dim= hidden_dim, batchsize= batchsize, num_layers= num_layers, lstm_dropout= lstm_dropout, fc_dropout= fc_dropout).to(device)\r\n",
        "# print(summary(model,[(128, 300), (128,300)]))\r\n",
        "\r\n",
        "train_x, test_x, train_y, test_y = train_test_split(stcs, labels, test_size= 0.2, shuffle= True, random_state= 42)\r\n",
        "print('test gt tags unique數 :{}'.format(len(set([tag for label in test_y for tag in label]))))\r\n",
        "\r\n",
        "train_dataset = bert_stc_dataset(stcs= train_x, labels= train_y, tokenizer= tokenizer, max_length= max_length)\r\n",
        "print(train_x[0:3])\r\n",
        "print(train_dataset[0:3]['input_ids'])\r\n",
        "print(train_dataset[0:3]['labels'])\r\n",
        "test_dataset = bert_stc_dataset(stcs= test_x, labels= test_y, tokenizer= tokenizer, max_length= max_length)\r\n",
        "\r\n",
        "print('training stcs 總數: {}'.format(len(train_dataset)))\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size= batchsize, shuffle= True, num_workers= 4)\r\n",
        "test_dataloader = DataLoader(test_dataset, batch_size= batchsize, shuffle= False, num_workers= 4)\r\n",
        "\r\n",
        "num_iteration = len(train_dataloader)\r\n",
        "print('num_iteration',num_iteration)\r\n",
        "total_iter = num_iteration * num_epochs\r\n",
        "warmup = total_iter * warmup_ratio\r\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay= weight_decay)\r\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps= warmup, num_training_steps=total_iter)\r\n",
        "# warmup_scheduler = warmup.ExponentialWarmup(optimizer, warmup_period=150)\r\n",
        "\r\n",
        "# ---------------訓練---------------\r\n",
        "# train_model = train(model= model, optimizer= optimizer, train_loader= train_dataloader, test_loader= 0, num_epochs= 5, device= device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test gt tags unique數 :21\n",
            "['本來我，那個藥師查，他也覺得，覺得好像，覺得好像跟這個沒有，沒有關係', '比以前都高', '沒有睡']\n",
            "tensor([ 138,  112, 3315,  889, 2769, 8024, 6929,  943, 5973, 2374, 3389, 8024,\n",
            "         800,  738, 6221, 2533, 8024, 6221, 2533, 1962, 1008, 8024, 6221, 2533,\n",
            "        1962, 1008, 6656, 6857,  943, 3760, 3300, 8024, 3760, 3300, 7302,  913,\n",
            "         112,  117,  112, 3683,  809, 1184, 6963, 7770,  112,  117,  112, 3760,\n",
            "        3300, 4717,  112,  140,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0])\n",
            "tensor([[24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
            "         24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0],\n",
            "        [24, 24, 24, 24, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0],\n",
            "        [24, 24, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0]])\n",
            "training stcs 總數: 9956\n",
            "num_iteration 156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rPf2heKjCd7"
      },
      "source": [
        "# test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfjiko8GjBZD"
      },
      "source": [
        "def test(model, test_dataloader, device):\r\n",
        "\r\n",
        "  preds_epoch = []\r\n",
        "  gts_epoch = []\r\n",
        "  epoch_loss = 0\r\n",
        "  iteration = 0\r\n",
        "\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  for idx, batch_dict in enumerate(test_dataloader):\r\n",
        "\r\n",
        "    # print('idx: ',idx+1)\r\n",
        "    input_ids = batch_dict['input_ids'].to(device)\r\n",
        "    attention_mask = batch_dict['attention_mask'].to(device)\r\n",
        "    labels = batch_dict['labels'].to(device)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "      loss, pred_labels = model(input_ids, attention_mask.bool(), labels)\r\n",
        "\r\n",
        "    # mask gt labels \r\n",
        "    labels = batch_dict['labels'].numpy()\r\n",
        "    masks = batch_dict['attention_mask'].numpy()\r\n",
        "\r\n",
        "    labels_nopad = []\r\n",
        "    for label , seq_mask in zip(labels, masks):\r\n",
        "\r\n",
        "      seq = [tag for tag, mask in zip(label, seq_mask) if mask == 1]\r\n",
        "      labels_nopad.append(seq)\r\n",
        "\r\n",
        "    # one dim array \r\n",
        "    preds= [tag for seq in pred_labels for tag in seq]\r\n",
        "    gts= [tag for seq in labels_nopad for tag in seq]\r\n",
        "\r\n",
        "    preds_epoch += preds\r\n",
        "    gts_epoch += gts\r\n",
        "\r\n",
        "    epoch_loss += loss.item()\r\n",
        "    iteration += 1\r\n",
        "\r\n",
        "  f1_macro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'macro')\r\n",
        "  f1_micro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'micro')\r\n",
        "  f1 = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= None)\r\n",
        "  avg_loss = epoch_loss / iteration\r\n",
        "\r\n",
        "  print('gt tag unique 數: {}'.format(len(set(gts_epoch))))\r\n",
        "  print('pred tag unique 數: {}'.format(len(set(preds_epoch))))\r\n",
        "  print('test_f1(macro, micro) ({:.2f},{:.2f}) | test_avg_loss {:.3f} | f1 for each class\\n{}\\n'.format(f1_macro, f1_micro, avg_loss, f1))\r\n",
        "\r\n",
        "  return f1_macro, f1_micro, avg_loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NSLpK9g7bp5"
      },
      "source": [
        "# test out function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2yAR0_67gGZ"
      },
      "source": [
        "class test_output():\r\n",
        "\tdef __init__(self, data, model, tokenizer, batch_size):\r\n",
        "\r\n",
        "\t\tself.model = model\r\n",
        "\t\tself.tokenizer = tokenizer\r\n",
        "\t\tself.batch_size = batch_size\r\n",
        "\t\tself.data_list = []\r\n",
        "\t\tself.word_id = []\r\n",
        "\t\tself.word_article_id = [] \r\n",
        "\t\tarticle_id = 0\r\n",
        "\t\tword_id = 0\r\n",
        "\t\tdata_list_tmp = []\r\n",
        "\t\tarticle_id_tmp = []\r\n",
        "\t\tword_id_tmp = []\r\n",
        "\t\t\r\n",
        "\t\tfor row in data:\r\n",
        "\t\t\t\r\n",
        "\t\t\tdata_tuple = tuple()\r\n",
        "\t\t\tif row == '\\n':\r\n",
        "\t\t\t\t\r\n",
        "\t\t\t\tarticle_id += 1 \r\n",
        "\t\t\t\tword_id = 0\r\n",
        "\t\t\t\tself.word_id.append(word_id)\r\n",
        "\t\t\t\tself.word_article_id.append(article_id_tmp)\r\n",
        "\t\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\t\t\t\tdata_list_tmp = []\r\n",
        "\t\t\t\tarticle_id_tmp = []\r\n",
        "\t\t\t\tword_id_tmp = []\r\n",
        "\r\n",
        "\t\t\telse:\r\n",
        "\t\t\t\t\r\n",
        "\t\t\t\trow = row.strip('\\n').split(' ')\r\n",
        "\r\n",
        "\t\t\t\tif row[0] in ['。', '？','！','，','～','：']:\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\t\tself.word_id.append(word_id_tmp)\r\n",
        "\t\t\t\t\tself.word_article_id.append(article_id_tmp)\r\n",
        "\t\t\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\t\t\t\t\tdata_list_tmp = []\r\n",
        "\t\t\t\t\tarticle_id_tmp = []\r\n",
        "\t\t\t\t\tword_id_tmp = []\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\telif row[0] in ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']:\r\n",
        "\t\t\t\t\t  \r\n",
        "\t\t\t\t\tdata_tuple = (row[0].lower(), article_id, word_id)\r\n",
        "\t\t\t\t\tdata_list_tmp.append(data_tuple)\r\n",
        "\t\t\t\t\tarticle_id_tmp.append(article_id)\r\n",
        "\t\t\t\t\tword_id_tmp.append(word_id)\r\n",
        "\r\n",
        "\t\t\t\telif row[0] not in ['摁','嗯','啦','喔','欸','啊','齁','嘿','…','...','，']:\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\t\tdata_tuple = (row[0], article_id, word_id)\r\n",
        "\t\t\t\t\tdata_list_tmp.append(data_tuple)\r\n",
        "\t\t\t\t\tarticle_id_tmp.append(article_id)\r\n",
        "\t\t\t\t\tword_id_tmp.append(word_id)\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\tword_id += 1\r\n",
        "\t\t\t\t\r\n",
        "\t\tif len(data_list_tmp) != 0:\r\n",
        "\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\t\t\tself.word_id.append(word_id_tmp)\r\n",
        "\t\t\tself.word_article_id.append(article_id_tmp)\r\n",
        "\t\t\t\r\n",
        "\tdef raw_output(self):\r\n",
        "\t\treturn self.data_list, self.word_id, self.word_article_id\r\n",
        "\r\n",
        "\tdef get_stcs(self):\r\n",
        "\t\t\r\n",
        "\t\tall_stcs = list()\r\n",
        "\t\tall_article_ids = list()\r\n",
        "\t\tall_word_ids = list()\r\n",
        "\r\n",
        "\t\tfor stc_list in self.data_list:\r\n",
        "\r\n",
        "\t\t\ttxt_len = len(stc_list) #(文章數，每個文章對應的總字數) (word, label)\r\n",
        "\t\t\tstc = str() #存字數= max_stc_len的字串\r\n",
        "\t\t\tarticle_ids = []\r\n",
        "\t\t\tword_ids = []\r\n",
        "\t\t\t\r\n",
        "\r\n",
        "\t\t\tfor idx, (word,article_id, word_id) in enumerate(stc_list):\r\n",
        "\r\n",
        "\t\t\t\tstc += word\r\n",
        "\t\t\t\tarticle_ids.append(article_id)\r\n",
        "\t\t\t\tword_ids.append(word_id)\r\n",
        "\r\n",
        "\t\t\tall_stcs.append(stc)\r\n",
        "\t\t\tall_article_ids.append(article_ids)\r\n",
        "\t\t\tall_word_ids.append(word_ids)\r\n",
        "\r\n",
        "\t\tassert len(all_stcs) > 0, 'all stcs len = 0' \r\n",
        "\r\n",
        "\t\tall_stcs_clean = []\r\n",
        "\t\tall_article_ids_clean = []\r\n",
        "\t\tall_word_ids_clean = []\r\n",
        "\t\tidx = 0\r\n",
        "\t\t\r\n",
        "\t\tfor stc, article_id, word_id in zip(all_stcs, all_article_ids, all_word_ids):\r\n",
        "\t\t\tstc_clean = re.sub(r'(醫師)|(個管師)|(民眾)|(家屬)|(護理師)', '', stc)\r\n",
        "\t\t\t# print(stc, stc_clean, label)\r\n",
        "\t\t\tif len(stc_clean) > 1:  \r\n",
        "\t\t\t# print(stc_clean, stc)\r\n",
        "\t\t\t\tall_stcs_clean.append(stc)\r\n",
        "\t\t\t\tall_article_ids_clean.append(article_id)\r\n",
        "\t\t\t\tall_word_ids_clean.append(word_id)\r\n",
        "\r\n",
        "\t\t\t# 這一步就先把label 做 0 padding\r\n",
        "\t\t\t\r\n",
        "\t\tmax_length = len(max(all_stcs_clean, key=len))\r\n",
        "\t\tassert max_length > 0, 'max length less than 1'\r\n",
        "\r\n",
        "\t\tprint('sentences總數: {}'.format(len(all_stcs_clean)))\r\n",
        "\t\t\t\r\n",
        "\t\t# return all_stcs_clean, all_article_ids_clean, all_word_ids_clean\r\n",
        "\r\n",
        "\t\tself.clean_stcs, self.clean_article_id, self.clean_word_id = [], [] ,[]\r\n",
        "\r\n",
        "\t\tfor stc, article_id, word_id in zip(stcs, article_ids, word_ids):\r\n",
        "\t\t\t# print(stc, article_id, word_id)\r\n",
        "\t\t\tif stc not in ['沒有','也沒有','哪個','那個','算了','不用','有','有有有','有嗎','一點點', '謝謝','不會','不好意思','對不對','好不好','要嗎','還好']:\r\n",
        "\t\t\t\tself.clean_stcs.append(stc)\r\n",
        "\t\t\t\tself.clean_article_id.append(article_id)\r\n",
        "\t\t\t\tself.clean_word_id.append(word_id)\r\n",
        "\t\treturn self.clean_stcs, self.clean_article_id, self.clean_word_id\r\n",
        "\r\n",
        "\tdef encoding(self):\r\n",
        "    \r\n",
        "\t\tclean_stcs, _, _ = self.get_stcs()\r\n",
        "\t\tmax_len = max(len(txt) for txt in clean_stcs)\r\n",
        "\r\n",
        "\t\tencoding = self.tokenizer.batch_encode_plus(clean_stcs, \r\n",
        "\t\t\tpadding=True,\r\n",
        "\t\t\tadd_special_tokens=False,\r\n",
        "\t\t\treturn_attention_mask= True,\r\n",
        "\t\t\treturn_token_type_ids= False,\r\n",
        "\t\t\t#  is_split_into_words=True,\r\n",
        "\t\t\treturn_tensors='pt')\r\n",
        "\r\n",
        "\t\t# batch_size= 32\r\n",
        "\t\tpred_labels = []\r\n",
        "\r\n",
        "\t\tfor idx in range(int((len(clean_stcs)/self.batch_size))):\r\n",
        "\t\t\tinput= encoding['input_ids'][idx*self.batch_size:(idx+1)*self.batch_size].to(device)\r\n",
        "\t\t\tmask = encoding['attention_mask'][idx*self.batch_size:(idx+1)*self.batch_size].to(device)\r\n",
        "\t\t\ttags= torch.zeros((input.size(0),input.size(1)), dtype=torch.long).to(device)\r\n",
        "\t\t\t_, preds = model(input, mask, tags)\r\n",
        "\t\t\tfor pred in preds:\r\n",
        "\t\t\t\tpred_labels.append(pred)\r\n",
        "\r\n",
        "\t\tidx = int((len(clean_stcs)/self.batch_size))\r\n",
        "\t\tinput= encoding['input_ids'][idx*self.batch_size:].to(device)\r\n",
        "\t\tmask = encoding['attention_mask'][idx*self.batch_size:].to(device)\r\n",
        "\t\ttags= torch.zeros((input.size(0),input.size(1)), dtype=torch.long).to(device)\r\n",
        "\t\t_, preds = model(input, mask, tags)\r\n",
        "\t\tfor pred in preds:\r\n",
        "\t\t\tpred_labels.append(pred)\r\n",
        "\r\n",
        "\t\ttag2id = {'[PAD]': 0, 'B-ID': 1, 'B-clinical_event': 2, 'B-contact': 3, 'B-education': 4, 'B-family': 5, 'B-location': 6, 'B-med_exam': 7, 'B-money': 8, 'B-name': 9, 'B-organization': 10, 'B-profession': 11, 'B-time': 12, 'I-ID': 13, 'I-clinical_event': 14, 'I-contact': 15, 'I-education': 16, 'I-family': 17, 'I-location': 18, 'I-med_exam': 19, 'I-money': 20, 'I-name': 21, 'I-organization': 22, 'I-profession': 23, 'I-time': 24, 'O': 25}\r\n",
        "\t\tid2tag ={v:k for k, v in tag2id.items()}\r\n",
        "\r\n",
        "\t\tself.pred_labels_tag = []\r\n",
        "\t\tfor label in pred_labels:\r\n",
        "\t\t\tstc_label = [id2tag[id] for id in label]\r\n",
        "\t\t\tself.pred_labels_tag.append(stc_label)\r\n",
        "\r\n",
        "\t\treturn self.pred_labels_tag\r\n",
        "\r\n",
        "\tdef pred_out_tsv(self):\r\n",
        "\t\t\r\n",
        "\t\tclean_stcs, clean_article_id, clean_word_id = self.get_stcs()\r\n",
        "\t\tpred_labels_tag = self.encoding()\r\n",
        "\r\n",
        "\t\tentity_text = []\r\n",
        "\r\n",
        "\t\tfor stc, labels, article_id, word_id in zip(clean_stcs, pred_labels_tag, clean_article_id, clean_word_id):\r\n",
        "\r\n",
        "\t\t\tentity = str()\r\n",
        "\r\n",
        "\t\t\tstart_pos = 0\r\n",
        "\t\t\tend_pos = 0\r\n",
        "\t\t\tarticle = 0\r\n",
        "\r\n",
        "\t\t\tentity_type = str()\r\n",
        "\r\n",
        "\r\n",
        "\t\t\tfor idx, label in enumerate(labels):\r\n",
        "\t\t\t\tif bool(re.match(r'B-', label)):\r\n",
        "\t\t\t\t\tentity += list(stc)[idx]\r\n",
        "\t\t\t\t\tstart_pos = word_id[idx]\r\n",
        "\t\t\t\t\tarticle = article_id[idx]\r\n",
        "\t\t\t\t\tentity_type = label.split('B-')[1]\r\n",
        "\r\n",
        "\t\t\t\telif bool(re.match(r'I-', label)):\r\n",
        "\t\t\t\t\tentity += list(stc)[idx]\r\n",
        "\t\t\t\t\tend_pos= word_id[idx]\r\n",
        "\t\t\t\t\ttry:\r\n",
        "\t\t\t\t\t\tif (labels[idx+1] == 'O') & (entity_type!=''):\r\n",
        "\t\t\t\t\t\t\tentity_text.append((article, start_pos, end_pos, entity, entity_type))\r\n",
        "\r\n",
        "\t\t\t\t\t\t\tentity = str()\r\n",
        "\t\t\t\t\t\t\tstart_pos = 0\r\n",
        "\t\t\t\t\t\t\tend_pos = 0\r\n",
        "\t\t\t\t\t\t\tarticle = 0\r\n",
        "\t\t\t\t\t\t\tentity_type = str()\r\n",
        "\t\t\t\t\texcept:\r\n",
        "\t\t\t\t\t\tpass\r\n",
        "\t\twith open('test_output.tsv', 'w', encoding='utf-8',newline='\\n') as f:\r\n",
        "\t\t\twriter = csv.writer(f, delimiter='\\t')\r\n",
        "\t\t\twriter.writerow(['article_id','start_position', 'end_position', 'entity_text', 'entity_type'])\r\n",
        "\t\t\tfor (article, start_pos, end_pos, entity, entity_type) in entity_text:\r\n",
        "\t\t\t\twriter.writerow([str(article), str(start_pos), str(end_pos), str(entity), str(entity_type)])\r\n",
        "\r\n",
        "\t\treturn entity_text\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTZoeeJLtl9K"
      },
      "source": [
        "# 訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyZYZVdJhxFa",
        "outputId": "f2de68e4-1286-405e-b753-3af3cfad3391"
      },
      "source": [
        "train_loss = {}\r\n",
        "test_loss = {}\r\n",
        "train_f1 = {}\r\n",
        "test_f1 = {}\r\n",
        "stop_epoch = 0\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "\r\n",
        "  preds_epoch = []\r\n",
        "  gts_epoch = []\r\n",
        "  epoch_loss = 0\r\n",
        "  iteration = 0\r\n",
        "  st = time.time()\r\n",
        "\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  for idx, batch_dict in enumerate(train_dataloader):\r\n",
        "\r\n",
        "    # print('idx: ',idx+1)\r\n",
        "    input_ids = batch_dict['input_ids'].to(device)\r\n",
        "    attention_mask = batch_dict['attention_mask'].to(device)\r\n",
        "    labels = batch_dict['labels'].to(device)\r\n",
        "\r\n",
        "    loss, pred_labels = model(input_ids, attention_mask.bool(), labels)\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\r\n",
        "\r\n",
        "    optimizer.step()\r\n",
        "    scheduler.step()\r\n",
        "    model.zero_grad()\r\n",
        "\r\n",
        "    # mask gt labels \r\n",
        "    labels = batch_dict['labels'].numpy()\r\n",
        "    masks = batch_dict['attention_mask'].numpy()\r\n",
        "\r\n",
        "    labels_nopad = []\r\n",
        "    for label , seq_mask in zip(labels, masks):\r\n",
        "\r\n",
        "        seq = [tag for tag, mask in zip(label, seq_mask) if mask == 1]\r\n",
        "        labels_nopad.append(seq)\r\n",
        "\r\n",
        "    # one dim array \r\n",
        "    preds= [tag for seq in pred_labels for tag in seq]\r\n",
        "    gts= [tag for seq in labels_nopad for tag in seq]\r\n",
        "\r\n",
        "    preds_epoch += preds\r\n",
        "    gts_epoch += gts\r\n",
        "\r\n",
        "    epoch_loss += loss.item()\r\n",
        "    iteration += 1\r\n",
        "\r\n",
        "  f1_macro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'macro')\r\n",
        "  f1_micro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'micro')\r\n",
        "  f1 = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= None)\r\n",
        "  avg_loss = epoch_loss / iteration\r\n",
        "  stop_epoch = epoch+1\r\n",
        "  en = time.time()\r\n",
        "  \r\n",
        "  print('training time :{:.2f}'.format(en-st))\r\n",
        "  print('epoch {}/{} | train_f1(macro, micro) ({:.2f},{:.2f}) | train_epoch_avg_loss {:.3f}| f1 for each class \\n{}\\n'.format(epoch+1, num_epochs, f1_macro, f1_micro, avg_loss, f1))\r\n",
        "\r\n",
        "  test_f1_macro, test_f1_micro, test_avg_loss = test(model= model, test_dataloader= test_dataloader, device= device)\r\n",
        "\r\n",
        "  train_loss[epoch+1] = avg_loss\r\n",
        "  test_loss[epoch+1] = test_avg_loss\r\n",
        "  train_f1[epoch+1] = f1_macro\r\n",
        "  test_f1[epoch+1] = test_f1_macro\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training time :143.83\n",
            "epoch 1/30 | train_f1(macro, micro) (0.02,0.40) | train_epoch_avg_loss 15.201| f1 for each class \n",
            "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 1.43850396e-03 2.08550574e-03 0.00000000e+00\n",
            " 1.83150183e-03 0.00000000e+00 0.00000000e+00 1.34952767e-02\n",
            " 3.97298371e-04 0.00000000e+00 0.00000000e+00 6.40409862e-04\n",
            " 0.00000000e+00 1.64428611e-03 4.41014333e-03 1.72748866e-03\n",
            " 7.09723208e-04 0.00000000e+00 3.74111485e-04 2.09658017e-02\n",
            " 5.74854406e-01]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 2\n",
            "test_f1(macro, micro) (0.05,0.96) | test_avg_loss 0.720 | f1 for each class\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.00643087 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.9798592 ]\n",
            "\n",
            "training time :142.24\n",
            "epoch 2/30 | train_f1(macro, micro) (0.04,0.91) | train_epoch_avg_loss 0.713| f1 for each class \n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.00578871 0.         0.         0.         0.         0.0445015\n",
            " 0.         0.         0.         0.         0.         0.00297177\n",
            " 0.01816118 0.00662252 0.         0.         0.         0.02725208\n",
            " 0.95260129]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 8\n",
            "test_f1(macro, micro) (0.05,0.96) | test_avg_loss 0.292 | f1 for each class\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.01892744 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.07894737 0.98016777]\n",
            "\n",
            "training time :141.64\n",
            "epoch 3/30 | train_f1(macro, micro) (0.07,0.95) | train_epoch_avg_loss 0.269| f1 for each class \n",
            "[0.         0.         0.         0.         0.         0.02247191\n",
            " 0.05188679 0.01183432 0.01365188 0.         0.         0.17005545\n",
            " 0.         0.         0.01470588 0.         0.         0.06451613\n",
            " 0.13169985 0.08501119 0.05591398 0.         0.         0.21844343\n",
            " 0.975898  ]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 13\n",
            "test_f1(macro, micro) (0.16,0.97) | test_avg_loss 0.124 | f1 for each class\n",
            "[0.         0.         0.         0.         0.         0.11111111\n",
            " 0.         0.         0.         0.22857143 0.         0.\n",
            " 0.55555556 0.         0.03636364 0.3697479  0.14285714 0.34693878\n",
            " 0.         0.         0.65248963 0.98834368]\n",
            "\n",
            "training time :141.17\n",
            "epoch 4/30 | train_f1(macro, micro) (0.16,0.96) | train_epoch_avg_loss 0.135| f1 for each class \n",
            "[0.         0.         0.         0.         0.         0.16778523\n",
            " 0.1943734  0.04761905 0.10569106 0.         0.         0.36730946\n",
            " 0.05555556 0.         0.14705882 0.         0.         0.27536232\n",
            " 0.32840237 0.40409207 0.34105263 0.         0.01960784 0.54299697\n",
            " 0.98651132]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 11\n",
            "test_f1(macro, micro) (0.24,0.98) | test_avg_loss 0.096 | f1 for each class\n",
            "[0.         0.         0.         0.         0.38596491 0.04545455\n",
            " 0.         0.11111111 0.         0.61774744 0.         0.\n",
            " 0.         0.         0.63945578 0.35714286 0.52631579 0.66666667\n",
            " 0.         0.76046089 0.99128159]\n",
            "\n",
            "training time :141.27\n",
            "epoch 5/30 | train_f1(macro, micro) (0.22,0.97) | train_epoch_avg_loss 0.099| f1 for each class \n",
            "[0.         0.         0.         0.         0.         0.37857143\n",
            " 0.24068768 0.22580645 0.25409836 0.         0.         0.45903124\n",
            " 0.         0.         0.39726027 0.         0.         0.51908397\n",
            " 0.45454545 0.48087432 0.48953975 0.         0.         0.65228758\n",
            " 0.98985959]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 11\n",
            "test_f1(macro, micro) (0.30,0.98) | test_avg_loss 0.076 | f1 for each class\n",
            "[0.         0.         0.         0.         0.59701493 0.41269841\n",
            " 0.         0.67924528 0.         0.67430442 0.         0.\n",
            " 0.         0.         0.33802817 0.5412844  0.48780488 0.72727273\n",
            " 0.         0.8019802  0.99154384]\n",
            "\n",
            "training time :141.13\n",
            "epoch 6/30 | train_f1(macro, micro) (0.28,0.98) | train_epoch_avg_loss 0.074| f1 for each class \n",
            "[0.         0.         0.         0.         0.04081633 0.5\n",
            " 0.46334311 0.36923077 0.40160643 0.         0.         0.61243144\n",
            " 0.         0.         0.34862385 0.         0.09302326 0.62176166\n",
            " 0.60278207 0.65951743 0.59016393 0.         0.05970149 0.73000423\n",
            " 0.9925251 ]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 12\n",
            "test_f1(macro, micro) (0.35,0.98) | test_avg_loss 0.082 | f1 for each class\n",
            "[0.         0.         0.         0.         0.66666667 0.54054054\n",
            " 0.52173913 0.41860465 0.         0.7242921  0.         0.\n",
            " 0.         0.         0.78571429 0.57446809 0.67857143 0.70175439\n",
            " 0.         0.80692042 0.99188474]\n",
            "\n",
            "training time :141.31\n",
            "epoch 7/30 | train_f1(macro, micro) (0.31,0.98) | train_epoch_avg_loss 0.064| f1 for each class \n",
            "[0.         0.         0.         0.         0.         0.60606061\n",
            " 0.49283668 0.47244094 0.4743083  0.         0.06666667 0.67572464\n",
            " 0.         0.         0.44247788 0.         0.15384615 0.73056995\n",
            " 0.61586314 0.60162602 0.62820513 0.         0.03125    0.78808085\n",
            " 0.99319569]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 10\n",
            "test_f1(macro, micro) (0.33,0.98) | test_avg_loss 0.077 | f1 for each class\n",
            "[0.         0.         0.         0.         0.88       0.\n",
            " 0.375      0.67924528 0.         0.71076923 0.         0.\n",
            " 0.         0.         0.86538462 0.36734694 0.54166667 0.81415929\n",
            " 0.         0.78056426 0.99123385]\n",
            "\n",
            "training time :140.99\n",
            "epoch 8/30 | train_f1(macro, micro) (0.36,0.99) | train_epoch_avg_loss 0.052| f1 for each class \n",
            "[0.         0.         0.08333333 0.         0.05263158 0.72180451\n",
            " 0.64477612 0.6504065  0.5877551  0.         0.07692308 0.741004\n",
            " 0.         0.         0.41025641 0.         0.14634146 0.7989418\n",
            " 0.70253165 0.74929577 0.68980477 0.         0.09090909 0.81745363\n",
            " 0.99444764]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 13\n",
            "test_f1(macro, micro) (0.40,0.98) | test_avg_loss 0.083 | f1 for each class\n",
            "[0.         0.         0.         0.2        0.79411765 0.57894737\n",
            " 0.5        0.6969697  0.         0.6957638  0.         0.\n",
            " 0.52631579 0.         0.75268817 0.61403509 0.57142857 0.73529412\n",
            " 0.         0.80227416 0.99105889]\n",
            "\n",
            "training time :141.01\n",
            "epoch 9/30 | train_f1(macro, micro) (0.39,0.99) | train_epoch_avg_loss 0.048| f1 for each class \n",
            "[0.         0.         0.08333333 0.         0.36363636 0.70817121\n",
            " 0.64589235 0.75968992 0.6122449  0.         0.         0.76666667\n",
            " 0.         0.         0.53225806 0.         0.34146341 0.8\n",
            " 0.74193548 0.85470085 0.71491228 0.         0.03389831 0.84898131\n",
            " 0.99501541]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 14\n",
            "test_f1(macro, micro) (0.45,0.98) | test_avg_loss 0.071 | f1 for each class\n",
            "[0.         0.         0.         0.25       0.84615385 0.64646465\n",
            " 0.58333333 0.68571429 0.         0.73850575 0.         0.\n",
            " 0.5        0.5        0.84210526 0.63157895 0.61538462 0.72340426\n",
            " 0.         0.82713633 0.9924903 ]\n",
            "\n",
            "training time :142.20\n",
            "epoch 10/30 | train_f1(macro, micro) (0.41,0.99) | train_epoch_avg_loss 0.046| f1 for each class \n",
            "[0.         0.         0.19047619 0.         0.17777778 0.8125\n",
            " 0.68571429 0.75       0.66666667 0.         0.         0.77330977\n",
            " 0.         0.         0.31775701 0.         0.53658537 0.85245902\n",
            " 0.80250784 0.81176471 0.76890756 0.         0.19607843 0.85098446\n",
            " 0.99493466]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 14\n",
            "test_f1(macro, micro) (0.43,0.98) | test_avg_loss 0.089 | f1 for each class\n",
            "[0.         0.         0.         0.22222222 0.84210526 0.61538462\n",
            " 0.57142857 0.8358209  0.         0.75118859 0.         0.\n",
            " 0.         0.4        0.88888889 0.65945946 0.57142857 0.8\n",
            " 0.         0.81300813 0.99300885]\n",
            "\n",
            "training time :141.50\n",
            "epoch 11/30 | train_f1(macro, micro) (0.43,0.99) | train_epoch_avg_loss 0.039| f1 for each class \n",
            "[0.         0.         0.         0.         0.58536585 0.87346939\n",
            " 0.79532164 0.88059701 0.7734375  0.         0.         0.80425155\n",
            " 0.         0.         0.25       0.         0.46808511 0.90425532\n",
            " 0.86871961 0.88515406 0.77951002 0.         0.025      0.87849687\n",
            " 0.99583962]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 13\n",
            "test_f1(macro, micro) (0.34,0.98) | test_avg_loss 0.098 | f1 for each class\n",
            "[0.         0.         0.         0.         0.78378378 0.58333333\n",
            " 0.14285714 0.72727273 0.         0.73476112 0.         0.\n",
            " 0.         0.         0.74725275 0.55263158 0.19354839 0.84536082\n",
            " 0.         0.81665449 0.9925826 ]\n",
            "\n",
            "training time :141.25\n",
            "epoch 12/30 | train_f1(macro, micro) (0.40,0.99) | train_epoch_avg_loss 0.041| f1 for each class \n",
            "[0.         0.         0.         0.         0.42105263 0.76862745\n",
            " 0.69740634 0.76190476 0.7755102  0.         0.         0.81550802\n",
            " 0.         0.         0.46017699 0.         0.35       0.8125\n",
            " 0.73765432 0.81305638 0.80888889 0.         0.         0.88471962\n",
            " 0.99619354]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 15\n",
            "test_f1(macro, micro) (0.47,0.98) | test_avg_loss 0.088 | f1 for each class\n",
            "[0.         0.         0.4        0.28571429 0.86842105 0.58181818\n",
            " 0.69565217 0.8125     0.         0.69303202 0.         0.\n",
            " 0.58823529 0.25       0.85981308 0.62275449 0.70175439 0.81415929\n",
            " 0.         0.755      0.99212829]\n",
            "\n",
            "training time :141.51\n",
            "epoch 13/30 | train_f1(macro, micro) (0.46,0.99) | train_epoch_avg_loss 0.033| f1 for each class \n",
            "[0.         0.         0.13793103 0.         0.47368421 0.87704918\n",
            " 0.77272727 0.84210526 0.88148148 0.         0.         0.84342755\n",
            " 0.         0.         0.52713178 0.         0.51282051 0.90410959\n",
            " 0.82278481 0.91291291 0.86567164 0.         0.16666667 0.89985426\n",
            " 0.99675765]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 15\n",
            "test_f1(macro, micro) (0.44,0.98) | test_avg_loss 0.062 | f1 for each class\n",
            "[0.         0.         0.         0.28571429 0.89473684 0.46428571\n",
            " 0.57142857 0.89230769 0.         0.75118859 0.         0.\n",
            " 0.4        0.28571429 0.85436893 0.56074766 0.52631579 0.89090909\n",
            " 0.         0.83540802 0.99295113]\n",
            "\n",
            "training time :142.66\n",
            "epoch 14/30 | train_f1(macro, micro) (0.49,0.99) | train_epoch_avg_loss 0.029| f1 for each class \n",
            "[0.         0.         0.28571429 0.         0.86486486 0.86507937\n",
            " 0.7638484  0.62903226 0.84210526 0.         0.125      0.90673806\n",
            " 0.         0.         0.78195489 0.         0.80851064 0.88235294\n",
            " 0.78980892 0.6996904  0.88546256 0.         0.16666667 0.93066447\n",
            " 0.99760982]\n",
            "\n",
            "gt tag unique 數: 21\n",
            "pred tag unique 數: 15\n",
            "test_f1(macro, micro) (0.42,0.98) | test_avg_loss 0.076 | f1 for each class\n",
            "[0.         0.         0.4        0.         0.87179487 0.55238095\n",
            " 0.         0.87096774 0.         0.76291793 0.         0.\n",
            " 0.86956522 0.         0.81415929 0.61904762 0.3902439  0.87719298\n",
            " 0.         0.84583333 0.99336326]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hHf_PDUEPkg"
      },
      "source": [
        "# 訓練成果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAFcxDva2iGf"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows= 2, ncols= 1, figsize= (10,8), sharex= True)\r\n",
        "ax1.plot([*range(1, stop_epoch+1)], list(train_loss.values()), label= 'train loss')\r\n",
        "ax1.plot([*range(1, stop_epoch+1)], list(test_loss.values()), label= 'test loss')\r\n",
        "ax1.legend()\r\n",
        "\r\n",
        "ax2.plot([*range(1, stop_epoch+1)], list(train_f1.values()), label= 'train f1 (macro)')\r\n",
        "ax2.plot([*range(1, stop_epoch+1)], list(test_f1.values()), label= 'test f1 (macro)')\r\n",
        "ax2.legend()\r\n",
        "plt.savefig('0.65.jpeg')\r\n",
        "plt.tight_layout()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pWa4TQhw5PI"
      },
      "source": [
        "# torch.save(model, '64_wp0.4_256_lr1e-4_30epoch_1e-4_layer1_0.75.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1UTlctlqT8g"
      },
      "source": [
        "# !cp '64_wp0.4_256_lr1e-4_30epoch_1e-4_layer1_0.75.pt' '/content/drive/My Drive/python檔/aicup'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvc6xWNC85De"
      },
      "source": [
        "with open('/content/drive/My Drive/python檔/aicup/test_input.data', 'r', encoding= 'utf-8') as f:\r\n",
        "    data = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApTe5VvOqO-P"
      },
      "source": [
        "pred_outs = test_output(data= data, model= model, tokenizer=tokenizer, batch_size= 32).pred_out_tsv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4qmYVsB9GYp"
      },
      "source": [
        "pred_outs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}