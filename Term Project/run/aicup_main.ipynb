{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "aicup_main",
      "provenance": [],
      "collapsed_sections": [
        "cfh42ir6I9FM",
        "tpu1QT-5tYsi",
        "F72vtVN9tbhD"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk66K6AWaW1q"
      },
      "source": [
        "# !cp drive/MyDrive/python檔/aicup/run/dataset.py .\r\n",
        "# !cp drive/MyDrive/python檔/aicup/run "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GHLkq1bcEox",
        "outputId": "1b8990b6-279a-4e68-929b-ddfa7bbb1e70"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59rSMkbocYfI"
      },
      "source": [
        "import sys\r\n",
        "sys.path.insert(0,\"/content/drive/My Drive/python檔/aicup/run\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3D3ktlwcq1p",
        "outputId": "0c8d58c2-0381-42ce-a86d-70afa6722fa0"
      },
      "source": [
        "pip install transformers==3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.1.94)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3) (20.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6RsrpxBWe1H",
        "outputId": "d2ada259-f03c-4ae3-b40c-3858425b3097"
      },
      "source": [
        "pip install pytorch-crf"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.6/dist-packages (0.7.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6eL0_ORc3lj",
        "outputId": "ece96356-dc50-4e56-8cdc-7d21cd557e9c"
      },
      "source": [
        "pip install pytorch_warmup"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_warmup in /usr/local/lib/python3.6/dist-packages (0.0.4)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_warmup) (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNw4yCiwa1e2"
      },
      "source": [
        "from dataset import bert_stc_dataset\r\n",
        "# from model2 import model_crf\r\n",
        "from train import train\r\n",
        "# from txt_preprocess2 import preprocess2\r\n",
        "import re\r\n",
        "\r\n",
        "from transformers import BertModel, BertTokenizer, get_cosine_schedule_with_warmup\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchsummary import summary\r\n",
        "from torchcrf import CRF\r\n",
        "import pytorch_warmup as warmup\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Qnou0tbJfb",
        "outputId": "c0cba25f-7940-42ef-e709-63df9b0b14b4"
      },
      "source": [
        "file_path = '/content/drive/My Drive/python檔/aicup/run/data/train_input.data'\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "print('{} is being used'.format(device))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda is being used\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfh42ir6I9FM"
      },
      "source": [
        "# preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr4UxdsTI8Y-"
      },
      "source": [
        "class preprocess2():\r\n",
        "\tdef __init__(self, data):\r\n",
        "\t\tself.data = data\r\n",
        "\t\tself.article_id_list = list()\r\n",
        "\t\tself.data_list= list()\r\n",
        "\t\tdata_list_tmp = list()\r\n",
        "\t\tidx = 0\r\n",
        "\r\n",
        "\t\tfor row in data:\r\n",
        "\t\t\tdata_tuple = tuple()\r\n",
        "\t\t\tif row == '\\n':\r\n",
        "\t\t\t\tself.article_id_list.append(idx)\r\n",
        "\t\t\t\tidx+=1\r\n",
        "\t\t\t\t# data_list_tmp.append(('[SEP]','[SEP]'))\r\n",
        "\t\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\t\t\t\t# data_list_tmp = [('[CLS]','[CLS]')]\r\n",
        "\t\t\t\tdata_list_tmp = []\r\n",
        "\r\n",
        "\t\t\telse:\r\n",
        "\t\t\t\trow = row.strip('\\n').split(' ')\r\n",
        "\r\n",
        "\t\t\t\tif row[0] in ['。', '？','！','～','：','，']:\r\n",
        "\t\t\t\t\tself.article_id_list.append(idx)\r\n",
        "\t\t\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\t\t\t\t\tdata_list_tmp= []\r\n",
        "\r\n",
        "\t\t\t\telif row[0] not in ['摁','嗯','啦','喔','欸','啊','齁','嘿','…','...']:\r\n",
        "\t\t\t\t\tdata_tuple = (row[0], row[1])\r\n",
        "\t\t\t\t\tdata_list_tmp.append(data_tuple)\r\n",
        "\t\t\t\t#data_list_tmp 儲存暫時的data_tuple(token,label)\r\n",
        "\t\tif len(data_list_tmp) != 0:\r\n",
        "\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\r\n",
        "\t\t# print(len(self.data_list), len(self.article_id_list))\r\n",
        "\t\t# print(self.data_list[0])\r\n",
        "\t\t# traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(self.data_list,self.article_id_list,test_size= 0.33)\r\n",
        "\t\t# print('ex1 ',self.data_list[2])\r\n",
        "\r\n",
        "\tdef get_stc_label(self):\r\n",
        "\t\tall_stcs = list()\r\n",
        "\t\tall_labels = list()\r\n",
        "\r\n",
        "\t\tfor article_txt_tuple, article_id in zip(self.data_list, self.article_id_list):\r\n",
        "\r\n",
        "\t\t\ttxt_len = len(article_txt_tuple) #(文章數，每個文章對應的總字數) (word, label)\r\n",
        "\t\t\tstc = str() #存字數= max_stc_len的字串\r\n",
        "\t\t\t# labels = ['[CLS]'] # 存該字串對應的label # pytorch crf不需要\r\n",
        "\t\t\tlabels = []\r\n",
        "\r\n",
        "\t\t\tfor idx, (word, label) in enumerate(article_txt_tuple):\r\n",
        "\r\n",
        "\t\t\t\tstc += word\r\n",
        "\t\t\t\tlabels.append(label)\r\n",
        "\r\n",
        "\t\t\t# labels.append('[SEP]') # pytorch crf 不需要sep\r\n",
        "\t\t\t\r\n",
        "\t\t\tall_stcs.append(stc)\r\n",
        "\t\t\tall_labels.append(labels)\r\n",
        "\r\n",
        "\t\tall_stcs_clean = []\r\n",
        "\t\tall_labels_clean = []\r\n",
        "\r\n",
        "\t\tfor stc, label in zip(all_stcs,all_labels):\r\n",
        "\t\t\t\r\n",
        "\t\t\tstc_clean = re.sub(r'(醫師)|(個管師)|(民眾)|(家屬)|(護理師)', '', stc)\r\n",
        "\t\t\t# print(stc, stc_clean, label)\r\n",
        "\t\t\tif (len(stc_clean)>=2) & (len(set(label)) >= 2):\t\r\n",
        "\t\t\t\t# print(stc_clean, stc)\r\n",
        "\t\t\t\tall_stcs_clean.append(stc)\r\n",
        "\r\n",
        "\t\t\t\t# len_diff = len(stc) - len(stc_clean)\r\n",
        "\t\t\t\t\r\n",
        "\t\t\t\t# if len_diff >= 3:\r\n",
        "\r\n",
        "\t\t\t\t# \tdel label[1:1+len_diff]\r\n",
        "\t\t\t\t# if len(set(label)) >= 2:\r\n",
        "\t\t\t\t# \tprint(stc,stc_clean, label)\r\n",
        "\t\t\t\tall_labels_clean.append(label)\r\n",
        "\r\n",
        "\t\t# 這一步就先把label 做 0 padding\r\n",
        "\r\n",
        "\t\tmax_length = len(max(all_stcs_clean, key=len)) \r\n",
        "\t\tpad_labels = []\r\n",
        "\r\n",
        "\t\tfor i in range(len(all_labels_clean)):\r\n",
        "\t\t\ttemp_label = ['[PAD]']*max_length\r\n",
        "\t\t\ttemp_label[:len(all_labels_clean[i])] = all_labels_clean[i]\r\n",
        "\t\t\tpad_labels.append(temp_label)\r\n",
        "\r\n",
        "\t\tprint('sentences總數: {}'.format(len(all_stcs_clean)))\r\n",
        "\t\tprint('labels總數: {}'.format(len(all_labels_clean)))\r\n",
        "\t\t# print(all_stcs[0])\r\n",
        "\t\t# print(all_labels[0])\r\n",
        "\t\treturn all_stcs_clean, pad_labels\r\n",
        "\r\n",
        "\tdef tag2id(self, stcs_label):\r\n",
        "\r\n",
        "\t\tall_label = list()\r\n",
        "\t\tfor stc_label in stcs_label:\r\n",
        "\t\t\tfor label in stc_label:\r\n",
        "\t\t\t\tall_label.append(label)\r\n",
        "\r\n",
        "\t\tlabels_set = sorted(set(all_label))\r\n",
        "\t\ttag2id_dict = {'[PAD]':0} #固定將PAD id設為0\r\n",
        "\r\n",
        "\t\tlabels_set.remove('[PAD]')\r\n",
        "\r\n",
        "\t\tfor idx, label in enumerate(labels_set):\r\n",
        "\t\t\ttag2id_dict[label] = idx+1\r\n",
        "\r\n",
        "\t\t# tag2id_dict['[CLS]'] = len(tag2id_dict) \r\n",
        "\t\t# tag2id_dict['[SEP]'] = len(tag2id_dict)\r\n",
        "\r\n",
        "\t\treturn tag2id_dict\r\n",
        "\r\n",
        "\tdef label_to_ids(self, tag_to_id, raw_labels):\r\n",
        "\r\n",
        "\t\tlabel2id = []\r\n",
        "\t\tfor stc_labels in raw_labels:\r\n",
        "\t\t\tstc_label_ids = [tag_to_id[label] for label in stc_labels]\r\n",
        "\t\t\tlabel2id.append(stc_label_ids)\r\n",
        "\t\treturn label2id\r\n",
        "\r\n",
        "\tdef get_stcs_label2ids(self):\r\n",
        "\r\n",
        "\t\tstcs, labels = self.get_stc_label()\r\n",
        "\t\ttag2id = self.tag2id(stcs_label= labels)\r\n",
        "\t\tlabels_ids= self.label_to_ids(tag_to_id= tag2id, raw_labels= labels)\r\n",
        "\r\n",
        "\t\treturn stcs, labels_ids\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpu1QT-5tYsi"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48kSvtOFtQRg"
      },
      "source": [
        "class model_crf(nn.Module):\r\n",
        "\tdef __init__(self, n_tags, hidden_dim=768, batchsize= 64):\r\n",
        "\t\tsuper(model_crf, self).__init__()\r\n",
        "\t\tself.n_tags = n_tags\r\n",
        "\t\tself.lstm =  nn.LSTM(bidirectional=True, num_layers=2, input_size=768, hidden_size=hidden_dim//2, dropout= 0.1, batch_first=True)\t\t\r\n",
        "\t\tself.hidden_dim = hidden_dim\r\n",
        "\t\tself.fc = nn.Linear(hidden_dim, self.n_tags)\r\n",
        "\t\tself.bert = BertModel.from_pretrained('bert-base-chinese')\r\n",
        "\t\t# self.bert.eval()  # 知用来取bert embedding\r\n",
        "\t\tself.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "\t\tself.CRF = CRF(n_tags, batch_first= True)\r\n",
        "\t\tself.hidden = self.init_hidden(batchsize)\r\n",
        "\r\n",
        "\tdef init_hidden(self, batchsize):\r\n",
        "\t\treturn Variable(torch.randn(2*2, batchsize, self.hidden_dim // 2)).to(self.device), Variable(torch.randn(2*2, batchsize, self.hidden_dim // 2)).to(self.device)\r\n",
        "\r\n",
        "\tdef neg_log_likelihood(self, input_ids, attention_mask, tags):\r\n",
        "\r\n",
        "\t\t# feats : [batchsize, seq_len, n_tags]\r\n",
        "\t\t# tags : [batchsize, n_tags]\r\n",
        "\t\tencoded_layer, _  = self.bert(input_ids.long(), attention_mask)\r\n",
        "\t\tenc, _ = self.lstm(encoded_layer, self.hidden)\r\n",
        "\t\tlstm_feats = self.fc(enc)\r\n",
        "\t\t# lstm_feats[:,:,-1] = lstm_feats[:,:,-1]*0.001\r\n",
        "\r\n",
        "\t\tloss = -self.CRF(lstm_feats, tags, attention_mask.bool(), reduction= 'mean')\r\n",
        "\r\n",
        "\t\treturn loss \r\n",
        "\r\n",
        "\tdef forward(self, input_ids, attention_mask, tags):\r\n",
        "\r\n",
        "\t\tencoded_layer, _  = self.bert(input_ids.long(), attention_mask)\r\n",
        "\t\tenc, _ = self.lstm(encoded_layer)\r\n",
        "\t\tlstm_feats = self.fc(enc)\r\n",
        "\t\tlstm_feats[:,:,1:-1] = lstm_feats[:,:,1:-1]*10\r\n",
        "\t\tloss = -self.CRF(lstm_feats, tags, attention_mask.bool(), reduction= 'token_mean')\r\n",
        "\t\tpred_seqs = self.CRF.decode(emissions= lstm_feats, mask= attention_mask.bool())\r\n",
        "\t\treturn loss, pred_seqs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F72vtVN9tbhD"
      },
      "source": [
        "# 參數設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zTvRlmOldMGf",
        "outputId": "0ade2eeb-d386-4bf5-d54f-b11376f8ebdf"
      },
      "source": [
        "# ---------------前處理---------------\r\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\r\n",
        "\tdata=f.readlines()#.encode('utf-8').decode('utf-8-sig')\r\n",
        "\r\n",
        "preprocessor = preprocess2(data)\r\n",
        "\r\n",
        "stcs, original_labels= preprocessor.get_stc_label()\r\n",
        "stcs, labels = preprocessor.get_stcs_label2ids()\r\n",
        "tag2id_dict = preprocessor.tag2id(original_labels)\r\n",
        "n_tags = len(tag2id_dict)\r\n",
        "print(tag2id_dict)\r\n",
        "print('tags數: {}'.format(n_tags))\r\n",
        "\r\n",
        "gt_tags = [tag for label in labels for tag in label]\r\n",
        "\r\n",
        "for tag in set(gt_tags):\r\n",
        "  print('{}|{}'.format(tag, gt_tags.count(tag)/len(gt_tags)))\r\n",
        "# plt.hist(gt_tags)\r\n",
        "plt.hist([len(stc) for stc in stcs])\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\r\n",
        "# max_length = max([len(txt) for txt in stcs]) # [CLS] + [SEP]\r\n",
        "max_length = len(max(stcs, key=len)) \r\n",
        "print('max_stc_length', max_length)\r\n",
        "\r\n",
        "# ---------------模型---------------\r\n",
        "batchsize= 32\r\n",
        "model = model_crf(n_tags= n_tags, hidden_dim= 256, batchsize= batchsize).to(device)\r\n",
        "# print(summary(model,[(128, 300), (128,300)]))\r\n",
        "\r\n",
        "train_x, test_x, train_y, test_y = train_test_split(stcs, labels, test_size= 0.2, shuffle= False, random_state= 42)\r\n",
        "\r\n",
        "train_dataset = bert_stc_dataset(stcs= train_x, labels= train_y, tokenizer= tokenizer, max_length= max_length)\r\n",
        "print('training stcs 總數: {}'.format(len(train_dataset)))\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size= batchsize, shuffle= True)\r\n",
        "\r\n",
        "num_epochs = 40\r\n",
        "num_iteration = len(train_dataloader)\r\n",
        "print('num_interation',num_iteration)\r\n",
        "total_iter = num_iteration * num_epochs\r\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\r\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=3000, num_training_steps=total_iter)\r\n",
        "# warmup_scheduler = warmup.ExponentialWarmup(optimizer, warmup_period=150)\r\n",
        "\r\n",
        "# ---------------訓練---------------\r\n",
        "# train_model = train(model= model, optimizer= optimizer, train_loader= train_dataloader, test_loader= 0, num_epochs= 5, device= device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentences總數: 1943\n",
            "labels總數: 1943\n",
            "sentences總數: 1943\n",
            "labels總數: 1943\n",
            "{'[PAD]': 0, 'B-ID': 1, 'B-clinical_event': 2, 'B-contact': 3, 'B-education': 4, 'B-family': 5, 'B-location': 6, 'B-med_exam': 7, 'B-money': 8, 'B-name': 9, 'B-organization': 10, 'B-profession': 11, 'B-time': 12, 'I-ID': 13, 'I-clinical_event': 14, 'I-contact': 15, 'I-education': 16, 'I-family': 17, 'I-location': 18, 'I-med_exam': 19, 'I-money': 20, 'I-name': 21, 'I-organization': 22, 'I-profession': 23, 'I-time': 24, 'O': 25}\n",
            "tags數: 26\n",
            "0|0.7639760192513666\n",
            "1|0.00011127957602481535\n",
            "2|6.954973501550959e-05\n",
            "3|0.00026428899305893646\n",
            "4|4.1729841009305755e-05\n",
            "5|0.00034774867507754794\n",
            "6|0.0022395014674994086\n",
            "7|0.00304627839367932\n",
            "8|0.0010849758662419497\n",
            "9|0.002267321361505613\n",
            "10|1.3909947003101918e-05\n",
            "11|0.00018082931104032492\n",
            "12|0.019835584426423334\n",
            "13|0.00020864920504652876\n",
            "14|0.00020864920504652876\n",
            "15|0.0010988858132450516\n",
            "16|4.1729841009305755e-05\n",
            "17|0.0003894785160868537\n",
            "18|0.00333838728074446\n",
            "19|0.00536923954319734\n",
            "20|0.002698529718601772\n",
            "21|0.0037835055848437216\n",
            "22|2.7819894006203837e-05\n",
            "23|0.0005563978801240767\n",
            "24|0.04371896343074933\n",
            "25|0.145080747242353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "max_stc_length 37\n",
            "training stcs 總數: 1554\n",
            "num_interation 49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTZoeeJLtl9K"
      },
      "source": [
        "# 訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyZYZVdJhxFa",
        "outputId": "e85b1474-f409-4aca-ff64-13ba7d49a183"
      },
      "source": [
        "train_loss = {}\r\n",
        "test_loss = {}\r\n",
        "train_f1 = {}\r\n",
        "test_f1 = {}\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "\r\n",
        "  preds_epoch = []\r\n",
        "  gts_epoch = []\r\n",
        "  epoch_loss = 0\r\n",
        "  iteration = 0\r\n",
        "\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  for idx, batch_dict in enumerate(train_dataloader):\r\n",
        "\r\n",
        "    # print('idx: ',idx+1)\r\n",
        "    input_ids = batch_dict['input_ids'].to(device)\r\n",
        "    attention_mask = batch_dict['attention_mask'].to(device)\r\n",
        "    labels = batch_dict['labels'].to(device)\r\n",
        "\r\n",
        "    loss, pred_labels = model(input_ids, attention_mask.bool(), labels)\r\n",
        "\r\n",
        "    # loss = model.neg_log_likelihood(input_ids, attention_mask, labels)\r\n",
        "    loss.backward()\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\r\n",
        "\r\n",
        "    # if (idx+1) % 5 == 0:\r\n",
        "    optimizer.step()\r\n",
        "    scheduler.step()\r\n",
        "    # warmup_scheduler.dampen()\r\n",
        "    model.zero_grad()\r\n",
        "\r\n",
        "    # mask gt labels \r\n",
        "    labels = batch_dict['labels'].numpy()\r\n",
        "    masks = batch_dict['attention_mask'].numpy()\r\n",
        "    # masked_list = (labels*mask).numpy()\r\n",
        "\r\n",
        "    labels_nopad = []\r\n",
        "    for label , seq_mask in zip(labels, masks):\r\n",
        "\r\n",
        "      seq = [tag for tag, mask in zip(label, seq_mask) if mask == 1]\r\n",
        "      labels_nopad.append(seq)\r\n",
        "\r\n",
        "    # one dim array \r\n",
        "    preds= [tag for seq in pred_labels for tag in seq]\r\n",
        "    gts= [tag for seq in labels_nopad for tag in seq]\r\n",
        "\r\n",
        "    preds_epoch += preds\r\n",
        "    gts_epoch += gts\r\n",
        "\r\n",
        "    epoch_loss += loss.item()\r\n",
        "    iteration += 1\r\n",
        "\r\n",
        "    # print('pred_labels', pred_labels)\r\n",
        "    # # print('preds:', preds)\r\n",
        "    # print('gts:',gts)\r\n",
        "    # print(round(f1_score(y_true= gts, y_pred= preds, average= 'macro'),2))\r\n",
        "    # # print(f1_score(y_true= gts, y_pred= preds, average= 'micro'))\r\n",
        "    # print(round(loss.item(),2))\r\n",
        "\r\n",
        "    # print('idx:{} loss:{}'.format(idx, loss))\r\n",
        "\r\n",
        "  f1 = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'macro')\r\n",
        "  f1_micro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'micro')\r\n",
        "  avg_loss = epoch_loss / iteration\r\n",
        "  \r\n",
        "\r\n",
        "  print('epoch {}/{} | train_f1(macro, micro) ({:.2f},{:.2f}) | train_epoch_avg_loss {:.2f}'.format(epoch+1, num_epochs, f1, f1_micro, avg_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/40 | train_f1(macro, micro) (0.01,0.04 | train_epoch_avg_loss 3.31\n",
            "epoch 2/40 | train_f1(macro, micro) (0.02,0.17 | train_epoch_avg_loss 2.79\n",
            "epoch 3/40 | train_f1(macro, micro) (0.04,0.42 | train_epoch_avg_loss 2.10\n",
            "epoch 4/40 | train_f1(macro, micro) (0.06,0.63 | train_epoch_avg_loss 1.60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2gOIFIinusA"
      },
      "source": [
        "# f1_score(y_true= gts, y_pred= preds, average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pWa4TQhw5PI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}