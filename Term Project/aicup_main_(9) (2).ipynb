{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "aicup_main (9).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk66K6AWaW1q"
      },
      "source": [
        "# !cp drive/MyDrive/python檔/aicup/run/dataset.py .\r\n",
        "# !cp drive/MyDrive/python檔/aicup/run "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsnrmjRWrSmN"
      },
      "source": [
        "# loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GHLkq1bcEox",
        "outputId": "7442b0cd-fc4a-4880-c347-33b26c770507"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59rSMkbocYfI"
      },
      "source": [
        "import sys\r\n",
        "sys.path.insert(0,\"/content/drive/My Drive/python檔/aicup/run\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3D3ktlwcq1p",
        "outputId": "375feae5-14e0-4a38-9e60-2f0a218f0247"
      },
      "source": [
        "pip install transformers==3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.1.94)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3) (20.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6RsrpxBWe1H",
        "outputId": "228bb33a-053c-4d28-af05-a51c9bb21f43"
      },
      "source": [
        "pip install pytorch-crf"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.6/dist-packages (0.7.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6eL0_ORc3lj",
        "outputId": "bd2604d4-0dd7-4767-ac54-990dc3314545"
      },
      "source": [
        "pip install pytorch_warmup"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_warmup in /usr/local/lib/python3.6/dist-packages (0.0.4)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_warmup) (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (1.18.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNw4yCiwa1e2"
      },
      "source": [
        "# from dataset import bert_stc_dataset\r\n",
        "# from model2 import model_crf\r\n",
        "from train import train\r\n",
        "# from txt_preprocess2 import preprocess2\r\n",
        "import re\r\n",
        "\r\n",
        "from transformers import BertModel, BertTokenizer, get_cosine_schedule_with_warmup\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchsummary import summary\r\n",
        "from torchcrf import CRF\r\n",
        "import pytorch_warmup as warmup\r\n",
        "# from torch.autograd import Variable\r\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Qnou0tbJfb",
        "outputId": "21621fcc-cd8b-4768-cc86-65179d3a1930"
      },
      "source": [
        "file_path = '/content/drive/My Drive/python檔/aicup/run/data/train_input.data'\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "print('{} is being used'.format(device))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda is being used\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6sIQ8wbXGnc"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo2E2d2TU7dG"
      },
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "import torch\r\n",
        "\r\n",
        "class bert_stc_dataset(Dataset):\r\n",
        "    \r\n",
        "    def __init__(self, stcs, labels, tokenizer, max_length):\r\n",
        "        \r\n",
        "        self.stcs = stcs\r\n",
        "        self.labels = labels\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.max_length = max_length\r\n",
        "        self.pad_labels = []\r\n",
        "\r\n",
        "        # 已經在preprocess2 做完label 0 padding\r\n",
        "        for i in range(len(labels)):\r\n",
        "            temp_label = [0]*max_length\r\n",
        "            temp_label[:len(labels[i])] = labels[i]\r\n",
        "            self.pad_labels.append(temp_label)\r\n",
        "            \r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.stcs)\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        \r\n",
        "        txt = str(self.stcs[idx])\r\n",
        "        \r\n",
        "        txt = ' '.join(list(txt))\r\n",
        "        # print(txt)\r\n",
        "        \r\n",
        "        encoding = self.tokenizer.encode_plus(\r\n",
        "            txt,\r\n",
        "#             truncation= True,\r\n",
        "            max_length= self.max_length,\r\n",
        "            padding = 'max_length',\r\n",
        "            add_special_tokens=False,\r\n",
        "#             pad_to_multiple_of=True,\r\n",
        "            return_attention_mask= True,\r\n",
        "            return_token_type_ids= False,\r\n",
        "            return_tensors='pt')\r\n",
        "        \r\n",
        "        return {\r\n",
        "            'input_ids': encoding['input_ids'].flatten(),\r\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\r\n",
        "            'labels' : torch.tensor(self.pad_labels[idx], dtype= torch.long)\r\n",
        "        }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfh42ir6I9FM"
      },
      "source": [
        "# preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr4UxdsTI8Y-"
      },
      "source": [
        "class preprocess2():\r\n",
        "  def __init__(self, data):\r\n",
        "    self.data = data\r\n",
        "    self.article_id_list = list()\r\n",
        "    self.data_list= list()\r\n",
        "    data_list_tmp = list()\r\n",
        "    idx = 0\r\n",
        "\r\n",
        "    for row in data:\r\n",
        "      data_tuple = tuple()\r\n",
        "      if row == '\\n':\r\n",
        "        self.article_id_list.append(idx)\r\n",
        "        idx+=1\r\n",
        "        self.data_list.append(data_list_tmp)\r\n",
        "        data_list_tmp = []\r\n",
        "\r\n",
        "      else:\r\n",
        "        row = row.strip('\\n').split(' ')\r\n",
        "\r\n",
        "        if (row[0] == '，') & (len(data_list_tmp) >= 64):\r\n",
        "          self.article_id_list.append(idx)\r\n",
        "          self.data_list.append(data_list_tmp)\r\n",
        "          data_list_tmp= []\r\n",
        "\r\n",
        "        elif row[0] in ['。', '？','！','～','：']:\r\n",
        "          self.article_id_list.append(idx)\r\n",
        "          self.data_list.append(data_list_tmp)\r\n",
        "          data_list_tmp= []\r\n",
        "        \r\n",
        "        elif row[0] in ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']:\r\n",
        "          data_tuple = (row[0].lower(), row[1])\r\n",
        "          data_list_tmp.append(data_tuple)\r\n",
        "\r\n",
        "        elif row[0] not in ['摁','嗯','啦','喔','欸','啊','齁','嘿','嘛','…','...','，']:\r\n",
        "          data_tuple = (row[0], row[1])\r\n",
        "          data_list_tmp.append(data_tuple)\r\n",
        "        #data_list_tmp 儲存暫時的data_tuple(token,label)\r\n",
        "    if len(data_list_tmp) != 0:\r\n",
        "      self.data_list.append(data_list_tmp)\r\n",
        "\r\n",
        "  def get_stc_label(self):\r\n",
        "    all_stcs = list()\r\n",
        "    all_labels = list()\r\n",
        "\r\n",
        "    for article_txt_tuple, article_id in zip(self.data_list, self.article_id_list):\r\n",
        "\r\n",
        "      txt_len = len(article_txt_tuple) #(文章數，每個文章對應的總字數) (word, label)\r\n",
        "      stc = str() #存字數= max_stc_len的字串\r\n",
        "      # labels = ['[CLS]'] # 存該字串對應的label # pytorch crf不需要\r\n",
        "      labels = []\r\n",
        "\r\n",
        "      for idx, (word, label) in enumerate(article_txt_tuple):\r\n",
        "\r\n",
        "        stc += word\r\n",
        "        labels.append(label)\r\n",
        "\r\n",
        "      if stc not in all_stcs:\r\n",
        "        all_stcs.append(stc)\r\n",
        "        all_labels.append(labels)\r\n",
        "\r\n",
        "    all_stcs_clean = []\r\n",
        "    all_labels_clean = []\r\n",
        "\r\n",
        "    idx = 0\r\n",
        "    for stc, label in zip(all_stcs,all_labels): #前處理 & downsampling\r\n",
        "      \r\n",
        "      stc_clean = re.sub(r'(醫師)|(個管師)|(民眾)|(家屬)|(護理師)', '', stc)\r\n",
        "      # print(stc, stc_clean, label)\r\n",
        "      if (len(stc_clean)>=2) & (len(set(label)) >= 2):  \r\n",
        "        # print(stc_clean, stc)\r\n",
        "        all_stcs_clean.append(stc)\r\n",
        "        all_labels_clean.append(label)\r\n",
        "    \r\n",
        "      elif (len(stc_clean)>=2) & (((idx+1) % 2) == 0):\r\n",
        "        all_stcs_clean.append(stc)\r\n",
        "        all_labels_clean.append(label)\r\n",
        "      idx += 1\r\n",
        "\r\n",
        "    # 這一步就先把label 做 0 padding\r\n",
        "\r\n",
        "    # max_length = len(max(all_stcs_clean, key=len)) \r\n",
        "    # pad_labels = []\r\n",
        "\r\n",
        "    # for i in range(len(all_labels_clean)):\r\n",
        "    #   temp_label = [0]*max_length\r\n",
        "    #   temp_label[:len(all_labels_clean[i])] = all_labels_clean[i]\r\n",
        "    #   pad_labels.append(temp_label)\r\n",
        "\r\n",
        "    # print('sentences總數: {}'.format(len(all_stcs_clean)))\r\n",
        "    # print('labels總數: {}'.format(len(all_labels_clean)))\r\n",
        "    # return all_stcs_clean, pad_labels\r\n",
        "    return all_stcs_clean, all_labels_clean\r\n",
        "\r\n",
        "  def tag2id(self, stcs_label):\r\n",
        "\r\n",
        "    all_label = list()\r\n",
        "    for stc_label in stcs_label:\r\n",
        "      for label in stc_label:\r\n",
        "        all_label.append(label)\r\n",
        "\r\n",
        "    labels_set = sorted(set(all_label))\r\n",
        "    tag2id_dict = {}\r\n",
        "    # tag2id_dict = {'[PAD]':0} #固定將PAD id設為0\r\n",
        "\r\n",
        "    # labels_set.remove('[PAD]')\r\n",
        "\r\n",
        "    for idx, label in enumerate(labels_set):\r\n",
        "      tag2id_dict[label] = idx\r\n",
        "\r\n",
        "    return tag2id_dict\r\n",
        "\r\n",
        "  def label_to_ids(self, tag_to_id, raw_labels):\r\n",
        "\r\n",
        "    label2id = []\r\n",
        "    for stc_labels in raw_labels:\r\n",
        "      stc_label_ids = [tag_to_id[label] for label in stc_labels]\r\n",
        "      label2id.append(stc_label_ids)\r\n",
        "    return label2id\r\n",
        "\r\n",
        "  def get_stcs_label2ids(self):\r\n",
        "\r\n",
        "    stcs, labels = self.get_stc_label()\r\n",
        "    tag2id = self.tag2id(stcs_label= labels)\r\n",
        "    labels_ids= self.label_to_ids(tag_to_id= tag2id, raw_labels= labels)\r\n",
        "\r\n",
        "    return stcs, labels_ids"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpu1QT-5tYsi"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48kSvtOFtQRg"
      },
      "source": [
        "class model_crf(nn.Module):\r\n",
        "\tdef __init__(self, n_tags, hidden_dim=768, batchsize= 32, num_layers= 1, lstm_dropout= 0.3, fc_dropout= 0.2):\r\n",
        "\t\tsuper(model_crf, self).__init__()\r\n",
        "\t\tself.num_layers = num_layers\r\n",
        "\t\tself.n_tags = n_tags\r\n",
        "\t\tself.lstm =  nn.LSTM(bidirectional=True, num_layers=num_layers, input_size=768, hidden_size=hidden_dim//2, dropout= lstm_dropout, batch_first=True)\t\t\r\n",
        "\t\tself.hidden_dim = hidden_dim\r\n",
        "\t\tself.fc = nn.Linear(hidden_dim, self.n_tags)\r\n",
        "\t\tself.bert = BertModel.from_pretrained('bert-base-chinese')\r\n",
        "\r\n",
        "\t\t# for param in self.bert.parameters():\r\n",
        "\t\t# \tparam.requires_grad = False\r\n",
        "\t\t# self.bert.eval()  # 知用来取bert embedding\r\n",
        "\r\n",
        "\t\tself.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "\t\tself.CRF = CRF(n_tags, batch_first= True)\r\n",
        "\t\tself.dropout = nn.Dropout(p= fc_dropout)\r\n",
        "\t\tself.hidden = self.init_hidden(batchsize)\r\n",
        "\r\n",
        "\tdef init_hidden(self, batch_size):\r\n",
        "\t\treturn (torch.randn(2*self.num_layers, batch_size, self.hidden_dim // 2).to(self.device),\r\n",
        "\t\t\t\ttorch.randn(2*self.num_layers, batch_size, self.hidden_dim // 2).to(self.device))\r\n",
        "\r\n",
        "\tdef forward(self, input_ids, attention_mask, tags):\r\n",
        "\r\n",
        "\t\tbatch_size = input_ids.size(0)\r\n",
        "\t\tmax_seq_len = input_ids.size(1)\r\n",
        "\t\tbert_output, _  = self.bert(input_ids.long(), attention_mask)\r\n",
        "\t\tseq_len = torch.sum(attention_mask, dim= 1).cpu().int()\r\n",
        "\t\t# print(seq_len)\r\n",
        "\t\tpack_input = pack_padded_sequence(input= bert_output, lengths= seq_len, batch_first= True, enforce_sorted= False)\r\n",
        "\t\tpacked_lstm_out, _ = self.lstm(pack_input,self.init_hidden(batch_size= batch_size))\r\n",
        "\t\tlstm_enc, _=  pad_packed_sequence(packed_lstm_out, batch_first=True)\r\n",
        "\t\t# print(lstm_enc.size())\r\n",
        "\t\tlstm_enc = self.dropout(lstm_enc)\r\n",
        "\t\tlstm_feats = self.fc(lstm_enc)\r\n",
        "\r\n",
        "\t\tlstm_max_seq_len = lstm_feats.size(1)\r\n",
        "\t\tpad = torch.zeros(size=(batch_size, max_seq_len-lstm_max_seq_len, self.n_tags), dtype= torch.float).to(self.device)\r\n",
        "\t\tlstm_feats= torch.cat((lstm_feats, pad), dim= 1)\r\n",
        "  \r\n",
        "\t\t# lstm_feats[:,:,:4] = lstm_feats[:,:,:5]*100\r\n",
        "\t\t# lstm_feats[:,:,5:9] = lstm_feats[:,:,5:9]*10\r\n",
        "\t\t# lstm_feats[:,:,9:11] = lstm_feats[:,:,9:11]*100\r\n",
        "\t\t# lstm_feats[:,:,11] = lstm_feats[:,:,11]*100\r\n",
        "\t\t# lstm_feats[:,:,12:17] = lstm_feats[:,:,12:17]*100\r\n",
        "\t\t# lstm_feats[:,:,17:21] = lstm_feats[:,:,17:21]*10\r\n",
        "\t\t# lstm_feats[:,:,21:23] = lstm_feats[:,:,21:23]*100\r\n",
        "\t\t# lstm_feats[:,:,23] = lstm_feats[:,:,23]*1\r\n",
        "\r\n",
        "\t\tlstm_feats[:,:,:23] = lstm_feats[:,:,:23]*100\r\n",
        "\t\tlstm_feats[:,:,23] = lstm_feats[:,:,23]*10\r\n",
        "\r\n",
        "\t\tloss = -self.CRF(lstm_feats, tags, attention_mask.bool(), reduction= 'token_mean')\r\n",
        "\t\tpred_seqs = self.CRF.decode(emissions= lstm_feats, mask= attention_mask.bool())\r\n",
        "  \r\n",
        "\t\treturn loss, pred_seqs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYEi_mFctfCR"
      },
      "source": [
        "# 載入stcs, tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWEzS06NtqJZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "outputId": "9d83cf7f-e57e-4fb7-aab3-73cf64300a36"
      },
      "source": [
        "# ---------------前處理---------------\r\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\r\n",
        "\tdata=f.readlines()#.encode('utf-8').decode('utf-8-sig')\r\n",
        "\r\n",
        "preprocessor = preprocess2(data)\r\n",
        "\r\n",
        "stcs, original_labels= preprocessor.get_stc_label()\r\n",
        "stcs, labels = preprocessor.get_stcs_label2ids()\r\n",
        "tag2id_dict = preprocessor.tag2id(original_labels)\r\n",
        "n_tags = len(tag2id_dict)\r\n",
        "print(tag2id_dict)\r\n",
        "print('tags數: {}'.format(n_tags))\r\n",
        "\r\n",
        "gt_tags = [tag for label in labels for tag in label]\r\n",
        "\r\n",
        "for tag in set(gt_tags):\r\n",
        "  print('{}|{}'.format(tag, gt_tags.count(tag)/len(gt_tags)))\r\n",
        "# plt.hist(gt_tags)\r\n",
        "plt.hist([len(stc) for stc in stcs])\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\r\n",
        "max_length = len(max(stcs, key=len)) \r\n",
        "print('max_length', max_length)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-ID': 0, 'B-clinical_event': 1, 'B-contact': 2, 'B-education': 3, 'B-family': 4, 'B-location': 5, 'B-med_exam': 6, 'B-money': 7, 'B-name': 8, 'B-organization': 9, 'B-profession': 10, 'B-time': 11, 'I-ID': 12, 'I-clinical_event': 13, 'I-contact': 14, 'I-education': 15, 'I-family': 16, 'I-location': 17, 'I-med_exam': 18, 'I-money': 19, 'I-name': 20, 'I-organization': 21, 'I-profession': 22, 'I-time': 23, 'O': 24}\n",
            "tags數: 25\n",
            "0|8.351079377009478e-05\n",
            "1|5.219424610630924e-05\n",
            "2|0.00018789928598271326\n",
            "3|3.131654766378555e-05\n",
            "4|0.0002505323813102844\n",
            "5|0.0016388993277381102\n",
            "6|0.0022234748841287737\n",
            "7|0.0008142302392584242\n",
            "8|0.0017328489707294668\n",
            "9|1.0438849221261848e-05\n",
            "10|0.00013570503987640403\n",
            "11|0.014708338552757943\n",
            "12|0.00015658273831892772\n",
            "13|0.00015658273831892772\n",
            "14|0.0007724748423733768\n",
            "15|3.131654766378555e-05\n",
            "16|0.0002818489289740699\n",
            "17|0.002463568416217796\n",
            "18|0.0039041296087519313\n",
            "19|0.0020251367489247983\n",
            "20|0.002902000083510794\n",
            "21|2.0877698442523696e-05\n",
            "22|0.00041755396885047395\n",
            "23|0.03222472754603532\n",
            "24|0.9327738110150737\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "max_length 91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F72vtVN9tbhD"
      },
      "source": [
        "# 參數設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTvRlmOldMGf",
        "outputId": "26c772c6-c932-42d1-880b-16a3b0da2cf5"
      },
      "source": [
        "# --------------- 現在model只有單層---------------\r\n",
        "batchsize= 64\r\n",
        "hidden_dim= 256\r\n",
        "num_epochs= 30\r\n",
        "lr = 1e-4\r\n",
        "weight_decay = 1e-5\r\n",
        "warmup_ratio = 0.3\r\n",
        "lstm_dropout = 0.2\r\n",
        "fc_dropout = 0\r\n",
        "num_layers = 1\r\n",
        "\r\n",
        "model = model_crf(n_tags= n_tags, hidden_dim= hidden_dim, batchsize= batchsize, num_layers= num_layers, lstm_dropout= lstm_dropout, fc_dropout= fc_dropout).to(device)\r\n",
        "# print(summary(model,[(128, 300), (128,300)]))\r\n",
        "\r\n",
        "train_x, test_x, train_y, test_y = train_test_split(stcs, labels, test_size= 0.2, shuffle= True, random_state= 42)\r\n",
        "print('test gt tags unique數 :{}'.format(len(set([tag for label in test_y for tag in label]))))\r\n",
        "\r\n",
        "train_dataset = bert_stc_dataset(stcs= train_x, labels= train_y, tokenizer= tokenizer, max_length= max_length)\r\n",
        "print(train_x[0:3])\r\n",
        "print(train_dataset[0:3]['input_ids'])\r\n",
        "print(train_dataset[0:3]['labels'])\r\n",
        "test_dataset = bert_stc_dataset(stcs= test_x, labels= test_y, tokenizer= tokenizer, max_length= max_length)\r\n",
        "\r\n",
        "print('training stcs 總數: {}'.format(len(train_dataset)))\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size= batchsize, shuffle= True, num_workers= 4)\r\n",
        "test_dataloader = DataLoader(test_dataset, batch_size= batchsize, shuffle= False, num_workers= 4)\r\n",
        "\r\n",
        "num_iteration = len(train_dataloader)\r\n",
        "print('num_iteration',num_iteration)\r\n",
        "total_iter = num_iteration * num_epochs\r\n",
        "warmup = total_iter * warmup_ratio\r\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay= weight_decay)\r\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps= warmup, num_training_steps=total_iter)\r\n",
        "# warmup_scheduler = warmup.ExponentialWarmup(optimizer, warmup_period=150)\r\n",
        "\r\n",
        "# ---------------訓練---------------\r\n",
        "# train_model = train(model= model, optimizer= optimizer, train_loader= train_dataloader, test_loader= 0, num_epochs= 5, device= device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test gt tags unique數 :23\n",
            "['台北士林', '要住院那就完蛋了', '阿待會我們從後面']\n",
            "tensor([ 138,  112, 1378, 1266, 1894, 3360,  112,  117,  112, 6206,  857, 7368,\n",
            "        6929, 2218, 2130, 6028,  749,  112,  117,  112, 7350, 2521, 3298, 2769,\n",
            "         947, 2537, 2527, 7481,  112,  140,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0])\n",
            "tensor([[ 5, 17, 17, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0],\n",
            "        [24, 24, 24, 24, 24, 24, 24, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0],\n",
            "        [24, 24, 24, 24, 24, 24, 24, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0]])\n",
            "training stcs 總數: 5459\n",
            "num_iteration 86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rPf2heKjCd7"
      },
      "source": [
        "# test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfjiko8GjBZD"
      },
      "source": [
        "def test(model, test_dataloader, device):\r\n",
        "\r\n",
        "  preds_epoch = []\r\n",
        "  gts_epoch = []\r\n",
        "  epoch_loss = 0\r\n",
        "  iteration = 0\r\n",
        "\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  for idx, batch_dict in enumerate(test_dataloader):\r\n",
        "\r\n",
        "    # print('idx: ',idx+1)\r\n",
        "    input_ids = batch_dict['input_ids'].to(device)\r\n",
        "    attention_mask = batch_dict['attention_mask'].to(device)\r\n",
        "    labels = batch_dict['labels'].to(device)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "      loss, pred_labels = model(input_ids, attention_mask.bool(), labels)\r\n",
        "\r\n",
        "    # mask gt labels \r\n",
        "    labels = batch_dict['labels'].numpy()\r\n",
        "    masks = batch_dict['attention_mask'].numpy()\r\n",
        "\r\n",
        "    labels_nopad = []\r\n",
        "    for label , seq_mask in zip(labels, masks):\r\n",
        "\r\n",
        "      seq = [tag for tag, mask in zip(label, seq_mask) if mask == 1]\r\n",
        "      labels_nopad.append(seq)\r\n",
        "\r\n",
        "    # one dim array \r\n",
        "    preds= [tag for seq in pred_labels for tag in seq]\r\n",
        "    gts= [tag for seq in labels_nopad for tag in seq]\r\n",
        "\r\n",
        "    preds_epoch += preds\r\n",
        "    gts_epoch += gts\r\n",
        "\r\n",
        "    epoch_loss += loss.item()\r\n",
        "    iteration += 1\r\n",
        "\r\n",
        "  f1_macro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'macro')\r\n",
        "  f1_micro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'micro')\r\n",
        "  f1 = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= None)\r\n",
        "  avg_loss = epoch_loss / iteration\r\n",
        "\r\n",
        "  print('gt tag unique 數: {}'.format(len(set(gts_epoch))))\r\n",
        "  print('pred tag unique 數: {}'.format(len(set(preds_epoch))))\r\n",
        "  print('test_f1(macro, micro) ({:.2f},{:.2f}) | test_avg_loss {:.3f} | f1 for each class\\n{}'.format(f1_macro, f1_micro, avg_loss, f1))\r\n",
        "\r\n",
        "  return f1_macro, f1_micro, avg_loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NSLpK9g7bp5"
      },
      "source": [
        "# test out function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2yAR0_67gGZ"
      },
      "source": [
        "class test_output():\r\n",
        "\tdef __init__(self, data, model, tokenizer, batch_size):\r\n",
        "\r\n",
        "\t\tself.model = model\r\n",
        "\t\tself.tokenizer = tokenizer\r\n",
        "\t\tself.batch_size = batch_size\r\n",
        "\t\tself.data_list = []\r\n",
        "\t\tself.word_id = []\r\n",
        "\t\tself.word_article_id = [] \r\n",
        "\t\tarticle_id = 0\r\n",
        "\t\tword_id = 0\r\n",
        "\t\tdata_list_tmp = []\r\n",
        "\t\tarticle_id_tmp = []\r\n",
        "\t\tword_id_tmp = []\r\n",
        "\t\t\r\n",
        "\t\tfor row in data:\r\n",
        "\t\t\t\r\n",
        "\t\t\tdata_tuple = tuple()\r\n",
        "\t\t\tif row == '\\n':\r\n",
        "\t\t\t\t\r\n",
        "\t\t\t\tarticle_id += 1 \r\n",
        "\t\t\t\tword_id = 0\r\n",
        "\t\t\t\tself.word_id.append(word_id)\r\n",
        "\t\t\t\tself.word_article_id.append(article_id_tmp)\r\n",
        "\t\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\t\t\t\tdata_list_tmp = []\r\n",
        "\t\t\t\tarticle_id_tmp = []\r\n",
        "\t\t\t\tword_id_tmp = []\r\n",
        "\r\n",
        "\t\t\telse:\r\n",
        "\t\t\t\t\r\n",
        "\t\t\t\trow = row.strip('\\n').split(' ')\r\n",
        "\r\n",
        "\t\t\t\tif row[0] in ['。', '？','！','，','～','：']:\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\t\tself.word_id.append(word_id_tmp)\r\n",
        "\t\t\t\t\tself.word_article_id.append(article_id_tmp)\r\n",
        "\t\t\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\t\t\t\t\tdata_list_tmp = []\r\n",
        "\t\t\t\t\tarticle_id_tmp = []\r\n",
        "\t\t\t\t\tword_id_tmp = []\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\telif row[0] in ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']:\r\n",
        "\t\t\t\t\t  \r\n",
        "\t\t\t\t\tdata_tuple = (row[0].lower(), article_id, word_id)\r\n",
        "\t\t\t\t\tdata_list_tmp.append(data_tuple)\r\n",
        "\t\t\t\t\tarticle_id_tmp.append(article_id)\r\n",
        "\t\t\t\t\tword_id_tmp.append(word_id)\r\n",
        "\r\n",
        "\t\t\t\telif row[0] not in ['摁','嗯','啦','喔','欸','啊','齁','嘿','…','...','，']:\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\t\tdata_tuple = (row[0], article_id, word_id)\r\n",
        "\t\t\t\t\tdata_list_tmp.append(data_tuple)\r\n",
        "\t\t\t\t\tarticle_id_tmp.append(article_id)\r\n",
        "\t\t\t\t\tword_id_tmp.append(word_id)\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\tword_id += 1\r\n",
        "\t\t\t\t\r\n",
        "\t\tif len(data_list_tmp) != 0:\r\n",
        "\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\t\t\tself.word_id.append(word_id_tmp)\r\n",
        "\t\t\tself.word_article_id.append(article_id_tmp)\r\n",
        "\t\t\t\r\n",
        "\tdef raw_output(self):\r\n",
        "\t\treturn self.data_list, self.word_id, self.word_article_id\r\n",
        "\r\n",
        "\tdef get_stcs(self):\r\n",
        "\t\t\r\n",
        "\t\tall_stcs = list()\r\n",
        "\t\tall_article_ids = list()\r\n",
        "\t\tall_word_ids = list()\r\n",
        "\r\n",
        "\t\tfor stc_list in self.data_list:\r\n",
        "\r\n",
        "\t\t\ttxt_len = len(stc_list) #(文章數，每個文章對應的總字數) (word, label)\r\n",
        "\t\t\tstc = str() #存字數= max_stc_len的字串\r\n",
        "\t\t\tarticle_ids = []\r\n",
        "\t\t\tword_ids = []\r\n",
        "\t\t\t\r\n",
        "\r\n",
        "\t\t\tfor idx, (word,article_id, word_id) in enumerate(stc_list):\r\n",
        "\r\n",
        "\t\t\t\tstc += word\r\n",
        "\t\t\t\tarticle_ids.append(article_id)\r\n",
        "\t\t\t\tword_ids.append(word_id)\r\n",
        "\r\n",
        "\t\t\tall_stcs.append(stc)\r\n",
        "\t\t\tall_article_ids.append(article_ids)\r\n",
        "\t\t\tall_word_ids.append(word_ids)\r\n",
        "\r\n",
        "\t\tassert len(all_stcs) > 0, 'all stcs len = 0' \r\n",
        "\r\n",
        "\t\tall_stcs_clean = []\r\n",
        "\t\tall_article_ids_clean = []\r\n",
        "\t\tall_word_ids_clean = []\r\n",
        "\t\tidx = 0\r\n",
        "\t\t\r\n",
        "\t\tfor stc, article_id, word_id in zip(all_stcs, all_article_ids, all_word_ids):\r\n",
        "\t\t\tstc_clean = re.sub(r'(醫師)|(個管師)|(民眾)|(家屬)|(護理師)', '', stc)\r\n",
        "\t\t\t# print(stc, stc_clean, label)\r\n",
        "\t\t\tif len(stc_clean) > 1:  \r\n",
        "\t\t\t# print(stc_clean, stc)\r\n",
        "\t\t\t\tall_stcs_clean.append(stc)\r\n",
        "\t\t\t\tall_article_ids_clean.append(article_id)\r\n",
        "\t\t\t\tall_word_ids_clean.append(word_id)\r\n",
        "\r\n",
        "\t\t\t# 這一步就先把label 做 0 padding\r\n",
        "\t\t\t\r\n",
        "\t\tmax_length = len(max(all_stcs_clean, key=len))\r\n",
        "\t\tassert max_length > 0, 'max length less than 1'\r\n",
        "\r\n",
        "\t\tprint('sentences總數: {}'.format(len(all_stcs_clean)))\r\n",
        "\t\t\t\r\n",
        "\t\t# return all_stcs_clean, all_article_ids_clean, all_word_ids_clean\r\n",
        "\r\n",
        "\t\tself.clean_stcs, self.clean_article_id, self.clean_word_id = [], [] ,[]\r\n",
        "\r\n",
        "\t\tfor stc, article_id, word_id in zip(stcs, article_ids, word_ids):\r\n",
        "\t\t\t# print(stc, article_id, word_id)\r\n",
        "\t\t\tif stc not in ['沒有','也沒有','哪個','那個','算了','不用','有','有有有','有嗎','一點點', '謝謝','不會','不好意思','對不對','好不好','要嗎','還好']:\r\n",
        "\t\t\t\tself.clean_stcs.append(stc)\r\n",
        "\t\t\t\tself.clean_article_id.append(article_id)\r\n",
        "\t\t\t\tself.clean_word_id.append(word_id)\r\n",
        "\t\treturn self.clean_stcs, self.clean_article_id, self.clean_word_id\r\n",
        "\r\n",
        "\tdef encoding(self):\r\n",
        "    \r\n",
        "\t\tclean_stcs, _, _ = self.get_stcs()\r\n",
        "\t\tmax_len = max(len(txt) for txt in clean_stcs)\r\n",
        "\r\n",
        "\t\tencoding = self.tokenizer.batch_encode_plus(clean_stcs, \r\n",
        "\t\t\tpadding=True,\r\n",
        "\t\t\tadd_special_tokens=False,\r\n",
        "\t\t\treturn_attention_mask= True,\r\n",
        "\t\t\treturn_token_type_ids= False,\r\n",
        "\t\t\t#  is_split_into_words=True,\r\n",
        "\t\t\treturn_tensors='pt')\r\n",
        "\r\n",
        "\t\t# batch_size= 32\r\n",
        "\t\tpred_labels = []\r\n",
        "\r\n",
        "\t\tfor idx in range(int((len(clean_stcs)/self.batch_size))):\r\n",
        "\t\t\tinput= encoding['input_ids'][idx*self.batch_size:(idx+1)*self.batch_size].to(device)\r\n",
        "\t\t\tmask = encoding['attention_mask'][idx*self.batch_size:(idx+1)*self.batch_size].to(device)\r\n",
        "\t\t\ttags= torch.zeros((input.size(0),input.size(1)), dtype=torch.long).to(device)\r\n",
        "\t\t\t_, preds = model(input, mask, tags)\r\n",
        "\t\t\tfor pred in preds:\r\n",
        "\t\t\t\tpred_labels.append(pred)\r\n",
        "\r\n",
        "\t\tidx = int((len(clean_stcs)/self.batch_size))\r\n",
        "\t\tinput= encoding['input_ids'][idx*self.batch_size:].to(device)\r\n",
        "\t\tmask = encoding['attention_mask'][idx*self.batch_size:].to(device)\r\n",
        "\t\ttags= torch.zeros((input.size(0),input.size(1)), dtype=torch.long).to(device)\r\n",
        "\t\t_, preds = model(input, mask, tags)\r\n",
        "\t\tfor pred in preds:\r\n",
        "\t\t\tpred_labels.append(pred)\r\n",
        "\r\n",
        "\t\ttag2id = {'[PAD]': 0, 'B-ID': 1, 'B-clinical_event': 2, 'B-contact': 3, 'B-education': 4, 'B-family': 5, 'B-location': 6, 'B-med_exam': 7, 'B-money': 8, 'B-name': 9, 'B-organization': 10, 'B-profession': 11, 'B-time': 12, 'I-ID': 13, 'I-clinical_event': 14, 'I-contact': 15, 'I-education': 16, 'I-family': 17, 'I-location': 18, 'I-med_exam': 19, 'I-money': 20, 'I-name': 21, 'I-organization': 22, 'I-profession': 23, 'I-time': 24, 'O': 25}\r\n",
        "\t\tid2tag ={v:k for k, v in tag2id.items()}\r\n",
        "\r\n",
        "\t\tself.pred_labels_tag = []\r\n",
        "\t\tfor label in pred_labels:\r\n",
        "\t\t\tstc_label = [id2tag[id] for id in label]\r\n",
        "\t\t\tself.pred_labels_tag.append(stc_label)\r\n",
        "\r\n",
        "\t\treturn self.pred_labels_tag\r\n",
        "\r\n",
        "\tdef pred_out_tsv(self):\r\n",
        "\t\t\r\n",
        "\t\tclean_stcs, clean_article_id, clean_word_id = self.get_stcs()\r\n",
        "\t\tpred_labels_tag = self.encoding()\r\n",
        "\r\n",
        "\t\tentity_text = []\r\n",
        "\r\n",
        "\t\tfor stc, labels, article_id, word_id in zip(clean_stcs, pred_labels_tag, clean_article_id, clean_word_id):\r\n",
        "\r\n",
        "\t\t\tentity = str()\r\n",
        "\r\n",
        "\t\t\tstart_pos = 0\r\n",
        "\t\t\tend_pos = 0\r\n",
        "\t\t\tarticle = 0\r\n",
        "\r\n",
        "\t\t\tentity_type = str()\r\n",
        "\r\n",
        "\r\n",
        "\t\t\tfor idx, label in enumerate(labels):\r\n",
        "\t\t\t\tif bool(re.match(r'B-', label)):\r\n",
        "\t\t\t\t\tentity += list(stc)[idx]\r\n",
        "\t\t\t\t\tstart_pos = word_id[idx]\r\n",
        "\t\t\t\t\tarticle = article_id[idx]\r\n",
        "\t\t\t\t\tentity_type = label.split('B-')[1]\r\n",
        "\r\n",
        "\t\t\t\telif bool(re.match(r'I-', label)):\r\n",
        "\t\t\t\t\tentity += list(stc)[idx]\r\n",
        "\t\t\t\t\tend_pos= word_id[idx]\r\n",
        "\t\t\t\t\ttry:\r\n",
        "\t\t\t\t\t\tif (labels[idx+1] == 'O') & (entity_type!=''):\r\n",
        "\t\t\t\t\t\t\tentity_text.append((article, start_pos, end_pos, entity, entity_type))\r\n",
        "\r\n",
        "\t\t\t\t\t\t\tentity = str()\r\n",
        "\t\t\t\t\t\t\tstart_pos = 0\r\n",
        "\t\t\t\t\t\t\tend_pos = 0\r\n",
        "\t\t\t\t\t\t\tarticle = 0\r\n",
        "\t\t\t\t\t\t\tentity_type = str()\r\n",
        "\t\t\t\t\texcept:\r\n",
        "\t\t\t\t\t\tpass\r\n",
        "\t\twith open('test_output.tsv', 'w', encoding='utf-8',newline='\\n') as f:\r\n",
        "\t\t\twriter = csv.writer(f, delimiter='\\t')\r\n",
        "\t\t\twriter.writerow(['article_id','start_position', 'end_position', 'entity_text', 'entity_type'])\r\n",
        "\t\t\tfor (article, start_pos, end_pos, entity, entity_type) in entity_text:\r\n",
        "\t\t\t\twriter.writerow([str(article), str(start_pos), str(end_pos), str(entity), str(entity_type)])\r\n",
        "\r\n",
        "\t\treturn entity_text\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTZoeeJLtl9K"
      },
      "source": [
        "# 訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyZYZVdJhxFa",
        "outputId": "bdcb7b2a-af02-4db7-ad67-252c4b291b0a"
      },
      "source": [
        "train_loss = {}\r\n",
        "test_loss = {}\r\n",
        "train_f1 = {}\r\n",
        "test_f1 = {}\r\n",
        "stop_epoch = 0\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "\r\n",
        "  preds_epoch = []\r\n",
        "  gts_epoch = []\r\n",
        "  epoch_loss = 0\r\n",
        "  iteration = 0\r\n",
        "\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  for idx, batch_dict in enumerate(train_dataloader):\r\n",
        "\r\n",
        "    # print('idx: ',idx+1)\r\n",
        "    input_ids = batch_dict['input_ids'].to(device)\r\n",
        "    attention_mask = batch_dict['attention_mask'].to(device)\r\n",
        "    labels = batch_dict['labels'].to(device)\r\n",
        "\r\n",
        "    loss, pred_labels = model(input_ids, attention_mask.bool(), labels)\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\r\n",
        "\r\n",
        "    optimizer.step()\r\n",
        "    scheduler.step()\r\n",
        "    model.zero_grad()\r\n",
        "\r\n",
        "    # mask gt labels \r\n",
        "    labels = batch_dict['labels'].numpy()\r\n",
        "    masks = batch_dict['attention_mask'].numpy()\r\n",
        "\r\n",
        "    labels_nopad = []\r\n",
        "    for label , seq_mask in zip(labels, masks):\r\n",
        "\r\n",
        "        seq = [tag for tag, mask in zip(label, seq_mask) if mask == 1]\r\n",
        "        labels_nopad.append(seq)\r\n",
        "\r\n",
        "    # one dim array \r\n",
        "    preds= [tag for seq in pred_labels for tag in seq]\r\n",
        "    gts= [tag for seq in labels_nopad for tag in seq]\r\n",
        "\r\n",
        "    preds_epoch += preds\r\n",
        "    gts_epoch += gts\r\n",
        "\r\n",
        "    epoch_loss += loss.item()\r\n",
        "    iteration += 1\r\n",
        "\r\n",
        "  f1_macro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'macro')\r\n",
        "  f1_micro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'micro')\r\n",
        "  f1 = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= None)\r\n",
        "  avg_loss = epoch_loss / iteration\r\n",
        "  stop_epoch = epoch+1\r\n",
        "\r\n",
        "  print('epoch {}/{} | train_f1(macro, micro) ({:.2f},{:.2f}) | train_epoch_avg_loss {:.3f}| f1 for each class \\n{}'.format(epoch+1, num_epochs, f1_macro, f1_micro, avg_loss, f1))\r\n",
        "\r\n",
        "  test_f1_macro, test_f1_micro, test_avg_loss = test(model= model, test_dataloader= test_dataloader, device= device)\r\n",
        "\r\n",
        "  train_loss[epoch+1] = avg_loss\r\n",
        "  test_loss[epoch+1] = test_avg_loss\r\n",
        "  train_f1[epoch+1] = f1_macro\r\n",
        "  test_f1[epoch+1] = test_f1_macro\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/30 | train_f1(macro, micro) (0.03,0.52) | train_epoch_avg_loss 10.304| f1 for each class \n",
            "[0.00111732 0.         0.         0.         0.00094607 0.\n",
            " 0.         0.00145349 0.0058451  0.         0.         0.03140831\n",
            " 0.         0.         0.         0.         0.         0.00261952\n",
            " 0.00656007 0.00645161 0.00847458 0.         0.         0.00477137\n",
            " 0.69425768]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {0, 2, 4, 5, 6, 7, 8, 9, 11, 13, 14, 16, 17, 18, 19, 20, 23, 24}\n",
            "test_f1(macro, micro) (0.05,0.91) | test_avg_loss 0.67 | f1 for each class\n",
            "[0.         0.         0.         0.         0.         0.03636364\n",
            " 0.03846154 0.         0.         0.         0.         0.10176125\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.01388889 0.         0.         0.         0.00304878 0.95583868]\n",
            "epoch 2/30 | train_f1(macro, micro) (0.06,0.91) | train_epoch_avg_loss 0.546| f1 for each class \n",
            "[0.         0.         0.         0.         0.         0.0242915\n",
            " 0.04878049 0.01709402 0.00790514 0.         0.         0.16917626\n",
            " 0.         0.         0.02150538 0.         0.         0.04532578\n",
            " 0.08609272 0.06872852 0.01671309 0.         0.         0.02858277\n",
            " 0.95815157]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {0, 4, 5, 6, 7, 8, 11, 14, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "test_f1(macro, micro) (0.16,0.94) | test_avg_loss 0.27 | f1 for each class\n",
            "[0.         0.         0.         0.         0.         0.17021277\n",
            " 0.         0.08       0.         0.         0.49319728 0.\n",
            " 0.         0.         0.         0.         0.33333333 0.18897638\n",
            " 0.55696203 0.3877551  0.         0.49462366 0.97255695]\n",
            "epoch 3/30 | train_f1(macro, micro) (0.19,0.95) | train_epoch_avg_loss 0.249| f1 for each class \n",
            "[0.         0.         0.         0.         0.         0.27272727\n",
            " 0.22089552 0.20338983 0.25806452 0.         0.         0.47554348\n",
            " 0.         0.         0.12280702 0.         0.05263158 0.3880597\n",
            " 0.33073323 0.38438438 0.40512821 0.         0.         0.61978546\n",
            " 0.97612387]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {4, 5, 6, 7, 8, 11, 16, 17, 18, 19, 20, 23, 24}\n",
            "test_f1(macro, micro) (0.30,0.96) | test_avg_loss 0.16 | f1 for each class\n",
            "[0.         0.         0.         0.         0.125      0.58064516\n",
            " 0.29411765 0.6        0.48275862 0.         0.70470756 0.\n",
            " 0.         0.         0.         0.28571429 0.7394958  0.17582418\n",
            " 0.6097561  0.55491329 0.         0.79156909 0.9831621 ]\n",
            "epoch 4/30 | train_f1(macro, micro) (0.28,0.96) | train_epoch_avg_loss 0.160| f1 for each class \n",
            "[0.         0.         0.         0.         0.0625     0.61135371\n",
            " 0.375      0.54545455 0.44       0.         0.         0.61338123\n",
            " 0.         0.         0.27083333 0.         0.10810811 0.59610028\n",
            " 0.46308725 0.62820513 0.57736721 0.         0.03174603 0.76864888\n",
            " 0.98322243]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {1, 2, 5, 6, 7, 8, 11, 14, 16, 17, 18, 19, 20, 23, 24}\n",
            "test_f1(macro, micro) (0.39,0.97) | test_avg_loss 0.13 | f1 for each class\n",
            "[0.         0.         0.4        0.         0.         0.64285714\n",
            " 0.19230769 0.66666667 0.82352941 0.         0.7183908  0.\n",
            " 0.         0.6        0.         0.28571429 0.74725275 0.53383459\n",
            " 0.66666667 0.90163934 0.         0.83168317 0.98666055]\n",
            "epoch 5/30 | train_f1(macro, micro) (0.37,0.97) | train_epoch_avg_loss 0.117| f1 for each class \n",
            "[0.         0.         0.07692308 0.         0.42424242 0.71489362\n",
            " 0.41509434 0.65625    0.70038911 0.         0.         0.69526363\n",
            " 0.         0.         0.42990654 0.         0.42105263 0.70080863\n",
            " 0.53682488 0.76452599 0.70873786 0.         0.21276596 0.82641899\n",
            " 0.98737472]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {2, 5, 6, 7, 8, 11, 14, 17, 18, 19, 20, 22, 23, 24}\n",
            "test_f1(macro, micro) (0.43,0.97) | test_avg_loss 0.12 | f1 for each class\n",
            "[0.         0.         0.4        0.         0.         0.8\n",
            " 0.5        0.81481481 0.78787879 0.         0.71766849 0.\n",
            " 0.         0.875      0.         0.         0.796875   0.62275449\n",
            " 0.83870968 0.84297521 0.         0.8467354  0.98801316]\n",
            "epoch 6/30 | train_f1(macro, micro) (0.46,0.98) | train_epoch_avg_loss 0.082| f1 for each class \n",
            "[0.         0.         0.13333333 0.         0.44444444 0.81666667\n",
            " 0.6167147  0.78333333 0.77366255 0.         0.         0.76301616\n",
            " 0.22222222 0.125      0.63793103 0.         0.55555556 0.86813187\n",
            " 0.72572402 0.84076433 0.82983683 0.         0.4        0.86768603\n",
            " 0.99069793]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {2, 4, 5, 6, 7, 8, 11, 13, 16, 17, 18, 19, 20, 23, 24}\n",
            "test_f1(macro, micro) (0.40,0.97) | test_avg_loss 0.12 | f1 for each class\n",
            "[0.         0.         0.         0.         0.18181818 0.79411765\n",
            " 0.71264368 0.73333333 0.66666667 0.         0.64888889 0.\n",
            " 0.         0.         0.         0.5        0.85981308 0.7283237\n",
            " 0.66666667 0.8115942  0.         0.83306581 0.9877026 ]\n",
            "epoch 7/30 | train_f1(macro, micro) (0.46,0.98) | train_epoch_avg_loss 0.079| f1 for each class \n",
            "[0.2        0.         0.24       0.         0.63157895 0.8559322\n",
            " 0.65294118 0.7804878  0.84375    0.         0.         0.80410531\n",
            " 0.11111111 0.09090909 0.30252101 0.         0.72727273 0.90710383\n",
            " 0.64189189 0.81528662 0.85581395 0.         0.17391304 0.88044371\n",
            " 0.99147905]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {2, 4, 5, 6, 7, 8, 11, 14, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "test_f1(macro, micro) (0.52,0.98) | test_avg_loss 0.10 | f1 for each class\n",
            "[0.         0.         0.8        0.         0.5        0.87323944\n",
            " 0.68235294 0.92857143 0.75324675 0.         0.78095238 0.\n",
            " 0.         0.9375     0.         0.53333333 0.86440678 0.75824176\n",
            " 0.87878788 0.83333333 0.         0.87369985 0.99063553]\n",
            "epoch 8/30 | train_f1(macro, micro) (0.51,0.99) | train_epoch_avg_loss 0.057| f1 for each class \n",
            "[0.         0.33333333 0.17391304 0.         0.47058824 0.88983051\n",
            " 0.74924471 0.93650794 0.88537549 0.         0.         0.84432481\n",
            " 0.24       0.41666667 0.49122807 0.         0.6        0.91208791\n",
            " 0.81983471 0.88343558 0.90160183 0.         0.2962963  0.91251793\n",
            " 0.9940676 ]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {2, 4, 5, 6, 7, 8, 11, 13, 16, 17, 18, 19, 20, 23, 24}\n",
            "test_f1(macro, micro) (0.43,0.98) | test_avg_loss 0.10 | f1 for each class\n",
            "[0.         0.         0.4        0.         0.         0.82926829\n",
            " 0.68131868 0.75       0.79411765 0.         0.77830941 0.\n",
            " 0.66666667 0.         0.         0.         0.85950413 0.75555556\n",
            " 0.6122449  0.9        0.         0.8650261  0.99004819]\n",
            "epoch 9/30 | train_f1(macro, micro) (0.52,0.99) | train_epoch_avg_loss 0.056| f1 for each class \n",
            "[0.22222222 0.         0.38709677 0.         0.76470588 0.88607595\n",
            " 0.7195122  0.76335878 0.82539683 0.         0.31578947 0.8758465\n",
            " 0.         0.35294118 0.12121212 0.         0.84210526 0.92473118\n",
            " 0.76065574 0.89655172 0.85106383 0.         0.61016949 0.92525667\n",
            " 0.99456359]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {4, 5, 6, 7, 8, 11, 13, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "test_f1(macro, micro) (0.44,0.97) | test_avg_loss 0.10 | f1 for each class\n",
            "[0.         0.         0.         0.         0.66666667 0.94594595\n",
            " 0.67346939 0.57142857 0.84931507 0.         0.74306839 0.\n",
            " 0.35294118 0.         0.         0.55555556 0.94117647 0.68393782\n",
            " 0.11111111 0.87096774 0.22222222 0.83710407 0.98944914]\n",
            "epoch 10/30 | train_f1(macro, micro) (0.52,0.99) | train_epoch_avg_loss 0.050| f1 for each class \n",
            "[0.         0.         0.27272727 0.         0.86486486 0.88983051\n",
            " 0.75659824 0.81300813 0.828125   0.         0.13333333 0.88617886\n",
            " 0.28571429 0.10526316 0.4        0.         0.87804878 0.89502762\n",
            " 0.79061977 0.89677419 0.85977011 0.         0.53333333 0.92845327\n",
            " 0.99496187]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {2, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 23, 24}\n",
            "test_f1(macro, micro) (0.54,0.98) | test_avg_loss 0.10 | f1 for each class\n",
            "[0.         0.         0.8        0.         0.5        0.88888889\n",
            " 0.66666667 0.75       0.72222222 0.         0.76682316 0.\n",
            " 1.         0.84615385 0.         0.42857143 0.9        0.70344828\n",
            " 0.8        0.81034483 0.         0.86343612 0.9895055 ]\n",
            "epoch 11/30 | train_f1(macro, micro) (0.70,0.99) | train_epoch_avg_loss 0.032| f1 for each class \n",
            "[0.33333333 0.88888889 0.85714286 0.         0.86486486 0.94117647\n",
            " 0.88343558 0.91935484 0.91129032 0.         0.52631579 0.8963984\n",
            " 0.4        0.86956522 0.90756303 0.         0.95238095 0.96174863\n",
            " 0.90420168 0.9148265  0.89361702 0.         0.71641791 0.94356659\n",
            " 0.99629552]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {1, 2, 4, 5, 6, 7, 8, 11, 12, 13, 14, 16, 17, 18, 19, 20, 23, 24}\n",
            "test_f1(macro, micro) (0.64,0.98) | test_avg_loss 0.11 | f1 for each class\n",
            "[0.         1.         0.75       0.         0.4        0.88607595\n",
            " 0.74226804 0.92857143 0.75       0.         0.77490775 1.\n",
            " 1.         0.96551724 0.         0.33333333 0.88888889 0.73809524\n",
            " 0.85245902 0.85271318 0.         0.85057471 0.98995304]\n",
            "epoch 12/30 | train_f1(macro, micro) (0.65,0.99) | train_epoch_avg_loss 0.033| f1 for each class \n",
            "[0.         0.75       0.64285714 0.         0.82352941 0.92436975\n",
            " 0.82035928 0.87394958 0.8984375  0.         0.25       0.91276978\n",
            " 0.45454545 0.8        0.83760684 0.66666667 0.82051282 0.86666667\n",
            " 0.89517471 0.93081761 0.90610329 0.         0.26666667 0.95320197\n",
            " 0.99649826]\n",
            "gt tag unique 數: {0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24}\n",
            "pred tag unique 數: {0, 2, 4, 5, 6, 7, 8, 11, 12, 13, 16, 17, 18, 19, 20, 23, 24}\n",
            "test_f1(macro, micro) (0.53,0.98) | test_avg_loss 0.12 | f1 for each class\n",
            "[0.66666667 0.         0.4        0.         0.53333333 0.88607595\n",
            " 0.67241379 0.8125     0.76086957 0.         0.77530017 0.36363636\n",
            " 0.8        0.         0.         0.625      0.88695652 0.70935961\n",
            " 0.76056338 0.81578947 0.         0.8381877  0.98976362]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hHf_PDUEPkg"
      },
      "source": [
        "# 訓練成果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAFcxDva2iGf"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows= 2, ncols= 1, figsize= (10,8), sharex= True)\r\n",
        "ax1.plot([*range(1, stop_epoch+1)], list(train_loss.values()), label= 'train loss')\r\n",
        "ax1.plot([*range(1, stop_epoch+1)], list(test_loss.values()), label= 'test loss')\r\n",
        "ax1.legend()\r\n",
        "\r\n",
        "ax2.plot([*range(1, stop_epoch+1)], list(train_f1.values()), label= 'train f1 (macro)')\r\n",
        "ax2.plot([*range(1, stop_epoch+1)], list(test_f1.values()), label= 'test f1 (macro)')\r\n",
        "ax2.legend()\r\n",
        "plt.savefig('fig.jpeg')\r\n",
        "plt.tight_layout()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pWa4TQhw5PI"
      },
      "source": [
        "# torch.save(model, 'ner_model_batch32_wup4000_lstmhd256_lr5e-5_40epoch_adam_wde-3.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1UTlctlqT8g"
      },
      "source": [
        "# !cp 'ner_model_batch32_wup4000_lstmhd256_lr5e-5_40epoch_adam_wde-3.pt' '/content/drive/My Drive/python檔/aicup'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvc6xWNC85De"
      },
      "source": [
        "with open('/content/drive/My Drive/python檔/aicup/test_input.data', 'r', encoding= 'utf-8') as f:\r\n",
        "    data = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApTe5VvOqO-P"
      },
      "source": [
        "pred_outs = test_output(data= data, model= model, tokenizer=tokenizer, batch_size= 32).pred_out_tsv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4qmYVsB9GYp"
      },
      "source": [
        "pred_outs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}