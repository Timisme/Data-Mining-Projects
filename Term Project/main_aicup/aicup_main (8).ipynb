{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "aicup_main",
      "provenance": [],
      "collapsed_sections": [
        "a6sIQ8wbXGnc",
        "cfh42ir6I9FM",
        "5rPf2heKjCd7",
        "-NSLpK9g7bp5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk66K6AWaW1q"
      },
      "source": [
        "# !cp drive/MyDrive/python檔/aicup/run/dataset.py .\r\n",
        "# !cp drive/MyDrive/python檔/aicup/run "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GHLkq1bcEox",
        "outputId": "d59fa276-d8c3-48e5-ae29-a31bdec27c47"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59rSMkbocYfI"
      },
      "source": [
        "import sys\r\n",
        "sys.path.insert(0,\"/content/drive/My Drive/python檔/aicup/run\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3D3ktlwcq1p",
        "outputId": "b26de5bc-a056-40fe-c2fb-8dea84f70fbb"
      },
      "source": [
        "pip install transformers==3"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.1.94)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3) (20.7)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6RsrpxBWe1H",
        "outputId": "27a525ad-58b7-41d3-aea3-4efb85f031d0"
      },
      "source": [
        "pip install pytorch-crf"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.6/dist-packages (0.7.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6eL0_ORc3lj",
        "outputId": "01906fe7-84d8-447d-9d7e-04f06869fb7c"
      },
      "source": [
        "pip install pytorch_warmup"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_warmup in /usr/local/lib/python3.6/dist-packages (0.0.4)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_warmup) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1->pytorch_warmup) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNw4yCiwa1e2"
      },
      "source": [
        "# from dataset import bert_stc_dataset\r\n",
        "# from model2 import model_crf\r\n",
        "from train import train\r\n",
        "# from txt_preprocess2 import preprocess2\r\n",
        "import re\r\n",
        "\r\n",
        "from transformers import BertModel, BertTokenizer, get_cosine_schedule_with_warmup\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torchsummary import summary\r\n",
        "from torchcrf import CRF\r\n",
        "import pytorch_warmup as warmup\r\n",
        "# from torch.autograd import Variable\r\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Qnou0tbJfb",
        "outputId": "ffbd6d1b-123e-45ff-9039-5925dc719203"
      },
      "source": [
        "file_path = '/content/drive/My Drive/python檔/aicup/run/data/train_input.data'\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "print('{} is being used'.format(device))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda is being used\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6sIQ8wbXGnc"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo2E2d2TU7dG"
      },
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "import torch\r\n",
        "\r\n",
        "class bert_stc_dataset(Dataset):\r\n",
        "    \r\n",
        "    def __init__(self, stcs, labels, tokenizer, max_length):\r\n",
        "        \r\n",
        "        self.stcs = stcs\r\n",
        "        self.labels = labels\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.max_length = max_length\r\n",
        "        self.pad_labels = []\r\n",
        "\r\n",
        "        # 已經在preprocess2 做完label 0 padding\r\n",
        "        for i in range(len(labels)):\r\n",
        "            temp_label = [0]*max_length\r\n",
        "            temp_label[:len(labels[i])] = labels[i]\r\n",
        "            self.pad_labels.append(temp_label)\r\n",
        "            \r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.stcs)\r\n",
        "    \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        \r\n",
        "        txt = str(self.stcs[idx])\r\n",
        "        \r\n",
        "        txt = ' '.join(list(txt))\r\n",
        "        # print(txt)\r\n",
        "        \r\n",
        "        encoding = self.tokenizer.encode_plus(\r\n",
        "            txt,\r\n",
        "#             truncation= True,\r\n",
        "            max_length= self.max_length,\r\n",
        "            padding = 'max_length',\r\n",
        "            add_special_tokens=False,\r\n",
        "#             pad_to_multiple_of=True,\r\n",
        "            return_attention_mask= True,\r\n",
        "            return_token_type_ids= False,\r\n",
        "            return_tensors='pt')\r\n",
        "        \r\n",
        "        return {\r\n",
        "            'input_ids': encoding['input_ids'].flatten(),\r\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\r\n",
        "            'labels' : torch.tensor(self.pad_labels[idx], dtype= torch.long)\r\n",
        "        }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfh42ir6I9FM"
      },
      "source": [
        "# preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr4UxdsTI8Y-"
      },
      "source": [
        "class preprocess2():\r\n",
        "  def __init__(self, data):\r\n",
        "    self.data = data\r\n",
        "    self.article_id_list = list()\r\n",
        "    self.data_list= list()\r\n",
        "    data_list_tmp = list()\r\n",
        "    idx = 0\r\n",
        "\r\n",
        "    for row in data:\r\n",
        "      data_tuple = tuple()\r\n",
        "      if row == '\\n':\r\n",
        "        self.article_id_list.append(idx)\r\n",
        "        idx+=1\r\n",
        "        self.data_list.append(data_list_tmp)\r\n",
        "        data_list_tmp = []\r\n",
        "\r\n",
        "      else:\r\n",
        "        row = row.strip('\\n').split(' ')\r\n",
        "\r\n",
        "        if (row[0] == '，') & (len(data_list_tmp) > 128):\r\n",
        "          self.article_id_list.append(idx)\r\n",
        "          self.data_list.append(data_list_tmp)\r\n",
        "          data_list_tmp= []\r\n",
        "\r\n",
        "        elif row[0] in ['。', '？','！','～','：']:\r\n",
        "          self.article_id_list.append(idx)\r\n",
        "          self.data_list.append(data_list_tmp)\r\n",
        "          data_list_tmp= []\r\n",
        "        \r\n",
        "        elif row[0] in ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']:\r\n",
        "          data_tuple = (row[0].lower(), row[1])\r\n",
        "          data_list_tmp.append(data_tuple)\r\n",
        "\r\n",
        "        elif row[0] not in ['摁','嗯','啦','喔','欸','啊','齁','嘿','…','...','、','‧','，']:\r\n",
        "          data_tuple = (row[0], row[1])\r\n",
        "          data_list_tmp.append(data_tuple)\r\n",
        "        #data_list_tmp 儲存暫時的data_tuple(token,label)\r\n",
        "    if len(data_list_tmp) != 0:\r\n",
        "      self.data_list.append(data_list_tmp)\r\n",
        "\r\n",
        "  def get_stc_label(self):\r\n",
        "    all_stcs = list()\r\n",
        "    all_labels = list()\r\n",
        "\r\n",
        "    for article_txt_tuple, article_id in zip(self.data_list, self.article_id_list):\r\n",
        "\r\n",
        "      txt_len = len(article_txt_tuple) #(文章數，每個文章對應的總字數) (word, label)\r\n",
        "      stc = str() #存字數= max_stc_len的字串\r\n",
        "      # labels = ['[CLS]'] # 存該字串對應的label # pytorch crf不需要\r\n",
        "      labels = []\r\n",
        "\r\n",
        "      for idx, (word, label) in enumerate(article_txt_tuple):\r\n",
        "\r\n",
        "        stc += word\r\n",
        "        labels.append(label)\r\n",
        "\r\n",
        "      all_stcs.append(stc)\r\n",
        "      all_labels.append(labels)\r\n",
        "\r\n",
        "    all_stcs_clean = []\r\n",
        "    all_labels_clean = []\r\n",
        "\r\n",
        "    idx = 0\r\n",
        "    for stc, label in zip(all_stcs,all_labels): #前處理 & downsampling\r\n",
        "      \r\n",
        "      stc_clean = re.sub(r'(醫師)|(個管師)|(民眾)|(家屬)|(護理師)', '', stc)\r\n",
        "      # print(stc, stc_clean, label)\r\n",
        "      if (len(stc_clean)>=2) & (len(set(label)) >= 2):  \r\n",
        "        # print(stc_clean, stc)\r\n",
        "        all_stcs_clean.append(stc)\r\n",
        "        all_labels_clean.append(label)\r\n",
        "    \r\n",
        "      elif (len(stc_clean)>=2) & (((idx+1) % 5) == 0):\r\n",
        "        all_stcs_clean.append(stc)\r\n",
        "        all_labels_clean.append(label)\r\n",
        "      idx += 1\r\n",
        "\r\n",
        "    # 這一步就先把label 做 0 padding\r\n",
        "\r\n",
        "    # max_length = len(max(all_stcs_clean, key=len)) \r\n",
        "    # pad_labels = []\r\n",
        "\r\n",
        "    # for i in range(len(all_labels_clean)):\r\n",
        "    #   temp_label = [0]*max_length\r\n",
        "    #   temp_label[:len(all_labels_clean[i])] = all_labels_clean[i]\r\n",
        "    #   pad_labels.append(temp_label)\r\n",
        "\r\n",
        "    # print('sentences總數: {}'.format(len(all_stcs_clean)))\r\n",
        "    # print('labels總數: {}'.format(len(all_labels_clean)))\r\n",
        "    # return all_stcs_clean, pad_labels\r\n",
        "    return all_stcs_clean, all_labels_clean\r\n",
        "\r\n",
        "  def tag2id(self, stcs_label):\r\n",
        "\r\n",
        "    all_label = list()\r\n",
        "    for stc_label in stcs_label:\r\n",
        "      for label in stc_label:\r\n",
        "        all_label.append(label)\r\n",
        "\r\n",
        "    labels_set = sorted(set(all_label))\r\n",
        "    tag2id_dict = {}\r\n",
        "    # tag2id_dict = {'[PAD]':0} #固定將PAD id設為0\r\n",
        "\r\n",
        "    # labels_set.remove('[PAD]')\r\n",
        "\r\n",
        "    for idx, label in enumerate(labels_set):\r\n",
        "      tag2id_dict[label] = idx\r\n",
        "\r\n",
        "    return tag2id_dict\r\n",
        "\r\n",
        "  def label_to_ids(self, tag_to_id, raw_labels):\r\n",
        "\r\n",
        "    label2id = []\r\n",
        "    for stc_labels in raw_labels:\r\n",
        "      stc_label_ids = [tag_to_id[label] for label in stc_labels]\r\n",
        "      label2id.append(stc_label_ids)\r\n",
        "    return label2id\r\n",
        "\r\n",
        "  def get_stcs_label2ids(self):\r\n",
        "\r\n",
        "    stcs, labels = self.get_stc_label()\r\n",
        "    tag2id = self.tag2id(stcs_label= labels)\r\n",
        "    labels_ids= self.label_to_ids(tag_to_id= tag2id, raw_labels= labels)\r\n",
        "\r\n",
        "    return stcs, labels_ids"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpu1QT-5tYsi"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48kSvtOFtQRg"
      },
      "source": [
        "class model_crf(nn.Module):\r\n",
        "\tdef __init__(self, n_tags, hidden_dim=768, batchsize= 32):\r\n",
        "\t\tsuper(model_crf, self).__init__()\r\n",
        "\t\tself.n_tags = n_tags\r\n",
        "\t\tself.lstm =  nn.LSTM(bidirectional=True, num_layers=2, input_size=768, hidden_size=hidden_dim//2, dropout= 0.3, batch_first=True)\t\t\r\n",
        "\t\tself.hidden_dim = hidden_dim\r\n",
        "\t\tself.fc = nn.Linear(hidden_dim, self.n_tags)\r\n",
        "\t\tself.bert = BertModel.from_pretrained('bert-base-chinese')\r\n",
        "\r\n",
        "\t\t# for param in self.bert.parameters():\r\n",
        "\t\t# \tparam.requires_grad = False\r\n",
        "\t\t# self.bert.eval()  # 知用来取bert embedding\r\n",
        "\r\n",
        "\t\tself.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "\t\tself.CRF = CRF(n_tags, batch_first= True)\r\n",
        "\t\tself.hidden = self.init_hidden(batchsize)\r\n",
        "\r\n",
        "\tdef init_hidden(self, batch_size):\r\n",
        "\t\treturn (torch.randn(2*2, batch_size, self.hidden_dim // 2).to(self.device),\r\n",
        "\t\t\t\ttorch.randn(2*2, batch_size, self.hidden_dim // 2).to(self.device))\r\n",
        "\r\n",
        "\tdef forward(self, input_ids, attention_mask, tags):\r\n",
        "\r\n",
        "\t\tbatch_size = input_ids.size(0)\r\n",
        "\t\tmax_seq_len = input_ids.size(1)\r\n",
        "\t\tbert_output, _  = self.bert(input_ids.long(), attention_mask)\r\n",
        "\t\t\r\n",
        "\t\tseq_len = torch.sum(attention_mask, dim= 1).cpu().int()\r\n",
        "\t\t# print(seq_len)\r\n",
        "\t\tpack_input = pack_padded_sequence(input= bert_output, lengths= seq_len, batch_first= True, enforce_sorted= False)\r\n",
        "\t\tpacked_lstm_out, _ = self.lstm(pack_input,self.init_hidden(batch_size= batch_size))\r\n",
        "\t\tlstm_enc, _=  pad_packed_sequence(packed_lstm_out, batch_first=True, padding_value=0)\r\n",
        "\t\t# print(lstm_enc.size())\r\n",
        "\t\tlstm_feats = self.fc(lstm_enc)\r\n",
        "\r\n",
        "\t\tlstm_max_seq_len = lstm_feats.size(1)\r\n",
        "\t\tpad = torch.zeros(size=(batch_size, max_seq_len-lstm_max_seq_len, self.n_tags), dtype= torch.float).to(self.device)\r\n",
        "\t\tlstm_feats= torch.cat((lstm_feats, pad), dim= 1)\r\n",
        "  \r\n",
        "\t\tlstm_feats[:,:,:5] = lstm_feats[:,:,:5]*1000\r\n",
        "    lstm_feats[:,:,5:9] = lstm_feats[:,:,:9]*100\r\n",
        "    lstm_feats[:,:,9:11] = lstm_feats[:,:,9:11]*1000\r\n",
        "    lstm_feats[:,:11] = lstm_feats[:,:11]*100\r\n",
        "    lstm_feats[:,:,12:17] = lstm_feats[:,:,12:17]*1000\r\n",
        "    lstm_feats[:,:,12:17] = lstm_feats[:,:,12:17]*1000\r\n",
        "    lstm_feats[:,:,17:21] = lstm_feats[:,:,17:21]*100\r\n",
        "    lstm_feats[:,:,21:23] = lstm_feats[:,:,21:23]*1000\r\n",
        "    lstm_feats[:,:23] = lstm_feats[:,:23]*100\r\n",
        "\r\n",
        "\t\tloss = -self.CRF(lstm_feats, tags, attention_mask.bool(), reduction= 'token_mean')\r\n",
        "\t\tpred_seqs = self.CRF.decode(emissions= lstm_feats, mask= attention_mask.bool())\r\n",
        "  \r\n",
        "\t\treturn loss, pred_seqs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F72vtVN9tbhD"
      },
      "source": [
        "# 參數設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zTvRlmOldMGf",
        "outputId": "54863a16-3542-4320-b9e8-89070b2ff3fb"
      },
      "source": [
        "# ---------------前處理---------------\r\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\r\n",
        "\tdata=f.readlines()#.encode('utf-8').decode('utf-8-sig')\r\n",
        "\r\n",
        "preprocessor = preprocess2(data)\r\n",
        "\r\n",
        "stcs, original_labels= preprocessor.get_stc_label()\r\n",
        "stcs, labels = preprocessor.get_stcs_label2ids()\r\n",
        "tag2id_dict = preprocessor.tag2id(original_labels)\r\n",
        "n_tags = len(tag2id_dict)\r\n",
        "print(tag2id_dict)\r\n",
        "print('tags數: {}'.format(n_tags))\r\n",
        "\r\n",
        "gt_tags = [tag for label in labels for tag in label]\r\n",
        "\r\n",
        "for tag in set(gt_tags):\r\n",
        "  print('{}|{}'.format(tag, gt_tags.count(tag)/len(gt_tags)))\r\n",
        "# plt.hist(gt_tags)\r\n",
        "plt.hist([len(stc) for stc in stcs])\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\r\n",
        "max_length = len(max(stcs, key=len)) \r\n",
        "print('max_stc_length', max_length)\r\n",
        "\r\n",
        "# ---------------模型---------------\r\n",
        "batchsize= 32\r\n",
        "model = model_crf(n_tags= n_tags, hidden_dim= 256, batchsize= batchsize).to(device)\r\n",
        "# print(summary(model,[(128, 300), (128,300)]))\r\n",
        "\r\n",
        "train_x, test_x, train_y, test_y = train_test_split(stcs, labels, test_size= 0.2, shuffle= True, random_state= 42)\r\n",
        "\r\n",
        "train_dataset = bert_stc_dataset(stcs= train_x, labels= train_y, tokenizer= tokenizer, max_length= max_length)\r\n",
        "print(train_x[0])\r\n",
        "print(train_dataset[0]['input_ids'])\r\n",
        "print(train_dataset[0]['labels'])\r\n",
        "test_dataset = bert_stc_dataset(stcs= test_x, labels= test_y, tokenizer= tokenizer, max_length= max_length)\r\n",
        "\r\n",
        "print('training stcs 總數: {}'.format(len(train_dataset)))\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size= batchsize, shuffle= True, num_workers= 0)\r\n",
        "test_dataloader = DataLoader(test_dataset, batch_size= batchsize, shuffle= False, num_workers= 0)\r\n",
        "\r\n",
        "num_epochs = 40\r\n",
        "num_iteration = len(train_dataloader)\r\n",
        "print('num_iteration',num_iteration)\r\n",
        "total_iter = num_iteration * num_epochs\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4, weight_decay= 0.001)\r\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=2000, num_training_steps=total_iter)\r\n",
        "# warmup_scheduler = warmup.ExponentialWarmup(optimizer, warmup_period=150)\r\n",
        "\r\n",
        "# ---------------訓練---------------\r\n",
        "# train_model = train(model= model, optimizer= optimizer, train_loader= train_dataloader, test_loader= 0, num_epochs= 5, device= device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-ID': 0, 'B-clinical_event': 1, 'B-contact': 2, 'B-education': 3, 'B-family': 4, 'B-location': 5, 'B-med_exam': 6, 'B-money': 7, 'B-name': 8, 'B-organization': 9, 'B-profession': 10, 'B-time': 11, 'I-ID': 12, 'I-clinical_event': 13, 'I-contact': 14, 'I-education': 15, 'I-family': 16, 'I-location': 17, 'I-med_exam': 18, 'I-money': 19, 'I-name': 20, 'I-organization': 21, 'I-profession': 22, 'I-time': 23, 'O': 24}\n",
            "tags數: 25\n",
            "0|0.00014043957587248087\n",
            "1|8.777473492030054e-05\n",
            "2|0.00033354399269714205\n",
            "3|5.2664840952180324e-05\n",
            "4|0.0004388736746015027\n",
            "5|0.0028263464644336773\n",
            "6|0.003862088336493224\n",
            "7|0.0013692858647566884\n",
            "8|0.002931676146338038\n",
            "9|1.755494698406011e-05\n",
            "10|0.0002282143107927814\n",
            "11|0.025173793975142196\n",
            "12|0.0002633242047609016\n",
            "13|0.0002633242047609016\n",
            "14|0.0013868408117407485\n",
            "15|5.2664840952180324e-05\n",
            "16|0.000491538515553683\n",
            "17|0.0042131872761744255\n",
            "18|0.006758654588863142\n",
            "19|0.003405659714907661\n",
            "20|0.00491538515553683\n",
            "21|3.510989396812022e-05\n",
            "22|0.0007021978793624043\n",
            "23|0.0549645390070922\n",
            "24|0.8850853170423425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "max_stc_length 156\n",
            "在木葉忍者村木葉路上\n",
            "tensor([1762, 3312, 5864, 2556, 5442, 3333, 3312, 5864, 6662,  677,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
            "tensor([24,  5, 17, 17, 17, 17,  5, 17, 17, 24,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
            "training stcs 總數: 3262\n",
            "num_iteration 102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rPf2heKjCd7"
      },
      "source": [
        "# test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfjiko8GjBZD"
      },
      "source": [
        "def test(model, test_dataloader, device):\r\n",
        "\r\n",
        "  preds_epoch = []\r\n",
        "  gts_epoch = []\r\n",
        "  epoch_loss = 0\r\n",
        "  iteration = 0\r\n",
        "\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  for idx, batch_dict in enumerate(test_dataloader):\r\n",
        "\r\n",
        "    # print('idx: ',idx+1)\r\n",
        "    input_ids = batch_dict['input_ids'].to(device)\r\n",
        "    attention_mask = batch_dict['attention_mask'].to(device)\r\n",
        "    labels = batch_dict['labels'].to(device)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "      loss, pred_labels = model(input_ids, attention_mask.bool(), labels)\r\n",
        "\r\n",
        "    # mask gt labels \r\n",
        "    labels = batch_dict['labels'].numpy()\r\n",
        "    masks = batch_dict['attention_mask'].numpy()\r\n",
        "\r\n",
        "    labels_nopad = []\r\n",
        "    for label , seq_mask in zip(labels, masks):\r\n",
        "\r\n",
        "      seq = [tag for tag, mask in zip(label, seq_mask) if mask == 1]\r\n",
        "      labels_nopad.append(seq)\r\n",
        "\r\n",
        "    # one dim array \r\n",
        "    preds= [tag for seq in pred_labels for tag in seq]\r\n",
        "    gts= [tag for seq in labels_nopad for tag in seq]\r\n",
        "\r\n",
        "    preds_epoch += preds\r\n",
        "    gts_epoch += gts\r\n",
        "\r\n",
        "    epoch_loss += loss.item()\r\n",
        "    iteration += 1\r\n",
        "\r\n",
        "  f1_macro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'macro')\r\n",
        "  f1_micro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'micro')\r\n",
        "  f1 = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= None)\r\n",
        "  avg_loss = epoch_loss / iteration\r\n",
        "  \r\n",
        "  print('test_f1(macro, micro) ({:.2f},{:.2f}) | test_avg_loss {:.2f} | f1 for each class{}'.format(f1_macro, f1_micro, avg_loss, f1))\r\n",
        "\r\n",
        "  return f1_macro, f1_micro, avg_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NSLpK9g7bp5"
      },
      "source": [
        "# test out function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2yAR0_67gGZ"
      },
      "source": [
        "class test_output():\r\n",
        "\tdef __init__(self, data, model, tokenizer, batch_size):\r\n",
        "\r\n",
        "\t\tself.model = model\r\n",
        "\t\tself.tokenizer = tokenizer\r\n",
        "\t\tself.batch_size = batch_size\r\n",
        "\t\tself.data_list = []\r\n",
        "\t\tself.word_id = []\r\n",
        "\t\tself.word_article_id = [] \r\n",
        "\t\tarticle_id = 0\r\n",
        "\t\tword_id = 0\r\n",
        "\t\tdata_list_tmp = []\r\n",
        "\t\tarticle_id_tmp = []\r\n",
        "\t\tword_id_tmp = []\r\n",
        "\t\t\r\n",
        "\t\tfor row in data:\r\n",
        "\t\t\t\r\n",
        "\t\t\tdata_tuple = tuple()\r\n",
        "\t\t\tif row == '\\n':\r\n",
        "\t\t\t\t\r\n",
        "\t\t\t\tarticle_id += 1 \r\n",
        "\t\t\t\tword_id = 0\r\n",
        "\t\t\t\tself.word_id.append(word_id)\r\n",
        "\t\t\t\tself.word_article_id.append(article_id_tmp)\r\n",
        "\t\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\t\t\t\tdata_list_tmp = []\r\n",
        "\t\t\t\tarticle_id_tmp = []\r\n",
        "\t\t\t\tword_id_tmp = []\r\n",
        "\r\n",
        "\t\t\telse:\r\n",
        "\t\t\t\t\r\n",
        "\t\t\t\trow = row.strip('\\n').split(' ')\r\n",
        "\r\n",
        "\t\t\t\tif row[0] in ['。', '？','！','，','～','：','‧']:\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\t\tself.word_id.append(word_id_tmp)\r\n",
        "\t\t\t\t\tself.word_article_id.append(article_id_tmp)\r\n",
        "\t\t\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\t\t\t\t\tdata_list_tmp = []\r\n",
        "\t\t\t\t\tarticle_id_tmp = []\r\n",
        "\t\t\t\t\tword_id_tmp = []\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\telif row[0] in ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']:\r\n",
        "\t\t\t\t\t  \r\n",
        "\t\t\t\t\tdata_tuple = (row[0].lower(), article_id, word_id)\r\n",
        "\t\t\t\t\tdata_list_tmp.append(data_tuple)\r\n",
        "\t\t\t\t\tarticle_id_tmp.append(article_id)\r\n",
        "\t\t\t\t\tword_id_tmp.append(word_id)\r\n",
        "\r\n",
        "\t\t\t\telif row[0] not in ['摁','嗯','啦','喔','欸','啊','齁','嘿','…','...','、','，']:\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\t\tdata_tuple = (row[0], article_id, word_id)\r\n",
        "\t\t\t\t\tdata_list_tmp.append(data_tuple)\r\n",
        "\t\t\t\t\tarticle_id_tmp.append(article_id)\r\n",
        "\t\t\t\t\tword_id_tmp.append(word_id)\r\n",
        "\t\t\t\t\t\r\n",
        "\t\t\t\tword_id += 1\r\n",
        "\t\t\t\t\r\n",
        "\t\tif len(data_list_tmp) != 0:\r\n",
        "\t\t\tself.data_list.append(data_list_tmp)\r\n",
        "\t\t\tself.word_id.append(word_id_tmp)\r\n",
        "\t\t\tself.word_article_id.append(article_id_tmp)\r\n",
        "\t\t\t\r\n",
        "\tdef raw_output(self):\r\n",
        "\t\treturn self.data_list, self.word_id, self.word_article_id\r\n",
        "\r\n",
        "\tdef get_stcs(self):\r\n",
        "\t\t\r\n",
        "\t\tall_stcs = list()\r\n",
        "\t\tall_article_ids = list()\r\n",
        "\t\tall_word_ids = list()\r\n",
        "\r\n",
        "\t\tfor stc_list in self.data_list:\r\n",
        "\r\n",
        "\t\t\ttxt_len = len(stc_list) #(文章數，每個文章對應的總字數) (word, label)\r\n",
        "\t\t\tstc = str() #存字數= max_stc_len的字串\r\n",
        "\t\t\tarticle_ids = []\r\n",
        "\t\t\tword_ids = []\r\n",
        "\t\t\t\r\n",
        "\r\n",
        "\t\t\tfor idx, (word,article_id, word_id) in enumerate(stc_list):\r\n",
        "\r\n",
        "\t\t\t\tstc += word\r\n",
        "\t\t\t\tarticle_ids.append(article_id)\r\n",
        "\t\t\t\tword_ids.append(word_id)\r\n",
        "\r\n",
        "\t\t\tall_stcs.append(stc)\r\n",
        "\t\t\tall_article_ids.append(article_ids)\r\n",
        "\t\t\tall_word_ids.append(word_ids)\r\n",
        "\r\n",
        "\t\tassert len(all_stcs) > 0, 'all stcs len = 0' \r\n",
        "\r\n",
        "\t\tall_stcs_clean = []\r\n",
        "\t\tall_article_ids_clean = []\r\n",
        "\t\tall_word_ids_clean = []\r\n",
        "\t\tidx = 0\r\n",
        "\t\t\r\n",
        "\t\tfor stc, article_id, word_id in zip(all_stcs, all_article_ids, all_word_ids):\r\n",
        "\t\t\tstc_clean = re.sub(r'(醫師)|(個管師)|(民眾)|(家屬)|(護理師)', '', stc)\r\n",
        "\t\t\t# print(stc, stc_clean, label)\r\n",
        "\t\t\tif len(stc_clean) > 1:  \r\n",
        "\t\t\t# print(stc_clean, stc)\r\n",
        "\t\t\t\tall_stcs_clean.append(stc)\r\n",
        "\t\t\t\tall_article_ids_clean.append(article_id)\r\n",
        "\t\t\t\tall_word_ids_clean.append(word_id)\r\n",
        "\r\n",
        "\t\t\t# 這一步就先把label 做 0 padding\r\n",
        "\t\t\t\r\n",
        "\t\tmax_length = len(max(all_stcs_clean, key=len))\r\n",
        "\t\tassert max_length > 0, 'max length less than 1'\r\n",
        "\r\n",
        "\t\tprint('sentences總數: {}'.format(len(all_stcs_clean)))\r\n",
        "\t\t\t\r\n",
        "\t\t# return all_stcs_clean, all_article_ids_clean, all_word_ids_clean\r\n",
        "\r\n",
        "\t\tself.clean_stcs, self.clean_article_id, self.clean_word_id = [], [] ,[]\r\n",
        "\r\n",
        "\t\tfor stc, article_id, word_id in zip(stcs, article_ids, word_ids):\r\n",
        "\t\t#print(stc, article_id, word_id)\r\n",
        "\t\t\tif stc not in ['沒有','也沒有','哪個','那個','算了','不用','有','有有有','有嗎','一點點', '謝謝','不會','不好意思','對不對','好不好','要嗎','還好']:\r\n",
        "\t\t\t\tself.clean_stcs.append(stc)\r\n",
        "\t\t\t\tself.clean_article_id.append(article_id)\r\n",
        "\t\t\t\tself.clean_word_id.append(word_id)\r\n",
        "\t\treturn self.clean_stcs, self.clean_article_id, self.clean_word_id\r\n",
        "\r\n",
        "\tdef encoding(self):\r\n",
        "\t\t\r\n",
        "\t\tmax_len = max(len(txt) for txt in self.clean_stcs)\r\n",
        "\r\n",
        "\t\tencoding = self.tokenizer.batch_encode_plus(self.clean_stcs, \r\n",
        "\t\t\tpadding=True,\r\n",
        "\t\t\tadd_special_tokens=False,\r\n",
        "\t\t\treturn_attention_mask= True,\r\n",
        "\t\t\treturn_token_type_ids= False,\r\n",
        "\t\t\t#  is_split_into_words=True,\r\n",
        "\t\t\treturn_tensors='pt')\r\n",
        "\r\n",
        "\t\t# batch_size= 32\r\n",
        "\t\tpred_labels = []\r\n",
        "\r\n",
        "\t\tfor idx in range(int((len(clean_stcs)/self.batch_size))):\r\n",
        "\t\t\tinput= encoding['input_ids'][idx*self.batch_size:(idx+1)*self.batch_size].to(device)\r\n",
        "\t\t\tmask = encoding['attention_mask'][idx*self.batch_size:(idx+1)*self.batch_size].to(device)\r\n",
        "\t\t\ttags= torch.zeros((input.size(0),input.size(1)), dtype=torch.long).to(device)\r\n",
        "\t\t\t_, preds = model(input, mask, tags)\r\n",
        "\t\t\tfor pred in preds:\r\n",
        "\t\t\t\tpred_labels.append(pred)\r\n",
        "\r\n",
        "\t\tidx = int((len(clean_stcs)/self.batch_size))\r\n",
        "\t\tinput= encoding['input_ids'][idx*self.batch_size:].to(device)\r\n",
        "\t\tmask = encoding['attention_mask'][idx*self.batch_size:].to(device)\r\n",
        "\t\ttags= torch.zeros((input.size(0),input.size(1)), dtype=torch.long).to(device)\r\n",
        "\t\t_, preds = model(input, mask, tags)\r\n",
        "\t\tfor pred in preds:\r\n",
        "\t\t\tpred_labels.append(pred)\r\n",
        "\r\n",
        "\t\ttag2id = {'[PAD]': 0, 'B-ID': 1, 'B-clinical_event': 2, 'B-contact': 3, 'B-education': 4, 'B-family': 5, 'B-location': 6, 'B-med_exam': 7, 'B-money': 8, 'B-name': 9, 'B-organization': 10, 'B-profession': 11, 'B-time': 12, 'I-ID': 13, 'I-clinical_event': 14, 'I-contact': 15, 'I-education': 16, 'I-family': 17, 'I-location': 18, 'I-med_exam': 19, 'I-money': 20, 'I-name': 21, 'I-organization': 22, 'I-profession': 23, 'I-time': 24, 'O': 25}\r\n",
        "\t\tid2tag ={v:k for k, v in tag2id.items()}\r\n",
        "\r\n",
        "\t\tself.pred_labels_tag = []\r\n",
        "\t\tfor label in pred_labels:\r\n",
        "\t\t\tstc_label = [id2tag[id] for id in label]\r\n",
        "\t\t\tself.pred_labels_tag.append(stc_label)\r\n",
        "\r\n",
        "\t\treturn self.pred_labels_tag\r\n",
        "\r\n",
        "\tdef pred_out_tsv(self):\r\n",
        "\t\t\r\n",
        "\t\tclean_stcs, clean_article_id, clean_word_id = self.get_stcs()\r\n",
        "\t\tpred_labels_tag = self.encoding()\r\n",
        "\r\n",
        "\t\tentity_text = []\r\n",
        "\r\n",
        "\t\tfor stc, labels, article_id, word_id in zip(clean_stcs, pred_labels_tag, clean_article_id, clean_word_id):\r\n",
        "\r\n",
        "\t\t\tentity = str()\r\n",
        "\r\n",
        "\t\t\tstart_pos = 0\r\n",
        "\t\t\tend_pos = 0\r\n",
        "\t\t\tarticle = 0\r\n",
        "\r\n",
        "\t\t\tentity_type = str()\r\n",
        "\r\n",
        "\r\n",
        "\t\t\tfor idx, label in enumerate(labels):\r\n",
        "\t\t\t\tif bool(re.match(r'B-', label)):\r\n",
        "\t\t\t\t\tentity += list(stc)[idx]\r\n",
        "\t\t\t\t\tstart_pos = word_id[idx]\r\n",
        "\t\t\t\t\tarticle = article_id[idx]\r\n",
        "\t\t\t\t\tentity_type = label.split('B-')[1]\r\n",
        "\r\n",
        "\t\t\t\telif bool(re.match(r'I-', label)):\r\n",
        "\t\t\t\t\tentity += list(stc)[idx]\r\n",
        "\t\t\t\t\tend_pos= word_id[idx]\r\n",
        "\t\t\t\t\ttry:\r\n",
        "\t\t\t\t\t\tif (labels[idx+1] == 'O') & (entity_type!=''):\r\n",
        "\t\t\t\t\t\t\tentity_text.append((article, start_pos, end_pos, entity, entity_type))\r\n",
        "\r\n",
        "\t\t\t\t\t\t\tentity = str()\r\n",
        "\t\t\t\t\t\t\tstart_pos = 0\r\n",
        "\t\t\t\t\t\t\tend_pos = 0\r\n",
        "\t\t\t\t\t\t\tarticle = 0\r\n",
        "\t\t\t\t\t\t\tentity_type = str()\r\n",
        "\t\t\t\t\texcept:\r\n",
        "\t\t\t\t\t\tpass\r\n",
        "\t\twith open('test_output.tsv', 'w', encoding='utf-8',newline='\\n') as f:\r\n",
        "\t\t\twriter = csv.writer(f, delimiter='\\t')\r\n",
        "\t\t\twriter.writerow(['article_id','start_position', 'end_position', 'entity_text', 'entity_type'])\r\n",
        "\t\t\tfor (article, start_pos, end_pos, entity, entity_type) in entity_text:\r\n",
        "\t\t\t\twriter.writerow([str(article), str(start_pos), str(end_pos), str(entity), str(entity_type)])\r\n",
        "\r\n",
        "\t\treturn entity_text\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTZoeeJLtl9K"
      },
      "source": [
        "# 訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyZYZVdJhxFa",
        "outputId": "9d1afe11-5089-4806-d406-1425d084193e"
      },
      "source": [
        "train_loss = {}\r\n",
        "test_loss = {}\r\n",
        "train_f1 = {}\r\n",
        "test_f1 = {}\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "\r\n",
        "  preds_epoch = []\r\n",
        "  gts_epoch = []\r\n",
        "  epoch_loss = 0\r\n",
        "  iteration = 0\r\n",
        "\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  for idx, batch_dict in enumerate(train_dataloader):\r\n",
        "\r\n",
        "    # print('idx: ',idx+1)\r\n",
        "    input_ids = batch_dict['input_ids'].to(device)\r\n",
        "    attention_mask = batch_dict['attention_mask'].to(device)\r\n",
        "    labels = batch_dict['labels'].to(device)\r\n",
        "\r\n",
        "    loss, pred_labels = model(input_ids, attention_mask.bool(), labels)\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\r\n",
        "\r\n",
        "    optimizer.step()\r\n",
        "    scheduler.step()\r\n",
        "    model.zero_grad()\r\n",
        "\r\n",
        "    # mask gt labels \r\n",
        "    labels = batch_dict['labels'].numpy()\r\n",
        "    masks = batch_dict['attention_mask'].numpy()\r\n",
        "\r\n",
        "    labels_nopad = []\r\n",
        "    for label , seq_mask in zip(labels, masks):\r\n",
        "\r\n",
        "      seq = [tag for tag, mask in zip(label, seq_mask) if mask == 1]\r\n",
        "      labels_nopad.append(seq)\r\n",
        "\r\n",
        "    # one dim array \r\n",
        "    preds= [tag for seq in pred_labels for tag in seq]\r\n",
        "    gts= [tag for seq in labels_nopad for tag in seq]\r\n",
        "\r\n",
        "    preds_epoch += preds\r\n",
        "    gts_epoch += gts\r\n",
        "\r\n",
        "    epoch_loss += loss.item()\r\n",
        "    iteration += 1\r\n",
        "\r\n",
        "    # print('pred_labels', pred_labels)\r\n",
        "    # # print('preds:', preds)\r\n",
        "    # print('gts:',gts)\r\n",
        "    # print(round(f1_score(y_true= gts, y_pred= preds, average= 'macro'),2))\r\n",
        "    # # print(f1_score(y_true= gts, y_pred= preds, average= 'micro'))\r\n",
        "    # print(round(loss.item(),2))\r\n",
        "\r\n",
        "    # print('idx:{} loss:{}'.format(idx, loss))\r\n",
        "\r\n",
        "  f1_macro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'macro')\r\n",
        "  f1_micro = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= 'micro')\r\n",
        "  f1 = f1_score(y_true= gts_epoch, y_pred= preds_epoch, average= None)\r\n",
        "  avg_loss = epoch_loss / iteration\r\n",
        "  \r\n",
        "\r\n",
        "  print('epoch {}/{} | train_f1(macro, micro) ({:.2f},{:.2f}) | train_epoch_avg_loss {:.2f}| f1 for each class {}'.format(epoch+1, num_epochs, f1_macro, f1_micro, avg_loss, f1))\r\n",
        "\r\n",
        "  test_f1_macro, test_f1_micro, test_avg_loss = test(model= model, test_dataloader= test_dataloader, device= device)\r\n",
        "\r\n",
        "  train_loss[epoch+1] = avg_loss\r\n",
        "  test_loss[epoch+1] = test_avg_loss\r\n",
        "  train_f1[epoch+1] = f1_macro\r\n",
        "  test_f1[epoch+1] = test_f1_macro\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1/40 | train_f1(macro, micro) (0.01,0.02) | train_epoch_avg_loss 10.85| f1 for each class [0.00021573 0.         0.         0.         0.00062267 0.00334635\n",
            " 0.0091047  0.00525822 0.00845666 0.         0.00034376 0.0060652\n",
            " 0.         0.         0.         0.         0.         0.00431034\n",
            " 0.01908257 0.00541126 0.00137457 0.         0.         0.05970149\n",
            " 0.032471  ]\n",
            "test_f1(macro, micro) (0.01,0.04) | test_avg_loss 4.13 | f1 for each class[0.         0.         0.         0.         0.         0.\n",
            " 0.02941176 0.         0.         0.         0.         0.01293103\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.0295203  0.         0.         0.         0.         0.10707457\n",
            " 0.08482653]\n",
            "epoch 2/40 | train_f1(macro, micro) (0.03,0.26) | train_epoch_avg_loss 3.69| f1 for each class [3.86940750e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 2.91120815e-03 8.63309353e-03 8.63930886e-03 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.75735294e-02\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 2.45051838e-02 0.00000000e+00\n",
            " 2.65957447e-03 0.00000000e+00 0.00000000e+00 1.35766137e-01\n",
            " 4.27089804e-01]\n",
            "test_f1(macro, micro) (0.05,0.67) | test_avg_loss 2.93 | f1 for each class[0.         0.         0.         0.         0.         0.01724138\n",
            " 0.         0.         0.         0.         0.02941176 0.04324324\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.01793722 0.         0.01282051 0.         0.         0.26637233\n",
            " 0.81304659]\n",
            "epoch 3/40 | train_f1(macro, micro) (0.05,0.70) | train_epoch_avg_loss 2.80| f1 for each class [0.         0.         0.01049869 0.         0.         0.01571709\n",
            " 0.01919386 0.00569801 0.01321586 0.         0.00696864 0.05859873\n",
            " 0.         0.         0.         0.         0.01162791 0.\n",
            " 0.02140309 0.00358423 0.00275862 0.         0.         0.26641791\n",
            " 0.83721055]\n",
            "test_f1(macro, micro) (0.06,0.78) | test_avg_loss 2.33 | f1 for each class[0.         0.         0.         0.         0.         0.\n",
            " 0.015625   0.         0.         0.         0.         0.06708595\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.02870813 0.         0.         0.         0.         0.47045324\n",
            " 0.88456686]\n",
            "epoch 4/40 | train_f1(macro, micro) (0.06,0.79) | train_epoch_avg_loss 2.19| f1 for each class [0.         0.         0.         0.         0.         0.00483092\n",
            " 0.00434783 0.         0.00515464 0.         0.         0.13570823\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.04341927 0.00440529 0.         0.         0.         0.41641337\n",
            " 0.8908343 ]\n",
            "test_f1(macro, micro) (0.08,0.84) | test_avg_loss 1.70 | f1 for each class[0.         0.         0.         0.         0.         0.02597403\n",
            " 0.         0.03703704 0.         0.         0.         0.30560272\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.07048458 0.         0.         0.         0.         0.60876494\n",
            " 0.91735453]\n",
            "epoch 5/40 | train_f1(macro, micro) (0.08,0.83) | train_epoch_avg_loss 1.59| f1 for each class [0.         0.         0.         0.         0.         0.01595745\n",
            " 0.02863962 0.         0.         0.         0.         0.2502157\n",
            " 0.         0.         0.         0.         0.         0.02096436\n",
            " 0.1121751  0.04109589 0.00970874 0.         0.         0.57042665\n",
            " 0.91942875]\n",
            "test_f1(macro, micro) (0.12,0.87) | test_avg_loss 1.17 | f1 for each class[0.         0.         0.         0.         0.         0.02247191\n",
            " 0.16666667 0.         0.02739726 0.         0.         0.48071979\n",
            " 0.         0.         0.         0.         0.         0.03636364\n",
            " 0.42580645 0.28       0.01626016 0.         0.         0.7000771\n",
            " 0.94021935]\n",
            "epoch 6/40 | train_f1(macro, micro) (0.14,0.88) | train_epoch_avg_loss 1.07| f1 for each class [0.         0.         0.         0.         0.         0.03194888\n",
            " 0.07672634 0.06122449 0.03785489 0.         0.         0.41032258\n",
            " 0.         0.         0.28837209 0.         0.         0.12147505\n",
            " 0.29513344 0.33658537 0.12087912 0.         0.         0.69952867\n",
            " 0.94409157]\n",
            "test_f1(macro, micro) (0.20,0.90) | test_avg_loss 0.80 | f1 for each class[0.         0.         0.         0.         0.         0.06896552\n",
            " 0.31884058 0.16666667 0.16       0.         0.         0.6272\n",
            " 0.         0.         0.43478261 0.         0.         0.45801527\n",
            " 0.18518519 0.34951456 0.41573034 0.         0.         0.75017018\n",
            " 0.95946987]\n",
            "epoch 7/40 | train_f1(macro, micro) (0.19,0.91) | train_epoch_avg_loss 0.77| f1 for each class [0.         0.         0.         0.         0.         0.20394737\n",
            " 0.20396601 0.08695652 0.16901408 0.         0.         0.52599388\n",
            " 0.         0.         0.28571429 0.         0.03773585 0.33746898\n",
            " 0.31986532 0.42528736 0.34059406 0.         0.         0.7412478\n",
            " 0.9614367 ]\n",
            "test_f1(macro, micro) (0.27,0.93) | test_avg_loss 0.60 | f1 for each class[0.         0.         0.         0.         0.         0.44067797\n",
            " 0.33333333 0.43902439 0.44827586 0.         0.         0.66773676\n",
            " 0.         0.         0.19230769 0.         0.         0.57142857\n",
            " 0.59433962 0.71428571 0.61818182 0.         0.         0.81309091\n",
            " 0.97345851]\n",
            "epoch 8/40 | train_f1(macro, micro) (0.25,0.93) | train_epoch_avg_loss 0.58| f1 for each class [0.         0.         0.14545455 0.         0.         0.3655914\n",
            " 0.24431818 0.19444444 0.36226415 0.         0.         0.60831123\n",
            " 0.         0.         0.54545455 0.         0.         0.48730964\n",
            " 0.45742905 0.51282051 0.50939457 0.         0.         0.7913612\n",
            " 0.97215014]\n",
            "test_f1(macro, micro) (0.35,0.95) | test_avg_loss 0.47 | f1 for each class[0.         0.         0.         0.         0.5        0.54545455\n",
            " 0.51428571 0.58333333 0.         0.         0.70890411 0.\n",
            " 0.         0.28571429 0.         0.57425743 0.68539326 0.81927711\n",
            " 0.76119403 0.         0.82440476 0.97899138]\n",
            "epoch 9/40 | train_f1(macro, micro) (0.29,0.95) | train_epoch_avg_loss 0.45| f1 for each class [0.         0.         0.1        0.         0.         0.56704981\n",
            " 0.35913313 0.39473684 0.36585366 0.         0.         0.67979003\n",
            " 0.         0.         0.5433526  0.         0.03508772 0.66835443\n",
            " 0.53521127 0.66666667 0.57534247 0.         0.         0.82883593\n",
            " 0.97874082]\n",
            "test_f1(macro, micro) (0.36,0.94) | test_avg_loss 0.48 | f1 for each class[0.         0.         0.66666667 0.         0.63636364 0.53061224\n",
            " 0.26086957 0.67391304 0.         0.68014706 0.         0.\n",
            " 0.23255814 0.         0.         0.66666667 0.61788618 0.61971831\n",
            " 0.69273743 0.         0.17391304 0.82596291 0.97708608]\n",
            "epoch 10/40 | train_f1(macro, micro) (0.32,0.95) | train_epoch_avg_loss 0.38| f1 for each class [0.         0.         0.05405405 0.         0.03125    0.63529412\n",
            " 0.4648318  0.38655462 0.52873563 0.         0.         0.71853147\n",
            " 0.         0.         0.52631579 0.         0.10909091 0.72251309\n",
            " 0.59197324 0.7147541  0.69683258 0.         0.06451613 0.83587408\n",
            " 0.98235965]\n",
            "test_f1(macro, micro) (0.43,0.95) | test_avg_loss 0.43 | f1 for each class[0.         0.         0.         0.33333333 0.67741935 0.62385321\n",
            " 0.59259259 0.7012987  0.         0.         0.70442478 0.66666667\n",
            " 0.2        0.54545455 0.2        0.73043478 0.61714286 0.76086957\n",
            " 0.75757576 0.         0.         0.79098005 0.97890337]\n",
            "epoch 11/40 | train_f1(macro, micro) (0.36,0.96) | train_epoch_avg_loss 0.34| f1 for each class [0.         0.         0.14634146 0.         0.22580645 0.69731801\n",
            " 0.54216867 0.56737589 0.64031621 0.         0.         0.7623201\n",
            " 0.         0.16666667 0.24489796 0.         0.2173913  0.74151436\n",
            " 0.65630397 0.79617834 0.74193548 0.         0.         0.86904762\n",
            " 0.98407588]\n",
            "test_f1(macro, micro) (0.44,0.96) | test_avg_loss 0.40 | f1 for each class[0.         0.         0.         0.         0.63492063 0.71428571\n",
            " 0.64       0.77922078 0.         0.74662162 0.         0.\n",
            " 0.         0.72727273 0.81818182 0.78       0.79569892 0.78740157\n",
            " 0.         0.85428571 0.9820897 ]\n",
            "epoch 12/40 | train_f1(macro, micro) (0.38,0.97) | train_epoch_avg_loss 0.27| f1 for each class [0.         0.         0.         0.         0.24489796 0.67169811\n",
            " 0.60895522 0.56451613 0.692607   0.         0.         0.80475981\n",
            " 0.         0.         0.45517241 0.         0.36734694 0.77165354\n",
            " 0.73825503 0.81481481 0.77097506 0.         0.         0.88389662\n",
            " 0.98792169]\n",
            "test_f1(macro, micro) (0.48,0.95) | test_avg_loss 0.45 | f1 for each class[0.         0.         0.4        0.57142857 0.75862069 0.68627451\n",
            " 0.57894737 0.74418605 0.         0.73941368 0.         0.\n",
            " 0.25       0.66666667 0.80733945 0.68367347 0.61666667 0.82706767\n",
            " 0.         0.82993197 0.98222761]\n",
            "epoch 13/40 | train_f1(macro, micro) (0.42,0.97) | train_epoch_avg_loss 0.22| f1 for each class [0.         0.         0.16216216 0.         0.36363636 0.7578125\n",
            " 0.57419355 0.6969697  0.70992366 0.         0.         0.82398957\n",
            " 0.06451613 0.25       0.57142857 0.         0.375      0.796875\n",
            " 0.71731449 0.83333333 0.79726651 0.         0.04651163 0.90477133\n",
            " 0.98988749]\n",
            "test_f1(macro, micro) (0.48,0.96) | test_avg_loss 0.34 | f1 for each class[0.         0.         0.         0.4        0.74576271 0.70689655\n",
            " 0.82758621 0.77922078 0.         0.76777251 0.         0.\n",
            " 0.23529412 0.         0.61538462 0.81415929 0.72380952 0.78125\n",
            " 0.8        0.4137931  0.85735964 0.98335351]\n",
            "epoch 14/40 | train_f1(macro, micro) (0.45,0.97) | train_epoch_avg_loss 0.20| f1 for each class [0.         0.         0.16666667 0.         0.45833333 0.765625\n",
            " 0.65243902 0.70149254 0.70731707 0.         0.1        0.83893496\n",
            " 0.3030303  0.         0.64383562 0.         0.625      0.86096257\n",
            " 0.78965517 0.85623003 0.79445727 0.         0.15686275 0.91024876\n",
            " 0.99083363]\n",
            "test_f1(macro, micro) (0.41,0.96) | test_avg_loss 0.42 | f1 for each class[0.         0.         0.         0.         0.         0.79365079\n",
            " 0.62318841 0.60869565 0.7816092  0.         0.75       0.\n",
            " 0.2        0.28571429 0.         0.76923077 0.80357143 0.75982533\n",
            " 0.7761194  0.86567164 0.         0.         0.84375    0.98272981]\n",
            "epoch 15/40 | train_f1(macro, micro) (0.45,0.97) | train_epoch_avg_loss 0.19| f1 for each class [0.         0.         0.125      0.         0.39130435 0.72440945\n",
            " 0.64174455 0.74418605 0.76923077 0.         0.         0.84312859\n",
            " 0.14285714 0.54545455 0.61538462 0.         0.5106383  0.83989501\n",
            " 0.78726968 0.82539683 0.77701149 0.         0.         0.91249751\n",
            " 0.99041494]\n",
            "test_f1(macro, micro) (0.52,0.96) | test_avg_loss 0.38 | f1 for each class[0.         0.         0.         0.         0.5        0.81012658\n",
            " 0.74074074 0.75       0.81012658 0.         0.7883959  0.\n",
            " 0.53333333 0.5        0.8        0.80327869 0.79581152 0.84337349\n",
            " 0.81818182 0.         0.86535552 0.98374352]\n",
            "epoch 16/40 | train_f1(macro, micro) (0.51,0.98) | train_epoch_avg_loss 0.14| f1 for each class [0.         0.         0.3030303  0.         0.62222222 0.8203125\n",
            " 0.73556231 0.80645161 0.812749   0.         0.08333333 0.87856203\n",
            " 0.15384615 0.46153846 0.77124183 0.         0.65384615 0.86327078\n",
            " 0.82675815 0.91823899 0.86175115 0.         0.16666667 0.92696517\n",
            " 0.99301201]\n",
            "test_f1(macro, micro) (0.49,0.96) | test_avg_loss 0.52 | f1 for each class[0.         0.         0.         0.5        0.70588235 0.71304348\n",
            " 0.75       0.85365854 0.         0.753125   0.         0.70588235\n",
            " 0.3        0.         0.73170732 0.82162162 0.86842105 0.88888889\n",
            " 0.         0.34782609 0.83641342 0.98103615]\n",
            "epoch 17/40 | train_f1(macro, micro) (0.50,0.98) | train_epoch_avg_loss 0.16| f1 for each class [0.         0.         0.25       0.         0.63829787 0.796875\n",
            " 0.67462687 0.80952381 0.86055777 0.         0.10526316 0.86734244\n",
            " 0.07142857 0.375      0.59722222 0.         0.77272727 0.84946237\n",
            " 0.79522184 0.89032258 0.861678   0.         0.32432432 0.92209857\n",
            " 0.99219706]\n",
            "test_f1(macro, micro) (0.48,0.96) | test_avg_loss 0.34 | f1 for each class[0.         0.         0.28571429 0.66666667 0.75862069 0.75213675\n",
            " 0.8        0.79545455 0.         0.77723577 0.         0.\n",
            " 0.3125     0.         0.36363636 0.78723404 0.78125    0.7826087\n",
            " 0.81889764 0.0952381  0.85408709 0.98374188]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAFcxDva2iGf"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows= 2, ncols= 1, figsize= (10,8), sharex= True)\r\n",
        "ax1.plot([*range(1, num_epochs+1)], list(train_loss.values()), label= 'train loss')\r\n",
        "ax1.plot([*range(1, num_epochs+1)], list(test_loss.values()), label= 'test loss')\r\n",
        "ax1.legend()\r\n",
        "\r\n",
        "ax2.plot([*range(1, num_epochs+1)], list(train_f1.values()), label= 'train f1 (macro)')\r\n",
        "ax2.plot([*range(1, num_epochs+1)], list(test_f1.values()), label= 'test f1 (macro)')\r\n",
        "ax2.legend()\r\n",
        "\r\n",
        "plt.tight_layout()\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.savefig('fig.jpeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pWa4TQhw5PI"
      },
      "source": [
        "# torch.save(model, 'ner_model_batch32_wup4000_lstmhd256_lr5e-5_40epoch_adam_wde-3.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1UTlctlqT8g"
      },
      "source": [
        "# !cp 'ner_model_batch32_wup4000_lstmhd256_lr5e-5_40epoch_adam_wde-3.pt' '/content/drive/My Drive/python檔/aicup'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvc6xWNC85De"
      },
      "source": [
        "with open('/content/drive/My Drive/python檔/aicup/test_input.data', 'r', encoding= 'utf-8') as f:\r\n",
        "    data = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApTe5VvOqO-P"
      },
      "source": [
        "pred_outs = test_output(data= data, model= model, tokenizer=tokenizer, batch_size= 32).pred_out_tsv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4qmYVsB9GYp"
      },
      "source": [
        "pred_outs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}