{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "test_output (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w3Fi292QUOW",
        "outputId": "bf821bdc-e0d3-4658-bdb4-786552bc4fe6"
      },
      "source": [
        "pip install pytorch-crf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.6/dist-packages (0.7.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRxPB8nzQewF",
        "outputId": "4b922673-1081-4281-c832-5086c0994b8c"
      },
      "source": [
        "pip install transformers==3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.1.94)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3) (20.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pFHcNUOTNaA",
        "outputId": "0cba0a6c-ae24-4d75-cb73-83f3a87835c5"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hbl5ZCgUh3Z"
      },
      "source": [
        "import sys\r\n",
        "sys.path.insert(0,\"/content/drive/My Drive/python檔/aicup/run\")\r\n",
        "sys.path.insert(0,\"/content/drive/My Drive/python檔/aicup\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djFHaiZ4MiOA"
      },
      "source": [
        "import csv\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchcrf import CRF\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQZSpc_a4-x-"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz_M7clkMiOH"
      },
      "source": [
        "class model_crf(nn.Module):\n",
        "\tdef __init__(self, n_tags, hidden_dim=256, batchsize= 64, num_layers= 1, lstm_dropout= 0, fc_dropout= 0.2):\n",
        "\t\tsuper(model_crf, self).__init__()\n",
        "\t\tself.num_layers = num_layers\n",
        "\t\tself.n_tags = n_tags\n",
        "\t\tself.lstm =  nn.LSTM(bidirectional=True, num_layers=num_layers, input_size=768, hidden_size=hidden_dim//2, dropout= lstm_dropout, batch_first=True)\t\t\n",
        "\t\tself.hidden_dim = hidden_dim\n",
        "\t\tself.fc = nn.Linear(hidden_dim, self.n_tags)\n",
        "\t\tself.bert = BertModel.from_pretrained('bert-base-chinese')\n",
        "\n",
        "\t\t# for param in self.bert.parameters():\n",
        "\t\t# \tparam.requires_grad = False\n",
        "\t\t# self.bert.eval()  # 知用来取bert embedding\n",
        "\n",
        "\t\tself.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\t\tself.CRF = CRF(n_tags, batch_first= True)\n",
        "\t\tself.dropout = nn.Dropout(p= fc_dropout)\n",
        "\t\tself.hidden = self.init_hidden(batchsize)\n",
        "\n",
        "\tdef init_hidden(self, batch_size):\n",
        "\t\treturn (torch.randn(2*self.num_layers, batch_size, self.hidden_dim // 2).to(self.device),\n",
        "\t\t\t\ttorch.randn(2*self.num_layers, batch_size, self.hidden_dim // 2).to(self.device))\n",
        "\n",
        "\tdef forward(self, input_ids, attention_mask, tags):\n",
        "\n",
        "\t\tbatch_size = input_ids.size(0)\n",
        "\t\tmax_seq_len = input_ids.size(1)\n",
        "\t\tbert_output, _  = self.bert(input_ids.long(), attention_mask)\n",
        "\t\tseq_len = torch.sum(attention_mask, dim= 1).cpu().int()\n",
        "\t\t# print(seq_len)\n",
        "\t\tpack_input = pack_padded_sequence(input= bert_output, lengths= seq_len, batch_first= True, enforce_sorted= False)\n",
        "\t\tpacked_lstm_out, _ = self.lstm(pack_input,self.init_hidden(batch_size= batch_size))\n",
        "\t\tlstm_enc, _=  pad_packed_sequence(packed_lstm_out, batch_first=True)\n",
        "\t\t# print(lstm_enc.size())\n",
        "\t\tlstm_enc = self.dropout(lstm_enc)\n",
        "\t\tlstm_feats = self.fc(lstm_enc)\n",
        "\n",
        "\t\tlstm_max_seq_len = lstm_feats.size(1)\n",
        "\t\tpad = torch.zeros(size=(batch_size, max_seq_len-lstm_max_seq_len, self.n_tags), dtype= torch.float).to(self.device)\n",
        "\t\tlstm_feats= torch.cat((lstm_feats, pad), dim= 1)\n",
        "  \n",
        "\t\t# lstm_feats[:,:,:4] = lstm_feats[:,:,:5]*100\n",
        "\t\t# lstm_feats[:,:,5:9] = lstm_feats[:,:,5:9]*10\n",
        "\t\t# lstm_feats[:,:,9:11] = lstm_feats[:,:,9:11]*100\n",
        "\t\t# lstm_feats[:,:,11] = lstm_feats[:,:,11]*100\n",
        "\t\t# lstm_feats[:,:,12:17] = lstm_feats[:,:,12:17]*100\n",
        "\t\t# lstm_feats[:,:,17:21] = lstm_feats[:,:,17:21]*10\n",
        "\t\t# lstm_feats[:,:,21:23] = lstm_feats[:,:,21:23]*100\n",
        "\t\t# lstm_feats[:,:,23] = lstm_feats[:,:,23]*1\n",
        "\n",
        "\t\tlstm_feats[:,:,:23] = lstm_feats[:,:,:23]*100\n",
        "\t\tlstm_feats[:,:,23] = lstm_feats[:,:,23]*10\n",
        "\n",
        "\t\tloss = -self.CRF(lstm_feats, tags, attention_mask.bool(), reduction= 'token_mean')\n",
        "\t\tpred_seqs = self.CRF.decode(emissions= lstm_feats, mask= attention_mask.bool())\n",
        "  \n",
        "\t\treturn loss, pred_seqs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2GKK0wlMiOJ"
      },
      "source": [
        "class test_output():\n",
        "    def __init__(self, data):\n",
        "        \n",
        "        self.data_list = []\n",
        "        self.word_id = []\n",
        "        self.word_article_id = [] \n",
        "        article_id = 0\n",
        "        word_id = 0\n",
        "        data_list_tmp = []\n",
        "        article_id_tmp = []\n",
        "        word_id_tmp = []\n",
        "        \n",
        "        for row in data:\n",
        "            \n",
        "            data_tuple = tuple()\n",
        "            if row == '\\n':\n",
        "                \n",
        "                article_id += 1 \n",
        "                word_id = 0\n",
        "                self.word_id.append(word_id)\n",
        "                self.word_article_id.append(article_id_tmp)\n",
        "                self.data_list.append(data_list_tmp)\n",
        "                data_list_tmp = []\n",
        "                article_id_tmp = []\n",
        "                word_id_tmp = []\n",
        "            \n",
        "            if (row[0] == '，') & (len(data_list_tmp) >= 64):\n",
        "\n",
        "                article_id += 1 \n",
        "                word_id = 0\n",
        "                self.word_id.append(word_id)\n",
        "                self.word_article_id.append(article_id_tmp)\n",
        "                self.data_list.append(data_list_tmp)\n",
        "                data_list_tmp = []\n",
        "                article_id_tmp = []\n",
        "                word_id_tmp = []\n",
        "\n",
        "            else:\n",
        "                \n",
        "                row = row.strip('\\n').split(' ')\n",
        "\n",
        "                if row[0] in ['。', '？','！','～','：']:\n",
        "                    \n",
        "                    self.word_id.append(word_id_tmp)\n",
        "                    self.word_article_id.append(article_id_tmp)\n",
        "                    self.data_list.append(data_list_tmp)\n",
        "                    data_list_tmp = []\n",
        "                    article_id_tmp = []\n",
        "                    word_id_tmp = []\n",
        "                    \n",
        "                elif row[0] in ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']:\n",
        "                      \n",
        "                    data_tuple = (row[0].lower(), article_id, word_id)\n",
        "                    data_list_tmp.append(data_tuple)\n",
        "                    article_id_tmp.append(article_id)\n",
        "                    word_id_tmp.append(word_id)\n",
        "\n",
        "                elif row[0] not in ['摁','嗯','啦','喔','欸','啊','齁','嘿','…','...','，']:\n",
        "                    \n",
        "                    data_tuple = (row[0], article_id, word_id)\n",
        "                    data_list_tmp.append(data_tuple)\n",
        "                    article_id_tmp.append(article_id)\n",
        "                    word_id_tmp.append(word_id)\n",
        "                    \n",
        "                word_id += 1\n",
        "                \n",
        "        if len(data_list_tmp) != 0:\n",
        "            self.data_list.append(data_list_tmp)\n",
        "            self.word_id.append(word_id_tmp)\n",
        "            self.word_article_id.append(article_id_tmp)\n",
        "            \n",
        "    def raw_output(self):\n",
        "        return self.data_list, self.word_id, self.word_article_id\n",
        "    \n",
        "    def get_stcs(self):\n",
        "        \n",
        "        all_stcs = list()\n",
        "        all_article_ids = list()\n",
        "        all_word_ids = list()\n",
        "\n",
        "        for stc_list in self.data_list:\n",
        "\n",
        "            txt_len = len(stc_list) #(文章數，每個文章對應的總字數) (word, label)\n",
        "            stc = str() #存字數= max_stc_len的字串\n",
        "            article_ids = []\n",
        "            word_ids = []\n",
        "            \n",
        "\n",
        "            for idx, (word,article_id, word_id) in enumerate(stc_list):\n",
        "\n",
        "                stc += word\n",
        "                article_ids.append(article_id)\n",
        "                word_ids.append(word_id)\n",
        "\n",
        "            all_stcs.append(stc)\n",
        "            all_article_ids.append(article_ids)\n",
        "            all_word_ids.append(word_ids)\n",
        "        \n",
        "        assert len(all_stcs) > 0, 'all stcs len = 0' \n",
        "        \n",
        "        all_stcs_clean = []\n",
        "        all_article_ids_clean = []\n",
        "        all_word_ids_clean = []\n",
        "        idx = 0\n",
        "        \n",
        "        for stc, article_id, word_id in zip(all_stcs, all_article_ids, all_word_ids):\n",
        "            stc_clean = re.sub(r'(醫師)|(個管師)|(民眾)|(家屬)|(護理師)', '', stc)\n",
        "            # print(stc, stc_clean, label)\n",
        "            if len(stc_clean) > 1:  \n",
        "            # print(stc_clean, stc)\n",
        "                all_stcs_clean.append(stc)\n",
        "                all_article_ids_clean.append(article_id)\n",
        "                all_word_ids_clean.append(word_id)\n",
        "\n",
        "            # 這一步就先把label 做 0 padding\n",
        "            \n",
        "        max_length = len(max(all_stcs_clean, key=len))\n",
        "        assert max_length > 0, 'max length less than 1'\n",
        "\n",
        "        print('sentences總數: {}'.format(len(all_stcs_clean)))\n",
        "            \n",
        "        return all_stcs_clean, all_article_ids_clean, all_word_ids_clean"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L01b5oKm5DeZ"
      },
      "source": [
        "# loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otzi7hMUMiOI",
        "outputId": "c60b06a4-a369-41ea-d19a-c1420116fd4d"
      },
      "source": [
        "with open('/content/drive/My Drive/python檔/aicup/test_input.data', 'r', encoding= 'utf-8') as f:\n",
        "    data = f.readlines()\n",
        "\n",
        "print(data[:10])\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['醫\\n', '師\\n', '：\\n', '最\\n', '近\\n', '人\\n', '有\\n', '沒\\n', '有\\n', '什\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2LFN3MCMiOK",
        "outputId": "e637d701-1049-47b9-85ed-998586aa607a"
      },
      "source": [
        "stcs, article_ids, word_ids = test_output(data= data).get_stcs()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentences總數: 15391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaEg3_69-RJt",
        "outputId": "95543b50-d0b2-4e81-a275-c43e35f4c837"
      },
      "source": [
        "idx = 0\r\n",
        "for stc, word_id in zip(stcs, word_ids):\r\n",
        "    if idx == 150:\r\n",
        "        break\r\n",
        "    print(stc, word_id)\r\n",
        "    idx += 1 \r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "最近人有沒有什麼不舒服 [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
            "沒有 [18, 19]\n",
            "沒有 [24, 25]\n",
            "我們本來說要月年底才打對不對 [28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42]\n",
            "對但醫師 [47, 49, 52, 53]\n",
            "你是怕 [55, 56, 57]\n",
            "因為我要治療牙開始要治療牙齒了 [62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77]\n",
            "我又不常跑醫院 [88, 89, 90, 91, 92, 93, 94]\n",
            "我想說那先來打我今天來領藥嘛那順便過來打 [104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125]\n",
            "是是是 [132, 133, 134]\n",
            "你是要做 [137, 138, 139, 140]\n",
            "植牙 [145, 146]\n",
            "植牙的 [151, 152, 153]\n",
            "對對對 [158, 159, 160]\n",
            "你的免疫力是夠是沒有問題的 [168, 169, 170, 171, 172, 173, 174, 176, 177, 178, 179, 180, 181]\n",
            "在我們這裡做的嗎 [183, 184, 185, 186, 187, 188, 189, 190]\n",
            "對在新樓做的 [195, 197, 198, 199, 200, 201]\n",
            "那這樣離你住的地方也有點距離你是住麻豆嗎 [208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 224, 225, 226, 227, 228, 229]\n",
            "對住麻豆 [234, 236, 237, 238]\n",
            "那辛苦了那我們就去等下就去打針這樣子 [243, 244, 245, 246, 249, 250, 251, 252, 253, 255, 256, 257, 258, 259, 260, 261, 262, 263]\n",
            "沒什麼不舒服 [266, 267, 268, 269, 270, 271]\n",
            "最近 [275, 276]\n",
            "沒有都沒有 [281, 282, 284, 285, 286]\n",
            "最近今年都是打四價的 [292, 293, 295, 296, 297, 298, 299, 300, 301, 302]\n",
            "各個廠牌都有我覺得都沒差 [314, 315, 316, 317, 318, 319, 322, 323, 324, 325, 326, 327]\n",
            "反正我也都沒在挑的 [329, 330, 331, 332, 333, 334, 335, 336, 337]\n",
            "沒關係 [347, 348, 349]\n",
            "我不知道這個要醫師 [356, 357, 358, 359, 362, 363, 364, 367, 368]\n",
            "應該都ok都還好都很安全 [370, 371, 372, 373, 374, 377, 378, 379, 381, 382, 383, 384]\n",
            "這個你比較專業的 [389, 390, 391, 392, 393, 394, 395, 396]\n",
            "血壓這樣ok [404, 405, 406, 407, 408, 409]\n",
            "不過可能打針要等一下因為最近打的人很多 [411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 422, 423, 424, 425, 426, 427, 428, 429, 430]\n",
            "很踴躍 [442, 443, 444]\n",
            "對就聽說年底可能會打不到 [449, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462]\n",
            "會這樣 [469, 470, 471]\n",
            "好有可能假如拖到年底可能會打不完 [474, 476, 477, 478, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491]\n",
            "好那這樣我們這邊就結束了 [493, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505]\n",
            "要去上課嗎 [4, 5, 6, 7, 8]\n",
            "沒有 [13, 14]\n",
            "這學期已經開始不上課了 [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
            "今天的呃應該是昨天的抽血 [32, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45]\n",
            "肝、腎功能都很棒 [55, 56, 57, 58, 59, 60, 61, 62]\n",
            "肌酐酸0.79換算成腎絲球過濾率預測值是88 [73, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]\n",
            "這個通常60以上就夠了 [108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118]\n",
            "60 [123, 124]\n",
            "這個都很讚 [129, 130, 131, 132, 133]\n",
            "謝謝 [138, 139]\n",
            "ast/alt就是我們常講的got還有gpt [145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166]\n",
            "分別是20跟19 [177, 178, 179, 180, 181, 182, 183, 184]\n",
            "那也是很理想 [194, 195, 196, 197, 198, 199]\n",
            "crp就是那個我們那個發炎體內發炎的時候肝臟會分泌這個c反應蛋白 [210, 211, 212, 213, 214, 215, 216, 218, 219, 220, 221, 222, 223, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244]\n",
            "這個也都2.6 [255, 256, 257, 258, 259, 260, 261]\n",
            "是大概也是ok [271, 272, 273, 274, 275, 276, 277]\n",
            "ok好 [282, 283, 285]\n",
            "那我們接下來就是等我們就等那個民眾 [290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 305, 308, 309]\n",
            "超音波 [311, 312, 313]\n",
            "超音波做完再來停藥這樣子 [318, 319, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330]\n",
            "那個糖尿病那個血糖那個醫師 [338, 339, 340, 341, 342, 344, 345, 346, 347, 348, 349, 352, 353]\n",
            "血糖 [355, 356]\n",
            "黃醫師說現在6.6 [361, 362, 363, 364, 365, 366, 367, 368, 369]\n",
            "那這樣不錯 [376, 377, 378, 379, 380]\n",
            "6.6不錯那時候8.3 [386, 387, 388, 389, 390, 392, 393, 394, 395, 396, 397]\n",
            "我記得那時候8.3 [402, 403, 404, 405, 406, 407, 408, 409, 410]\n",
            "四個月前8.3 [415, 416, 417, 418, 419, 420, 421]\n",
            "所以這樣吃應該可以 [426, 427, 428, 429, 430, 431, 432, 433, 434]\n",
            "對可以 [440, 442, 443]\n",
            "應該可以 [448, 449, 450, 451]\n",
            "看下次可不可以變6.2說都三個月量一次 [456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 468, 469, 470, 471, 472, 473, 474, 475]\n",
            "也不要太低 [481, 482, 483, 484, 485]\n",
            "那劉老師就注意一下那個低血糖的症狀這樣 [497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 509, 510, 511, 512, 513, 514, 515, 516]\n",
            "如果要去運動身上要帶那個要帶個糖果 [527, 528, 529, 530, 531, 532, 534, 535, 536, 537, 538, 539, 541, 542, 543, 544, 545]\n",
            "糖果好 [550, 551, 553]\n",
            "有時候怕臨時 [558, 559, 560, 561, 562, 563]\n",
            "血糖降得太低 [573, 574, 575, 576, 577, 578]\n",
            "還好現在醫師 [583, 584, 586, 587, 590, 591]\n",
            "應該不會 [593, 594, 595, 596]\n",
            "應該不會 [603, 604, 605, 606]\n",
            "假如這個治療完看其實這個藥黃醫師他應該也有對不對 [612, 613, 614, 615, 616, 617, 618, 620, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637]\n",
            "他不知道我沒有問他他是都有在幫我開一個格里曼 [642, 644, 645, 646, 648, 649, 650, 651, 652, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666]\n",
            "格里曼 [671, 672, 673]\n",
            "那個飯前血糖 [678, 679, 681, 682, 683, 684]\n",
            "飯前血糖 [691, 692, 693, 694]\n",
            "以前我醫師 [699, 700, 701, 704, 705]\n",
            "你看一下這個嗎 [707, 708, 709, 710, 712, 713, 714]\n",
            "對格里曼 [719, 721, 722, 723]\n",
            "所以我現在是早、晚都吃這個捷糖穩中午吃這個格里曼 [725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 743, 744, 745, 746, 747, 748, 749, 750]\n",
            "格里曼 [755, 756, 757]\n",
            "那是他開的 [764, 765, 766, 767, 768]\n",
            "我來看一下我查一下 [773, 774, 775, 776, 777, 779, 780, 781, 782]\n",
            "所以之後民眾 [785, 786, 787, 788, 791, 792]\n",
            "那樣吃起來覺得還不錯 [794, 795, 796, 797, 798, 799, 800, 801, 802, 803]\n",
            "我們叫做費德泌得贊 [808, 809, 810, 811, 812, 813, 815, 816, 817]\n",
            "好沒關係到時候再來討論好了 [821, 823, 824, 825, 827, 828, 829, 830, 831, 832, 833, 834, 835]\n",
            "那我們誒 [845, 846, 847, 849]\n",
            "我們超音波是是排什麼時候 [851, 852, 853, 854, 855, 856, 858, 859, 860, 861, 862, 863]\n",
            "21 [868, 869]\n",
            "21 [874, 875]\n",
            "還有12天 [880, 881, 882, 883, 884]\n",
            "還有12天 [889, 890, 891, 892, 893]\n",
            "好那我們就預約三個星期 [895, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906]\n",
            "就照這樣 [916, 917, 918, 919]\n",
            "抗生素不知道吃多久 [927, 928, 929, 930, 931, 932, 933, 934, 935]\n",
            "現在是都 [937, 938, 939, 940]\n",
            "我們就繼續 [945, 946, 947, 948, 949]\n",
            "繼續好 [954, 955, 956]\n",
            "應該應該通常都會ok我是比較保守 [962, 963, 965, 966, 967, 968, 969, 970, 971, 972, 975, 976, 977, 978, 979, 980]\n",
            "好好 [985, 987]\n",
            "這個算是第一代的抗生素就有效所以應該也不是說很強的 [993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018]\n",
            "應該剛好這樣 [1029, 1030, 1031, 1032, 1033, 1034]\n",
            "謝謝謝謝 [1050, 1051, 1053, 1054]\n",
            "後來那個吃藥沒有問題對不對 [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
            "沒有 [21, 22]\n",
            "你是打針的 [29, 30, 31, 32, 33]\n",
            "阿身上有沒有出現什麼 [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]\n",
            "沒有 [57, 58]\n",
            "也沒有之前有嗎 [63, 64, 65, 67, 68, 69, 70]\n",
            "哪個 [75, 76]\n",
            "手也沒有嗎 [81, 82, 83, 84, 85]\n",
            "沒有诶之前還沒有就是因為有點連續對然後上次也沒有诶 [90, 91, 93, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 110, 112, 113, 114, 115, 116, 117, 118, 120]\n",
            "上次也沒有上上次也沒有測到病毒量诶 [122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140]\n",
            "那個比例 [142, 143, 145, 146]\n",
            "比例因為你的免疫力超好cd4都1100 [152, 153, 156, 157, 158, 159, 160, 161, 162, 163, 164, 167, 168, 169, 170, 171, 172, 173, 174]\n",
            "比正常人高嗎 [179, 180, 181, 182, 183, 184]\n",
            "對‧民眾 [189, 191, 192, 193]\n",
            "可是其實我覺得身體很虛耶 [195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206]\n",
            "可能太累了工作太累 [211, 212, 213, 214, 215, 217, 218, 219, 220]\n",
            "不然你的身體還不錯 [222, 223, 224, 225, 226, 227, 228, 229, 230]\n",
            "是嗎 [235, 236]\n",
            "你的免疫力1145耶 [241, 242, 243, 244, 245, 246, 247, 248, 249, 250]\n",
            "很高嗎 [255, 256, 257]\n",
            "很高阿民眾 [262, 263, 264, 266, 267]\n",
            "正常是 [269, 270, 271]\n",
            "500就夠了 [276, 277, 278, 279, 280, 281]\n",
            "500就夠了 [286, 287, 288, 289, 290, 291]\n",
            "對因為你年輕而且很早就開始規則用藥就繼續保持下去但這個免疫力好沒有預防梅毒梅毒沒有保護力‧民眾 [298, 300, 301, 302, 303, 304, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351]\n",
            "所以不小心就有時候還是會得到這樣子 [359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375]\n",
            "好那我們就是我這次還是開三個月 [377, 379, 380, 381, 382, 383, 385, 386, 387, 388, 389, 390, 391, 392, 393]\n",
            "三個月然後時間到因為我們反正要測那個指數小梅的指數我們就病毒量是不是 [403, 404, 405, 406, 407, 408, 409, 410, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 439, 440, 441]\n",
            "還是不用 [443, 444, 445, 446]\n",
            "算了 [448, 449]\n",
            "cd4這麼高不用就測那個就下次治療完就測那個‧民眾 [451, 452, 453, 454, 455, 456, 457, 458, 461, 462, 463, 464, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479]\n",
            "你決定 [481, 482, 483]\n",
            "我決定好單純一點好了 [489, 490, 491, 494, 496, 497, 498, 499, 500, 501]\n",
            "我有那個算另外一半本來的另外一半 [506, 507, 508, 509, 511, 512, 513, 514, 515, 518, 519, 520, 521, 522, 523, 524]\n",
            "他吃什麼藥 [529, 530, 531, 532, 533]\n",
            "我不知道可是他說他吃那個藥會變胖 [538, 539, 540, 541, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554]\n",
            "所以你吃這個沒有變胖 [559, 560, 561, 562, 563, 564, 565, 566, 567, 568]\n",
            "沒有 [573, 574]\n",
            "就是也胖不了我曾經想變胖的時候就是用了很不好的方法 [576, 577, 578, 579, 580, 581, 583, 584, 585, 586, 587, 588, 589, 590, 591, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602]\n",
            "確實沒錯大部分都變胖吃這個藥 [607, 608, 609, 610, 612, 613, 614, 615, 616, 617, 619, 620, 621, 622]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDhZqgsMMiOL",
        "outputId": "f944a463-762a-4edf-84e0-c334f04fd4e9"
      },
      "source": [
        "clean_stcs, clean_article_id, clean_word_id = [], [] ,[]\n",
        "\n",
        "for stc, article_id, word_id in zip(stcs, article_ids, word_ids):\n",
        "#     print(stc, article_id, word_id)\n",
        "    if stc not in ['沒有','也沒有','哪個','那個','算了','不用','有','有有有','有嗎','一點點', '謝謝','不會','不好意思','對不對','好不好','要嗎','還好']:\n",
        "        clean_stcs.append(stc)\n",
        "        clean_article_id.append(article_id)\n",
        "        clean_word_id.append(word_id)\n",
        "print(len(clean_stcs))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zVDV_utMiOL"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjg8d7ErMiOM",
        "outputId": "b7586029-6b63-4775-b3ff-c27cb216ff55"
      },
      "source": [
        "max_len = max(len(txt) for txt in clean_stcs)\n",
        "print(max_len)\n",
        "print(len(clean_stcs))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85\n",
            "15117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYMACkOsWdua"
      },
      "source": [
        "clean_stcs_space = [' '.join(list(stc)) for stc in clean_stcs]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdChc6CbWu_A",
        "outputId": "b7f19cdf-69a6-40af-96aa-189aad3344f2"
      },
      "source": [
        "clean_stcs_space[:50]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['最 近 人 有 沒 有 什 麼 不 舒 服',\n",
              " '我 們 本 來 說 要 月 年 底 才 打 對 不 對',\n",
              " '對 但 醫 師',\n",
              " '你 是 怕',\n",
              " '因 為 我 要 治 療 牙 開 始 要 治 療 牙 齒 了',\n",
              " '我 又 不 常 跑 醫 院',\n",
              " '我 想 說 那 先 來 打 我 今 天 來 領 藥 嘛 那 順 便 過 來 打',\n",
              " '是 是 是',\n",
              " '你 是 要 做',\n",
              " '植 牙',\n",
              " '植 牙 的',\n",
              " '對 對 對',\n",
              " '你 的 免 疫 力 是 夠 是 沒 有 問 題 的',\n",
              " '在 我 們 這 裡 做 的 嗎',\n",
              " '對 在 新 樓 做 的',\n",
              " '那 這 樣 離 你 住 的 地 方 也 有 點 距 離 你 是 住 麻 豆 嗎',\n",
              " '對 住 麻 豆',\n",
              " '那 辛 苦 了 那 我 們 就 去 等 下 就 去 打 針 這 樣 子',\n",
              " '沒 什 麼 不 舒 服',\n",
              " '最 近',\n",
              " '沒 有 都 沒 有',\n",
              " '最 近 今 年 都 是 打 四 價 的',\n",
              " '各 個 廠 牌 都 有 我 覺 得 都 沒 差',\n",
              " '反 正 我 也 都 沒 在 挑 的',\n",
              " '沒 關 係',\n",
              " '我 不 知 道 這 個 要 醫 師',\n",
              " '應 該 都 o k 都 還 好 都 很 安 全',\n",
              " '這 個 你 比 較 專 業 的',\n",
              " '血 壓 這 樣 o k',\n",
              " '不 過 可 能 打 針 要 等 一 下 因 為 最 近 打 的 人 很 多',\n",
              " '很 踴 躍',\n",
              " '對 就 聽 說 年 底 可 能 會 打 不 到',\n",
              " '會 這 樣',\n",
              " '好 有 可 能 假 如 拖 到 年 底 可 能 會 打 不 完',\n",
              " '好 那 這 樣 我 們 這 邊 就 結 束 了',\n",
              " '要 去 上 課 嗎',\n",
              " '這 學 期 已 經 開 始 不 上 課 了',\n",
              " '今 天 的 呃 應 該 是 昨 天 的 抽 血',\n",
              " '肝 、 腎 功 能 都 很 棒',\n",
              " '肌 酐 酸 0 . 7 9 換 算 成 腎 絲 球 過 濾 率 預 測 值 是 8 8',\n",
              " '這 個 通 常 6 0 以 上 就 夠 了',\n",
              " '6 0',\n",
              " '這 個 都 很 讚',\n",
              " 'a s t / a l t 就 是 我 們 常 講 的 g o t 還 有 g p t',\n",
              " '分 別 是 2 0 跟 1 9',\n",
              " '那 也 是 很 理 想',\n",
              " 'c r p 就 是 那 個 我 們 那 個 發 炎 體 內 發 炎 的 時 候 肝 臟 會 分 泌 這 個 c 反 應 蛋 白',\n",
              " '這 個 也 都 2 . 6',\n",
              " '是 大 概 也 是 o k',\n",
              " 'o k 好']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0LpSpqhMiOM"
      },
      "source": [
        "encoding = tokenizer.batch_encode_plus(clean_stcs_space, \n",
        "                                 padding=True,\n",
        "                                 add_special_tokens=False,\n",
        "                                 return_attention_mask= True,\n",
        "                                 return_token_type_ids= False,\n",
        "                                #  is_split_into_words=True,\n",
        "                                 return_tensors='pt')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsfKqarBMiON",
        "outputId": "c912440a-2568-4396-bfeb-93f3eecdd63d"
      },
      "source": [
        "encoding['input_ids'].size()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15117, 85])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRsN0MN3MiOO",
        "outputId": "2f8dbbc3-d729-413d-fdee-f48bd1a36775"
      },
      "source": [
        "encoding['attention_mask'].size()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15117, 85])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUbO00avMiOO",
        "outputId": "a36384a9-464f-41ce-c395-833da400115f"
      },
      "source": [
        "encoding['attention_mask'][:2]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeWi6JkEMiOP"
      },
      "source": [
        "# loss, preds = model(input_ids, "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAmCoXk7MiOP"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRRK_eLBMiOP"
      },
      "source": [
        "# 輸出成csv\n",
        "1. 需要id to tag dictionary\n",
        "2. 對每個預測句子做iteration\n",
        "3. pred : [句子數, 句子長度]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4KkIRNSMiOP"
      },
      "source": [
        "model = model_crf(n_tags= 25).to('cuda')\r\n",
        "device = 'cuda'"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYRrdD5PMiOQ"
      },
      "source": [
        "model = torch.load('/content/drive/My Drive/python檔/aicup/64_wp0.4_256_lr1e-4_30epoch_1e-4_layer1_0.75.pt')\n",
        "torch.save(model.state_dict(), 'model_dict.pt')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ibVf-MqMiOQ",
        "outputId": "0178fe6c-3248-4639-9df1-823f6809c957"
      },
      "source": [
        "model.load_state_dict(torch.load('model_dict.pt'), strict=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6USsFaNKMiOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8475e211-ba2a-43c1-9ac8-da6815736a2f"
      },
      "source": [
        "batch_size= 128\r\n",
        "pred_labels = []\r\n",
        "print(len(clean_stcs)/batch_size)\r\n",
        "print((len(clean_stcs) % batch_size))\r\n",
        "\r\n",
        "model.eval()\r\n",
        "\r\n",
        "for idx in range(int((len(clean_stcs)/batch_size))):\r\n",
        "  input= encoding['input_ids'][idx*batch_size:(idx+1)*batch_size].to(device)\r\n",
        "  mask = encoding['attention_mask'][idx*batch_size:(idx+1)*batch_size].to(device)\r\n",
        "  tags= torch.zeros((input.size(0),input.size(1)), dtype=torch.long).to(device)\r\n",
        "  with torch.no_grad():\r\n",
        "    _, preds = model(input, mask, tags)\r\n",
        "  for pred in preds:\r\n",
        "    pred_labels.append(pred)\r\n",
        "\r\n",
        "print('ok')\r\n",
        "\r\n",
        "if (len(clean_stcs) % batch_size) != 0:\r\n",
        "  idx = int((len(clean_stcs)/batch_size))\r\n",
        "  input= encoding['input_ids'][idx*batch_size:].to(device)\r\n",
        "  mask = encoding['attention_mask'][idx*batch_size:].to(device)\r\n",
        "  tags= torch.zeros((input.size(0),input.size(1)), dtype=torch.long).to(device)\r\n",
        "  with torch.no_grad():\r\n",
        "    _, preds = model(input, mask, tags)\r\n",
        "  for pred in preds:\r\n",
        "    pred_labels.append(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118.1015625\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybtm_KW_fivv"
      },
      "source": [
        "tag2id = {'B-ID': 0, 'B-clinical_event': 1, 'B-contact': 2, 'B-education': 3, 'B-family': 4, 'B-location': 5, 'B-med_exam': 6, 'B-money': 7, 'B-name': 8, 'B-organization': 9, 'B-profession': 10, 'B-time': 11, 'I-ID': 12, 'I-clinical_event': 13, 'I-contact': 14, 'I-education': 15, 'I-family': 16, 'I-location': 17, 'I-med_exam': 18, 'I-money': 19, 'I-name': 20, 'I-organization': 21, 'I-profession': 22, 'I-time': 23, 'O': 24}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMBimLbjfswg"
      },
      "source": [
        "id2tag ={v:k for k, v in tag2id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-TKZhJ-dv8T"
      },
      "source": [
        "pred_labels_tag = []\r\n",
        "for label in pred_labels:\r\n",
        "  stc_label = [id2tag[id] for id in label]\r\n",
        "  pred_labels_tag.append(stc_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkwRezcihGOt"
      },
      "source": [
        "pred_labels_tag[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghhCpleAMiOR"
      },
      "source": [
        "# _, preds = model(encoding['input_ids'].to('cuda'), encoding['attention_mask'].to('cuda'), tags= torch.zeros((encoding['input_ids'].size(0),encoding['input_ids'].size(1))).to('cuda'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWy1ymFeMiOR"
      },
      "source": [
        "entity_text = []\n",
        "\n",
        "for stc, labels, article_id, word_id in zip(clean_stcs, pred_labels_tag, clean_article_id, clean_word_id):\n",
        "    if len(set(labels)) >1:\n",
        "        print(stc)\n",
        "        print(labels)\n",
        "    entity = str()\n",
        "    \n",
        "    start_pos = 0\n",
        "    end_pos = 0\n",
        "    article = 0\n",
        "    \n",
        "    entity_type = str()\n",
        "    \n",
        "    \n",
        "    for idx, label in enumerate(labels):\n",
        "        if bool(re.match(r'B-', label)):\n",
        "            entity += list(stc)[idx]\n",
        "            start_pos = word_id[idx]\n",
        "            article = article_id[idx]\n",
        "            entity_type = label.split('B-')[1]\n",
        "            \n",
        "        elif bool(re.match(r'I-', label)):\n",
        "            entity += list(stc)[idx]\n",
        "            end_pos= word_id[idx]+1\n",
        "            try:\n",
        "              if (labels[idx+1] == 'O') & (entity_type!=''):\n",
        "                  entity_text.append((article, start_pos, end_pos, entity, entity_type))\n",
        "                  \n",
        "                  entity = str()\n",
        "                  start_pos = 0\n",
        "                  end_pos = 0\n",
        "                  article = 0\n",
        "                  entity_type = str()\n",
        "            except:\n",
        "              pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duLRJGzphlwM"
      },
      "source": [
        "entity_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suSdpY3FMiOR"
      },
      "source": [
        "with open('test_output.tsv', 'w', encoding='utf-8',newline='\\n') as f:\r\n",
        "  writer = csv.writer(f, delimiter='\\t')\r\n",
        "  writer.writerow(['article_id','start_position', 'end_position', 'entity_text', 'entity_type'])\r\n",
        "  for (article, start_pos, end_pos, entity, entity_type) in entity_text:\r\n",
        "    writer.writerow([str(article), str(start_pos), str(end_pos), str(entity), str(entity_type)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m6KR13gMiOS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}