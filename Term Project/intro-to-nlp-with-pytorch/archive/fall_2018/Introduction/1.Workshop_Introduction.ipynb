{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/logo.png\" align=\"left\" width=\"25%\">\n",
    "\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Who are we\n",
    "\n",
    "* Micheleen Harris (Microsoft) - Introduction and setup\n",
    "* Anna Bethke (Intel) - Lifecycle of an NLP Project\n",
    "* Mehrdad Yazdani (January, Inc.) - Speaking about Character Level Classification\n",
    "* Kendall Chuang (Ayasdi) - Introduction Word Embeddings\n",
    "* David Clark (Fresh Gravity) - Speaking about LSTMs\n",
    "* Micheleen - Bidirectional LSTMs, Conditional Random Fields\n",
    "* Kendall Chuang - ULMFit and Closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br><img src=\"https://media.licdn.com/dms/image/C5603AQEnxU_ugXgzAw/profile-displayphoto-shrink_200_200/0?e=1542844800&v=beta&t=AO-6ug5sU5NZp5cEdxLph311GCQbeHJBuUZOdRWaUjo\" width=\"10%\" align=\"left\"><img src=\"https://media.licdn.com/dms/image/C4E03AQEtECRVH6qAbA/profile-displayphoto-shrink_800_800/0?e=1542844800&v=beta&t=1AofoxAKH-0GQRqnjAbXMIzkFr93M3XEM6Owq0ZKVb4\" width=\"10%\" align=\"left\"><img src=\"https://media.licdn.com/dms/image/C5603AQHsjmEwetaDQA/profile-displayphoto-shrink_800_800/0?e=1542844800&v=beta&t=_W88AHw7R8H91FjH49qGVzfaF1tD2V3t5dD44ywPPH0\" width=\"10%\" align=\"left\"><img src=\"https://media.licdn.com/dms/image/C5603AQERz2htTAuRsA/profile-displayphoto-shrink_800_800/0?e=1542844800&v=beta&t=CxxKoiGNWi_7e-p5GxxuWMoY6SJqd4e24BHt8O1Vk9Y\" width=\"10%\" align=\"left\"><img src=\"https://media.licdn.com/dms/image/C5603AQGI3CoqK9Uv7Q/profile-displayphoto-shrink_800_800/0?e=1542844800&v=beta&t=_3t4rF6J2ER6O3WvsAdOfgcFBQqyxjBRSTe7tR8s-vQ\" width=\"10%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions about you\n",
    "* Python background?\n",
    "* Jupyter background?\n",
    "* Deep Learning background?\n",
    "* NLP background?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why sequences are important\n",
    "\n",
    "<img src=\"../images/why_sequences.png\" alt=\"why sequences\" width=\"75%\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Recurrent Neural Network (RNN)\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/rnn_inner_workings.png\" alt=\"inside rnn\" width=\"50%\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* The neural network is simply a tanh activation function\n",
    "* Shown unraveled is useful\n",
    "* Inputs outputs (hidden, x, and not showing some output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* x = input embedding for a word (vector)\n",
    "* h = hidden (or activation) state (vector)\n",
    "* tanh = hyperbolic tangent activation function/layer\n",
    "* y = output tag (not shown because we can have different schemes)\n",
    "\n",
    "**A Long Short-Term Memory (LSTM) network is a subclass of RNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What types are RNNs are there?\n",
    "\n",
    "* Many-to-many - e.g. find names with named entity recognition (NER)\n",
    "* Many-to-one - e.g. sentiment analysis\n",
    "* One-to-many - e.g. music generation\n",
    "* Another many-to-many - e.g. machine translation\n",
    "* One-to-one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/diff_types_rnns.png\" alt=\"why sequences\" width=\"75%\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Want to create a few tensors right now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x7fe66279d128>\n",
      "<MulBackward1 object at 0x7fe66279d0f0>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# First tensor calculation! # TODO a diagram\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# PyTorch takes care of gradient differentiation for you with something called \"autograd\"!\n",
    "# Makes backwards propagation super, duper simple (we don't have to worry about it!)\n",
    "# But you must tell the leafs that they require this tracking\n",
    "\n",
    "# Some random data (2D and 1D vectors)\n",
    "x = torch.randn(5, 5, requires_grad=True)\n",
    "b = torch.randn(5, requires_grad=True)\n",
    "\n",
    "# Just a one-element tensor\n",
    "w = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Do some multiplication\n",
    "y = w * x\n",
    "\n",
    "# Do some addition\n",
    "z = y + b\n",
    "\n",
    "# Let's trace back the operations\n",
    "print(z.grad_fn)\n",
    "print(y.grad_fn)\n",
    "print(x.grad_fn)\n",
    "\n",
    "# Why do you think x's auto gradient differentiation function is \"None\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "1.  [RNN video \"RNN1. Why sequence models?\"](https://www.youtube.com/watch?v=5Vl-bK7tfD8&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=1) by Andrew Ng\n",
    "2.  [Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works](https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec)\n",
    "3.  [Introduction to PyTorch fro pytorch.org](https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#sphx-glr-beginner-nlp-pytorch-tutorial-py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
