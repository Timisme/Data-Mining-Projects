{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etMAh1TAxxec",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Adding Problem\n",
    "\n",
    "### Mehrdad Yazdani\n",
    "### September 22, 2018\n",
    "\n",
    "Colab notebook online to play!!\n",
    "\n",
    "\n",
    "https://colab.research.google.com/drive/1TEDCmXo2ZqzExZZmpYKUdkkv0xwff0N5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The \"adding problem\" was original proposed by Schmidhuber and colleagues as an example of a sequential task that LSTM's are particularly well suited for: http://people.idsia.ch/~juergen/nipslstm/node4.html\n",
    "\n",
    "\n",
    ">![The Adding Problem](https://minpy.readthedocs.io/en/latest/_images/adding_problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As another example, the following sequence of length 5\n",
    "```\n",
    "{(0.443, 0), \n",
    " (0.112, 1), \n",
    " (0.950, 0), \n",
    " (0.839, 1), \n",
    " (0.142, 0)} \n",
    " ```\n",
    "\n",
    "yields 0.112 + 0.839 = 0.951 as the answer since the 2nd and 4th elmemts are added. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we will compare several different cells in PyTorch to see how well they solve the adding problem. The cells we consider are:\n",
    "\n",
    "- RNN\n",
    "- LSTM\n",
    "- RNN with identity initialization\n",
    "\n",
    "We will also consider a convolutional layer. Conv1D is not a recurrent layer, but has been shown to me useful for some sequential tasks. \n",
    "\n",
    "All methods will be compared using MSE on a held out test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "dqSmie_KEun4",
    "outputId": "0faf0fa7-69b7-49b5-a60c-77b97c0041c7",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: pip: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "squ9-CcuEwHF",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns;\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v49i2wJgJoZm",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k4-F5eWl6zI1",
    "outputId": "194e83a0-6819-4453-87fa-c1e7cc691657",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CPU!\n"
     ]
    }
   ],
   "source": [
    "# use CUDA or not\n",
    "use_cuda = False\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "  print(\"using cuda!\")\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  print(\"using CPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEuvUaYKynm5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data loading functions\n",
    "\n",
    "We will define some helper functions to generate our datasets. `generate_sequence` will genrate a single sequence whereas `get_set` returns multiple sequences (so a *dataset* of sequences).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CKmR1v-YGRHj",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sequence(seq_len = 10):\n",
    "  ''' generate sequences\n",
    "  \n",
    "  Args:\n",
    "  -----\n",
    "  seq_len : int (default 10)\n",
    "    The length of the sequence\n",
    "  \n",
    "  Returns:\n",
    "  --------\n",
    "  tuple of 3 numpy arrays x, z, y. x and z are 1D arrays and have same length\n",
    "  y is a float that is the target we want to predict (addition of x masked by z)\n",
    "  \n",
    "  Example:\n",
    "  --------\n",
    "  \n",
    "  >>> x_seq, z_seq, y_target = generate_sequence(seq_len = 100)\n",
    "  \n",
    "  '''\n",
    "  x = np.random.rand(seq_len)\n",
    "  z_p = np.arange(seq_len)\n",
    "  np.random.shuffle(z_p)\n",
    "  z = np.zeros(seq_len)\n",
    "  z[z_p[0]] = 1\n",
    "  z[z_p[1]] = 1\n",
    "  y = x[z_p[0]] + x[z_p[1]]\n",
    "  return x, z, y\n",
    "\n",
    "def get_set(num_examples = 100, seq_len = 10):\n",
    "  '''\n",
    "  Get the data set used for training/testing networks.\n",
    "  \n",
    "  Args:\n",
    "  -----\n",
    "  num_examples : int (default 100)\n",
    "    Number of sequences to generate\n",
    "  \n",
    "  seq_len : int (default 10)\n",
    "    The length of the sequence\n",
    "    \n",
    "  Returns:\n",
    "  --------\n",
    "  typle of length 2 where the first tuple is a numpy array of shape \n",
    "  num_examples x seq_len x 2 and the second tuple is length num_examples\n",
    "  \n",
    "  Example:\n",
    "  --------\n",
    "  \n",
    "  >>> X, y = get_set(num_examples=1000, seq_len = 50)\n",
    "  \n",
    "  '''\n",
    "  X_set, Z_set, y_set = [], [], []\n",
    "\n",
    "  for _ in range(num_examples):\n",
    "    x_example, z_example, y_example = generate_sequence(seq_len)\n",
    "    X_set.append(x_example)\n",
    "    Z_set.append(z_example)\n",
    "    y_set.append(y_example)\n",
    "    \n",
    "  X = np.zeros((num_examples,seq_len,2))\n",
    "  X[:,:,0] = np.array(X_set)\n",
    "  X[:,:,1] = np.array(Z_set)\n",
    "  return X, np.array(y_set)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1x7uKBP0ZNk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets see `get_set` in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0IANSgzLGTec",
    "outputId": "3c347a24-945d-4703-d705-6ba108823ee9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = get_set(num_examples=100, seq_len = 10)\n",
    "X_test, y_test = get_set(num_examples=100, seq_len = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ES3lsroR0jpu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So for the input we have a 3D array that has shape \"num examples\" x \"sequence length\" x \"num features.\"\n",
    "\n",
    "\n",
    "Note that the datasets that `get_set` returns are Numpy arrays, but PyTorch recquires PyTorch tensors. We could of course convert these Numpy arrays to PyTorch arrays, and then do some booking with indices to keep track of going through different batches when doing batch updates on the network.\n",
    "\n",
    "But that is tedious and PyTorch offers the Dataset class that we can inherit from to keep all this bookkeeping for us. Below we define the `SequenceDataset` generator class that will be used for all our data handilng for PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kl0sTbk6UhJ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "  \n",
    "  def __init__(self, num_examples, seq_len):\n",
    "    self.num_examples = num_examples\n",
    "    self.seq_len = seq_len\n",
    "    \n",
    "    X, y = get_set(num_examples=self.num_examples, seq_len = self.seq_len)\n",
    "    self.X = torch.from_numpy(X).float()\n",
    "    self.y = torch.from_numpy(y).float()\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "      self.X = self.X.to(device)\n",
    "      self.y = self.y.to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.num_examples\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZmrufss26CF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets create a training and test set with 100 examples for each and sequence lengths of 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPsIkWsTJxro",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_set = SequenceDataset(num_examples=100, seq_len = 10)\n",
    "test_set = SequenceDataset(num_examples=100, seq_len = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxjZMwEy3IaT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use PyTorch's `DataLoader` to specify the the batches of data to load for training. Note that each of the 100 example sequences are independent, so we also shuffle the order of the different sequences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-Rj0yQu3prw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset = train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle = True)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_set,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-khUeqXF2_g",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN\n",
    "\n",
    "We will start solving the Adding Problem with a simple RNN (the *Elman Network*). The network will update its internal hidden state for every element in the sequence until we reach the end. When we reach the end, we pass the final hidden state through a fully connected linear layer to predict the target. This type of architecture is sometimes called *many-to-one* since we are taking \"many\" elements (a sequence) to a single element (the target).\n",
    "\n",
    "<center>\n",
    "![Many to one](https://i.stack.imgur.com/QCnpU.jpg)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhkM836zGW82",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class RNNAdder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, input_size):    \n",
    "        super(RNNAdder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size \n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=self.input_size,\n",
    "                          hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state. The shape of the tensor is\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "        # since our input has the batch dimension in the first dim, \n",
    "        # we just use x.size(0)        \n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "          h_0 = h_0.to(device)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        _, h_f = self.rnn(x, h_0)\n",
    "        # we only care about the final hidden state. The intermediate values \n",
    "        # of the hidden state are discarded. We pass the final hidden state\n",
    "        # through the fully connected linear layer\n",
    "        return self.fc(h_f).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2ukZFPkJkNz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rnn_adder = RNNAdder(hidden_size = 12, input_size = 2)\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    rnn_adder = rnn_adder.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73HZrMRDKPz2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set loss and optimizer function\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn_adder.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "uONrEQwF93-r",
    "outputId": "8a96c460-fb96-4fb4-b27c-bf568d02a78c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.05227375403046608\n",
      "loss is 0.14001977443695068\n",
      "loss is 0.08981266617774963\n",
      "loss is 0.024839462712407112\n",
      "loss is 0.07050174474716187\n",
      "loss is 0.11493334174156189\n",
      "loss is 0.028457559645175934\n",
      "loss is 0.038266800343990326\n",
      "loss is 0.07356609404087067\n",
      "loss is 0.1033010482788086\n",
      "CPU times: user 1min 4s, sys: 398 ms, total: 1min 4s\n",
      "Wall time: 8.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (sequences, targets) in enumerate(train_loader):\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = rnn_adder(sequences)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "  if (epoch+1)%100 == 0:\n",
    "    print(\"loss is\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YOBl0zXCIUa0",
    "outputId": "22e5f618-369b-4718-f228-6ff90a08f127",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11353056877851486\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = rnn_adder(test_set.X)\n",
    "  test_mse = torch.mean((outputs - test_set.y)**2)\n",
    "print(test_mse.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BJIjhpEHwSW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM\n",
    "\n",
    "RNN's suffer from the vanishing gradient problem since creating the final hidden state is a result of updating the state through multiplications everytime a new element arrives in the sequence. LSTM's bypass this challenge by updating state additively. As a result, updaing gradients is much easier and longer memories can persist. Below is an `LSTMAdder` that is nearly identical to the `RNNAdder.`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mhr2amTvEshB",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMAdder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, input_size):    \n",
    "        super(LSTMAdder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size \n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                          hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "          h_0 = h_0.to(device)\n",
    "          c_0 = c_0.to(device)\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "        _, (h_f, c_f) = self.lstm(x, (h_0, c_0))\n",
    "        return self.fc(h_f).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zp2NuPyIKFBN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lstm_adder = LSTMAdder(hidden_size = 12, input_size = 2)\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    lstm_adder = lstm_adder.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ay85qAOeKJTT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set loss and optimizer function\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm_adder.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "sEr3G2M0KLXA",
    "outputId": "170a4a01-da3a-478b-e690-0f6bb6b1856d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 0.009904041886329651\n",
      "loss is 0.0013072346337139606\n",
      "loss is 0.00034542649518698454\n",
      "loss is 4.559542139759287e-05\n",
      "loss is 0.00023791706189513206\n",
      "loss is 0.0008242909098044038\n",
      "loss is 0.00016793828399386257\n",
      "loss is 0.00012301043898332864\n",
      "loss is 0.00021988825756125152\n",
      "loss is 0.00012232924927957356\n",
      "CPU times: user 1min 47s, sys: 965 ms, total: 1min 48s\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (sequences, targets) in enumerate(train_loader):\n",
    "    # forward pass\n",
    "    outputs = lstm_adder(sequences)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "  if (epoch+1)%100 == 0:\n",
    "    print(\"loss is\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VqCzc4weKPRL",
    "outputId": "2ff4234b-00d9-438e-c90a-86fd914d1846",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00038111911271698773\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = lstm_adder(test_set.X)\n",
    "  test_mse = torch.mean((outputs - test_set.y)**2)\n",
    "print(test_mse.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJMFpFxOiKja",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ReLU RNN\n",
    "\n",
    "The idea of the ReLU RNN is to initialize the hidden state of the RNN with the identity matrix and the bias with 0 and use the ReLU activation function. Below we demonstrate how such an RNN can be implemented. The results are not as good as the LSTM but certainly better than the traditional Elman Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQGhGZalh5y3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ReLURNNAdder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, input_size):    \n",
    "        super(ReLURNNAdder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size \n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=self.input_size,\n",
    "                          hidden_size=self.hidden_size, nonlinearity = \"relu\",\n",
    "                          batch_first=True)\n",
    "        \n",
    "        torch.nn.init.zeros_(self.rnn.weight_ih_l0)\n",
    "        torch.nn.init.eye_(self.rnn.weight_hh_l0)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state. The shape of the tensor is\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h_0 = Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "        # since our input has the batch dimension in the first dim, \n",
    "        # we just use x.size(0)        \n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "          h_0 = h_0.to(device)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        _, h_f = self.rnn(x, h_0)\n",
    "        # we only care about the final hidden state. The intermediate values \n",
    "        # of the hidden state are discarded. We pass the final hidden state\n",
    "        # through the fully connected linear layer\n",
    "        return self.fc(h_f).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEOdCuQEXn2K",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could train this model as before but if we want to be fair in our comparisons,  we should train each adder for each epoch for each batch. This can help us control the differences in training procedures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6qt1LU7jJZH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.init' has no attribute 'zeros_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-77e3d42882a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelu_rnn_adder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReLURNNAdder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrnn_adder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNAdder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlstm_adder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMAdder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-1fad4b553dda>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_size, input_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m                           batch_first=True)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih_l0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh_l0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.init' has no attribute 'zeros_'"
     ]
    }
   ],
   "source": [
    "relu_rnn_adder = ReLURNNAdder(hidden_size = 12, input_size = 2)\n",
    "rnn_adder = RNNAdder(hidden_size = 12, input_size = 2)\n",
    "lstm_adder = LSTMAdder(hidden_size = 12, input_size = 2)\n",
    "\n",
    "\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    relu_rnn_adder = relu_rnn_adder.cuda(device)\n",
    "    rnn_adder = rnn_adder.cuda(device)\n",
    "    lstm_adder = lstm_adder.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xov4xqRbjPNF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set loss and optimizer function\n",
    "criterion = torch.nn.MSELoss()\n",
    "relu_rnn_opt = torch.optim.Adam(relu_rnn_adder.parameters(), lr=0.01)\n",
    "rnn_opt = torch.optim.Adam(rnn_adder.parameters(), lr=0.01)\n",
    "lstm_opt = torch.optim.Adam(lstm_adder.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nw7s_T_0lH2F",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def update_model(model, optimizer, input_sequences, output_targets):\n",
    "  preds = model(input_sequences)\n",
    "  loss = criterion(preds, output_targets)\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "kmSGfpk6jSeq",
    "outputId": "c3e63ee4-2db7-4d35-f035-22739086434e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM loss:7.84e-03 RNN loss:1.03e-01 ReLURNN loss:3.34e-01\n",
      "LSTM loss:2.30e-04 RNN loss:5.75e-02 ReLURNN loss:4.10e-02\n",
      "LSTM loss:1.67e-04 RNN loss:4.40e-03 ReLURNN loss:2.56e-02\n",
      "LSTM loss:5.46e-04 RNN loss:3.14e-03 ReLURNN loss:1.14e-02\n",
      "LSTM loss:4.40e-04 RNN loss:4.15e-04 ReLURNN loss:7.64e-03\n",
      "LSTM loss:4.65e-04 RNN loss:2.17e-03 ReLURNN loss:1.93e-02\n",
      "LSTM loss:8.30e-05 RNN loss:5.40e-03 ReLURNN loss:1.21e-03\n",
      "LSTM loss:1.09e-04 RNN loss:5.69e-04 ReLURNN loss:5.51e-04\n",
      "LSTM loss:3.47e-04 RNN loss:1.71e-04 ReLURNN loss:2.05e-03\n",
      "LSTM loss:9.31e-05 RNN loss:1.15e-03 ReLURNN loss:3.66e-03\n",
      "LSTM loss:1.52e-04 RNN loss:7.90e-04 ReLURNN loss:5.76e-03\n",
      "LSTM loss:4.05e-04 RNN loss:1.50e-02 ReLURNN loss:6.46e-03\n",
      "LSTM loss:3.09e-05 RNN loss:3.69e-04 ReLURNN loss:5.80e-04\n",
      "LSTM loss:7.03e-06 RNN loss:5.70e-04 ReLURNN loss:1.01e-03\n",
      "LSTM loss:6.47e-05 RNN loss:9.25e-04 ReLURNN loss:1.42e-03\n",
      "LSTM loss:1.15e-04 RNN loss:7.27e-04 ReLURNN loss:1.14e-04\n",
      "LSTM loss:2.26e-04 RNN loss:4.90e-04 ReLURNN loss:1.54e-03\n",
      "LSTM loss:1.80e-04 RNN loss:3.39e-04 ReLURNN loss:1.08e-03\n",
      "LSTM loss:1.08e-04 RNN loss:1.53e-02 ReLURNN loss:5.13e-05\n",
      "LSTM loss:2.66e-05 RNN loss:5.79e-04 ReLURNN loss:1.93e-03\n",
      "LSTM loss:3.04e-05 RNN loss:4.25e-04 ReLURNN loss:4.29e-04\n",
      "LSTM loss:5.21e-05 RNN loss:6.47e-03 ReLURNN loss:2.42e-04\n",
      "LSTM loss:1.23e-04 RNN loss:8.23e-05 ReLURNN loss:4.18e-04\n",
      "LSTM loss:7.68e-06 RNN loss:2.25e-02 ReLURNN loss:2.57e-04\n",
      "LSTM loss:1.02e-04 RNN loss:2.55e-04 ReLURNN loss:6.06e-04\n",
      "LSTM loss:3.14e-05 RNN loss:1.03e-03 ReLURNN loss:1.38e-03\n",
      "LSTM loss:1.33e-04 RNN loss:2.55e-02 ReLURNN loss:1.67e-02\n",
      "LSTM loss:6.53e-04 RNN loss:5.44e-04 ReLURNN loss:1.91e-03\n",
      "LSTM loss:6.17e-05 RNN loss:1.63e-03 ReLURNN loss:2.69e-04\n",
      "LSTM loss:1.43e-05 RNN loss:2.66e-03 ReLURNN loss:3.32e-04\n",
      "LSTM loss:7.88e-05 RNN loss:2.31e-03 ReLURNN loss:1.64e-03\n",
      "LSTM loss:5.18e-05 RNN loss:2.33e-04 ReLURNN loss:1.90e-04\n",
      "LSTM loss:4.93e-06 RNN loss:3.90e-04 ReLURNN loss:2.35e-03\n",
      "LSTM loss:1.19e-05 RNN loss:8.29e-04 ReLURNN loss:2.94e-04\n",
      "LSTM loss:1.00e-04 RNN loss:1.75e-05 ReLURNN loss:8.80e-04\n",
      "LSTM loss:3.81e-06 RNN loss:1.02e-02 ReLURNN loss:1.51e-03\n",
      "LSTM loss:6.50e-06 RNN loss:9.91e-03 ReLURNN loss:2.16e-03\n",
      "LSTM loss:1.62e-04 RNN loss:2.76e-03 ReLURNN loss:1.80e-04\n",
      "LSTM loss:1.31e-05 RNN loss:1.22e-02 ReLURNN loss:4.16e-04\n",
      "LSTM loss:1.18e-05 RNN loss:2.53e-04 ReLURNN loss:1.26e-04\n",
      "LSTM loss:7.48e-05 RNN loss:9.47e-04 ReLURNN loss:2.68e-03\n",
      "LSTM loss:6.57e-06 RNN loss:2.76e-04 ReLURNN loss:3.83e-04\n",
      "LSTM loss:5.21e-04 RNN loss:3.06e-03 ReLURNN loss:1.06e-03\n",
      "LSTM loss:8.31e-05 RNN loss:8.71e-04 ReLURNN loss:2.36e-04\n",
      "LSTM loss:2.78e-05 RNN loss:2.48e-03 ReLURNN loss:4.67e-05\n",
      "LSTM loss:8.69e-07 RNN loss:5.68e-03 ReLURNN loss:1.54e-04\n",
      "LSTM loss:6.81e-05 RNN loss:1.20e-02 ReLURNN loss:4.38e-04\n",
      "LSTM loss:7.17e-07 RNN loss:1.79e-03 ReLURNN loss:1.26e-04\n",
      "LSTM loss:3.93e-06 RNN loss:8.80e-03 ReLURNN loss:5.18e-05\n",
      "LSTM loss:6.73e-06 RNN loss:3.38e-03 ReLURNN loss:1.30e-03\n",
      "CPU times: user 5min 6s, sys: 1min 25s, total: 6min 32s\n",
      "Wall time: 6min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lstm_losses = []\n",
    "rnn_losses = []\n",
    "relu_rnn_losses = []\n",
    "for epoch in range(5000):\n",
    "  for i, (sequences, targets) in enumerate(train_loader):\n",
    "    \n",
    "    loss = update_model(relu_rnn_adder, relu_rnn_opt, sequences, targets)\n",
    "    relu_rnn_losses.append(loss)\n",
    "    \n",
    "    loss = update_model(rnn_adder, rnn_opt, sequences, targets)\n",
    "    rnn_losses.append(loss)\n",
    "\n",
    "    loss = update_model(lstm_adder, lstm_opt, sequences, targets)\n",
    "    lstm_losses.append(loss)\n",
    "\n",
    "    \n",
    "  if (epoch+1)%100 == 0:\n",
    "    print(\"LSTM loss:{:.2e}\".format(lstm_losses[-1]) , \n",
    "          \"RNN loss:{:.2e}\".format(rnn_losses[-1]), \n",
    "          \"ReLURNN loss:{:.2e}\".format(relu_rnn_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4F_mZAZxjXT_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = relu_rnn_adder(test_set.X)\n",
    "  relu_rnn_mse = torch.mean((outputs - test_set.y)**2)\n",
    "\n",
    "  outputs = rnn_adder(test_set.X)\n",
    "  rnn_mse = torch.mean((outputs - test_set.y)**2)\n",
    "\n",
    "  outputs = lstm_adder(test_set.X)\n",
    "  lstm_mse = torch.mean((outputs - test_set.y)**2)\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oVlRUMyJjvjA",
    "outputId": "77d046f8-9b6b-45ed-ef11-ccd83703d7eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006150761619210243"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_mse.item(), rnn_mse.item(), relu_rnn_mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hZ3IfA-aLQxy",
    "outputId": "56ac66b5-d986-4112-f6b5-ebf8798e1480"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05440850928425789"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uXzjM82ALSLl",
    "outputId": "b80a3ef6-401c-4200-ef16-27364e3e6ca0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0014745582593604922"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QvV1FYzGYr91"
   },
   "source": [
    "While the LSTM is still the superior adder, the RNN initialized with the identity matrix and using the ReLU function is definitely better than the traidtional RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uy2C1wItLTl3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "The-Adding-Problem-PyTorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
