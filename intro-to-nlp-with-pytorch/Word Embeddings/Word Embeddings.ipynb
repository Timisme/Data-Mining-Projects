{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Embeddings\n",
    "Based on Word Embeddings tutorial by Robert Guthrie  https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Word Embeddings are dense vectors representations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Word embeddings compress information so you have a more dense representation. Compare this to sparse vectors like One-Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the quick brown fox jumped over the lazy dog\"\n",
    "words = sentence.split(' ')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the individual words in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'dog', 'brown', 'over', 'the', 'jumped', 'lazy', 'fox']\n"
     ]
    }
   ],
   "source": [
    "vocab1 = list(set(words))\n",
    "print(vocab1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of words in our vocabulary\n",
    "len(vocab1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The vocabulary size is seen above. Now we can one-hot encode the vocabulary words. The good news is that PyTorch (As of December 2018) now has a built-in one-hot encoding module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quick': 0, 'dog': 1, 'brown': 2, 'over': 3, 'the': 4, 'jumped': 5, 'lazy': 6, 'fox': 7}\n"
     ]
    }
   ],
   "source": [
    "# Convert words to indexes\n",
    "word_to_ix1 = {word: i for i, word in enumerate(vocab1)}\n",
    "print(word_to_ix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'dog', 'brown', 'over', 'the', 'jumped', 'lazy', 'fox']\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "words = torch.tensor([word_to_ix1[w] for w in vocab1], dtype=torch.long)\n",
    "\n",
    "one_hot_encoding = one_hot(words)\n",
    "print(vocab1)\n",
    "print(one_hot_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The issue with sparse one-hot encoding is that the vectors are very large \n",
    "and we have a very sparse representation of the vectors. As you can see there\n",
    "are a lot of zeros. For example, the popular data set WikiText-103 has 267,000\n",
    "words in the vocabulary. This means around 267,000 zeros in each vector with\n",
    "one-hot encoding.\n",
    "\n",
    "We should try to find a smaller encoding for our dataset. Let's try a denser vector using a\n",
    "Word Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Embedding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Context is the number of words we are using as a context for the next word we want to predict\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "# Embedding dimension is the size of the embedding vector\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# Size of the hidden layer\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"Tomorrow, and tomorrow, and tomorrow,\n",
    "Creeps in this petty pace from day to day,\n",
    "To the last syllable of recorded time;\n",
    "And all our yesterdays have lighted fools\n",
    "The way to dusty death. Out, out, brief candle!\n",
    "Life's but a walking shadow, a poor player,\n",
    "That struts and frets his hour upon the stage,\n",
    "And then is heard no more. It is a tale\n",
    "Told by an idiot, full of sound and fury,\n",
    "Signifying nothing.\n",
    "\"\"\".lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['tomorrow,', 'and'], 'tomorrow,'), (['and', 'tomorrow,'], 'and'), (['tomorrow,', 'and'], 'tomorrow,')]\n"
     ]
    }
   ],
   "source": [
    "# Build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab2 = list(set(test_sentence))\n",
    "word_to_ix2 = {word: i for i, word in enumerate(vocab2)}\n",
    "\n",
    "# Show what a trigram looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# N-Gram Language Model\n",
    "\n",
    "An N-Gram is a sequence of words as in a sentence. This is useful because it gives us some context to train a deep learning classifier. \n",
    "\n",
    "For a detailed post visit: https://www.microsoft.com/developerblog/2015/11/29/feature-representation-for-text-analyses/\n",
    "\n",
    "Here's what a diagram of our n-gram deep learning model would look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"../images/network_next_word.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ReLU\n",
    "Rectifier activation function: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "<img src=\"https://miro.medium.com/max/357/1*oePAhrm74RNnNEolprmTaQ.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Softmax function\n",
    "<a href=\"https://www.researchgate.net/figure/Softmax-function-image_fig1_325856086\"><img src=\"https://www.researchgate.net/profile/Shen_Leixian/publication/325856086/figure/fig1/AS:723221292789765@1549440801787/Softmax-function-image.png\" alt=\"Softmax function image\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training is based on preceding words\n",
    "Predict the probability of a word based on the words around it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Add imports here\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, HIDDEN_DIM)\n",
    "        self.linear2 = nn.Linear(HIDDEN_DIM, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()  # negative log likelihood\n",
    "model = NGramLanguageModeler(len(vocab2), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 432.11it/s, loss=3.6] \n",
      "100%|██████████| 73/73 [00:00<00:00, 507.88it/s, loss=3.57]\n",
      "100%|██████████| 73/73 [00:00<00:00, 508.73it/s, loss=3.54]\n",
      "100%|██████████| 73/73 [00:00<00:00, 497.76it/s, loss=3.51]\n",
      "100%|██████████| 73/73 [00:00<00:00, 481.95it/s, loss=3.49]\n",
      "100%|██████████| 73/73 [00:00<00:00, 534.10it/s, loss=3.46]\n",
      "100%|██████████| 73/73 [00:00<00:00, 475.73it/s, loss=3.43]\n",
      "100%|██████████| 73/73 [00:00<00:00, 503.46it/s, loss=3.4] \n",
      "100%|██████████| 73/73 [00:00<00:00, 480.42it/s, loss=3.37]\n",
      "100%|██████████| 73/73 [00:00<00:00, 509.81it/s, loss=3.34]\n",
      "100%|██████████| 73/73 [00:00<00:00, 489.63it/s, loss=3.31]\n",
      "100%|██████████| 73/73 [00:00<00:00, 497.34it/s, loss=3.28]\n",
      "100%|██████████| 73/73 [00:00<00:00, 509.76it/s, loss=3.25]\n",
      "100%|██████████| 73/73 [00:00<00:00, 510.08it/s, loss=3.22]\n",
      "100%|██████████| 73/73 [00:00<00:00, 521.02it/s, loss=3.19]\n",
      "100%|██████████| 73/73 [00:00<00:00, 507.04it/s, loss=3.16]\n",
      "100%|██████████| 73/73 [00:00<00:00, 507.96it/s, loss=3.13]\n",
      "100%|██████████| 73/73 [00:00<00:00, 526.83it/s, loss=3.1] \n",
      "100%|██████████| 73/73 [00:00<00:00, 521.02it/s, loss=3.07]\n",
      "100%|██████████| 73/73 [00:00<00:00, 515.49it/s, loss=3.04]\n",
      "100%|██████████| 73/73 [00:00<00:00, 529.46it/s, loss=3]   \n",
      "100%|██████████| 73/73 [00:00<00:00, 528.34it/s, loss=2.97]\n",
      "100%|██████████| 73/73 [00:00<00:00, 546.78it/s, loss=2.94]\n",
      "100%|██████████| 73/73 [00:00<00:00, 511.11it/s, loss=2.91]\n",
      "100%|██████████| 73/73 [00:00<00:00, 531.72it/s, loss=2.88]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(25):\n",
    "    total_loss = 0\n",
    "\n",
    "    iterator = tqdm(trigrams)\n",
    "    for context, target in iterator:\n",
    "        # (['When', 'forty'], 'winters')\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix2[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        iterator.set_postfix(loss=float(loss))\n",
    "    losses.append(total_loss)\n",
    "    # add progress bar with epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGramLanguageModeler(\n",
       "  (embeddings): Embedding(59, 10)\n",
       "  (linear1): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=59, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the structure of our model here\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.2813, -4.3018, -4.1416, -3.9168, -4.2167, -3.9121, -3.8985, -4.1464,\n",
      "         -4.3967, -3.6064, -3.8017, -3.9178, -4.3452, -4.3658, -4.1338, -3.7384,\n",
      "         -3.9032, -4.4974, -4.2858, -4.3245, -4.1646, -4.4215, -3.3084, -4.0751,\n",
      "         -4.1101, -4.3610, -4.2177, -4.1962, -4.4140, -4.0597, -3.0100, -3.8150,\n",
      "         -4.3852, -4.2511, -4.0806, -4.0710, -4.0321, -4.4454, -4.2866, -4.3894,\n",
      "         -3.9729, -3.6467, -3.8749, -4.4524, -4.0009, -4.0418, -4.3280, -3.9617,\n",
      "         -4.4341, -4.4680, -3.9659, -4.5848, -4.1615, -4.1848, -4.3031, -4.0144,\n",
      "         -4.1116, -4.1997, -4.4411]])\n",
      "tomorrow,\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "with torch.no_grad():\n",
    "    context = ['tomorrow,', 'and']\n",
    "    context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)\n",
    "    pred = model(context_idxs)\n",
    "    print(pred)\n",
    "    index_of_prediction = numpy.argmax(pred)\n",
    "    print(vocab2[index_of_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next Steps\n",
    "* RNN/LSTM/BiLSTM\n",
    "* Pointer to GloVe word embedding: https://nlp.stanford.edu/projects/glove/\n",
    "* https://github.com/fastai/word-embeddings-workshop/blob/master/Word%20Embeddings.ipynb\n",
    "* ELMo: https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise: Continuous Bag of Words\n",
    "Continuous Bag of Words is a model that tries to predict a word based on a few word before and after the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab3 = list(set(raw_text))\n",
    "vocab_size = len(vocab3)\n",
    "\n",
    "word_to_ix3 = {word: i for i, word in enumerate(vocab3)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([44,  4, 12, 42])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "def make_context_vector(context, word_to_ix3):\n",
    "    idxs = [word_to_ix3[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix3)  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Glossary\n",
    "\n",
    "* word embedding -- a dense vector representation of words\n",
    "* one-hot encoding -- a sparse vector representation of words with ones\n",
    "* vocabulary -- the set of words used in your language \n",
    "* tokenization -- the process of breaking down bodies of text into words\n",
    "* ReLU function -- a positive activation function for neural networks\n",
    "* softmax function -- activation function used to map probability distribution\n",
    "* negative log likelihood -- a probability function used in conjunction with softmax\n",
    "* loss function -- also known as a \"cost function\" a function to estimate the cost associated with an event\n",
    "* Stochastic Gradient Descent (SGD) -- \"an iterative method for optimizing an objective function\" * (https://en.wikipedia.org/wiki/Stochastic_gradient_descent) \n",
    "* learning rate -- a constant step to take in one iteration of stochastic gradient descnet\n",
    "* autograd -- PyTorch's automatic differentiation class that performs the backpropagation gradient calculations automatically so that a \"backward\" class does not need to be defined by the programmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "References: \n",
    "* https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "* https://github.com/fastai/word-embeddings-workshop/blob/master/Word%20Embeddings.ipynb"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
