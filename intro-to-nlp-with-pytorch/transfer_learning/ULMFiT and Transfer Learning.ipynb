{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Transfer Learning\n",
    "While one of the biggest advancements in deep learning for image processing has been transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Transfer Learning?\n",
    "\n",
    "Take a neural network.\n",
    "Train it and export the model.\n",
    "\n",
    "Import part or all of the model architecture into a new model!\n",
    "Use parts of it to build a new model!\n",
    "\n",
    "ImageNet Models:\n",
    "\n",
    "VGGNet\n",
    "ResNet\n",
    "Inception\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Transfer Learning?\n",
    "\n",
    "* Save training time 1 Month --> 1 Day\n",
    "* Reuse models\n",
    "* Train on a smaller data set for a specific task\n",
    "* Don't need to reinvent the wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Embeddings\n",
    "We previously discussed Word Embeddings. In the past, people have performed transfer learning with just the first Embedding layer. \n",
    "\n",
    "One common one is:\n",
    "\n",
    "GloVe: Global Vectors for Word Representation\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "However, beyond the embedding layer, there have not been many successful shared examples of Transfer Learning in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ULMFiT\n",
    "Jeremy Howard and Sebastian Ruder from Fast.AI\n",
    "\n",
    "## Why ULMFiT\n",
    "* Faster time retraining for classification task\n",
    "* Better performance\n",
    "* Smaller amount of labeled training data required\n",
    "\n",
    "## Blog Introduction\n",
    "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html\n",
    "\n",
    "## Fast.ai text library -- like PyTorch but with \n",
    "https://github.com/fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Paper\n",
    "\n",
    "https://arxiv.org/abs/1801.06146\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 1: General Language Model Pre-training\n",
    "First they train the model on a large corpus such as the Wikitext-103 data set.\n",
    "\n",
    "The Language model is fairly straightforward and similar to what David had shown:\n",
    "* Embedding Layer + LSTM and Linear\n",
    "\n",
    "They have two main classes:\n",
    "* RNN_Encoder (PyTorch nn.Module)\n",
    "* Learner \n",
    "** Has a model as an attribute\n",
    "** Has helper methods to work on the model (training, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Note: This is just for demo:\n",
    "\n",
    "class RNN_Encoder(nn.Module):\n",
    "\n",
    "    \"\"\"A custom RNN encoder network that uses\n",
    "        - an embedding matrix to encode input,\n",
    "        - a stack of LSTM or QRNN layers to drive the network, and\n",
    "        - variational dropouts in the embedding and LSTM/QRNN layers\n",
    "\n",
    "        The architecture for this network was inspired by the work done in\n",
    "        \"Regularizing and Optimizing LSTM Language Models\".\n",
    "        (https://arxiv.org/pdf/1708.02182.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, ntoken, emb_sz, n_hid, n_layers, pad_token, bidir=False,\n",
    "                 dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5, qrnn=False):\n",
    "        \"\"\" Default constructor for the RNN_Encoder class\n",
    "\n",
    "            Args:\n",
    "                bs (int): batch size of input data\n",
    "                ntoken (int): number of vocabulary (or tokens) in the source dataset\n",
    "                emb_sz (int): the embedding size to use to encode each token\n",
    "                n_hid (int): number of hidden activation per LSTM layer\n",
    "                n_layers (int): number of LSTM layers to use in the architecture\n",
    "                pad_token (int): the int value used for padding text.\n",
    "                dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "                dropouti (float): dropout to apply to the input layer.\n",
    "                dropoute (float): dropout to apply to the embedding layer.\n",
    "                wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "          \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.ndir = 2 if bidir else 1\n",
    "        self.bs, self.qrnn = 1, qrnn\n",
    "        self.encoder = nn.Embedding(ntoken, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_with_dropout = EmbeddingDropout(self.encoder)\n",
    "        if self.qrnn:\n",
    "            #Using QRNN requires cupy: https://github.com/cupy/cupy\n",
    "            from .torchqrnn.qrnn import QRNNLayer\n",
    "            self.rnns = [QRNNLayer(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) for l in range(n_layers)]\n",
    "            if wdrop:\n",
    "                for rnn in self.rnns:\n",
    "                    rnn.linear = WeightDrop(rnn.linear, wdrop, weights=['weight'])\n",
    "        else:\n",
    "            self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                1, bidirectional=bidir) for l in range(n_layers)]\n",
    "            if wdrop: self.rnns = [WeightDrop(rnn, wdrop) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "\n",
    "        self.emb_sz,self.n_hid,self.n_layers,self.dropoute = emb_sz,n_hid,n_layers,dropoute\n",
    "        self.dropouti = LockedDropout(dropouti)\n",
    "        self.dropouths = nn.ModuleList([LockedDropout(dropouth) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\" Invoked during the forward propagation of the RNN_Encoder module.\n",
    "        Args:\n",
    "            input (Tensor): input of shape (sentence length x batch_size)\n",
    "\n",
    "        Returns:\n",
    "            raw_outputs (tuple(list (Tensor), list(Tensor)): list of tensors evaluated from each RNN layer without using\n",
    "            dropouth, list of tensors evaluated from each RNN layer using dropouth,\n",
    "        \"\"\"\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        with set_grad_enabled(self.training):\n",
    "            emb = self.encoder_with_dropout(input, dropout=self.dropoute if self.training else 0)\n",
    "            emb = self.dropouti(emb)\n",
    "            raw_output = emb\n",
    "            new_hidden,raw_outputs,outputs = [],[],[]\n",
    "            for l, (rnn,drop) in enumerate(zip(self.rnns, self.dropouths)):\n",
    "                current_input = raw_output\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "                new_hidden.append(new_h)\n",
    "                raw_outputs.append(raw_output)\n",
    "                if l != self.n_layers - 1: raw_output = drop(raw_output)\n",
    "                outputs.append(raw_output)\n",
    "\n",
    "            self.hidden = repackage_var(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def one_hidden(self, l):\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        if IS_TORCH_04: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_())\n",
    "        else: return Variable(self.weights.new(self.ndir, self.bs, nh).zero_(), volatile=not self.training)\n",
    "\n",
    "    def reset(self):\n",
    "        if self.qrnn: [r.reset() for r in self.rnns]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self.one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.n_layers)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 2: Language Model Fine Tuning\n",
    "Then they fine-tune the language model on the target data set, so they can get the characteristics of the data set \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 3: Classifier Fine-Tuning\n",
    "\n",
    "In this part, they:\n",
    "1. Add classification task to model\n",
    "2. Unfreeze last layer and retrain\n",
    "3. Gradually unfreeze other layers and retrain\n",
    "\n",
    "https://github.com/fastai/fastai/blob/master/courses/dl2/imdb_scripts/train_clas.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# This is how Fast.ai freezes parameters and layers in \n",
    "\n",
    "def set_trainable_attr(m,b):\n",
    "    m.trainable=b\n",
    "    for p in m.parameters(): p.requires_grad=b\n",
    "\n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class PoolingLinearClassifier(nn.Module):\n",
    "    def __init__(self, layers, drops):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            LinearBlock(layers[i], layers[i + 1], drops[i]) for i in range(len(layers) - 1)])\n",
    "\n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(1,2,0), (1,)).view(bs,-1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = outputs[-1]\n",
    "        sl,bs,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        mxpool = self.pool(output, bs, True)\n",
    "        x = torch.cat([output[-1], mxpool, avgpool], 1)\n",
    "        for l in self.layers:\n",
    "            l_x = l(x)\n",
    "            x = F.relu(l_x)\n",
    "        return l_x, raw_outputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    ...\n",
    "    def freeze_to(self, n):\n",
    "        c=self.get_layer_groups()\n",
    "        for l in c:     set_trainable(l, False)\n",
    "        for l in c[n:]: set_trainable(l, True)\n",
    "\n",
    "    def freeze_all_but(self, n):\n",
    "        c=self.get_layer_groups()\n",
    "        for l in c: set_trainable(l, False)\n",
    "        set_trainable(c[n], True)\n",
    "        \n",
    "    def freeze_groups(self, groups):\n",
    "        c = self.get_layer_groups()\n",
    "        self.unfreeze()\n",
    "        for g in groups:\n",
    "            set_trainable(c[g], False)\n",
    "            \n",
    "    def unfreeze_groups(self, groups):\n",
    "        c = self.get_layer_groups()\n",
    "        for g in groups:\n",
    "            set_trainable(c[g], True)\n",
    "\n",
    "    def unfreeze(self): self.freeze_to(0)\n",
    "        \n",
    "        \n",
    "    def fit(self, lrs, n_cycle, wds=None, **kwargs):\n",
    "\n",
    "        \"\"\"Method gets an instance of LayerOptimizer and delegates to self.fit_gen(..)\n",
    "\n",
    "        Note that one can specify a list of learning rates which, when appropriately\n",
    "        defined, will be applied to different segments of an architecture. This seems\n",
    "        mostly relevant to ImageNet-trained models, where we want to alter the layers\n",
    "        closest to the images by much smaller amounts.\n",
    "\n",
    "        Likewise, a single or list of weight decay parameters can be specified, which\n",
    "        if appropriate for a model, will apply variable weight decay parameters to\n",
    "        different segments of the model.\n",
    "\n",
    "        Args:\n",
    "            lrs (float or list(float)): learning rate for the model\n",
    "\n",
    "            n_cycle (int): number of cycles (or iterations) to fit the model for\n",
    "\n",
    "            wds (float or list(float)): weight decay parameter(s).\n",
    "\n",
    "            kwargs: other arguments\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.sched = None\n",
    "        layer_opt = self.get_layer_opt(lrs, wds)\n",
    "        return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
