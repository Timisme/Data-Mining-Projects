{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/arclight/notebooks/workshop/intro-to-nlp-with-pytorch/Sarcasm_Detection/algorithm\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Entropy Algorithm\n",
    "\n",
    "The Max Entropy classifier is a probabilistic classifier which belongs to the class of exponential models. \n",
    "\n",
    "The Max Entropy does not assume that the features are conditionally independent of each other. \n",
    "\n",
    "The MaxEnt is based on the Principle of Maximum Entropy and from all the models that fit our training data, selects the one which has the largest entropy. The Max Entropy classifier can be used to solve a large variety of text classification problems such as language detection, topic classification, sentiment analysis and more.\n",
    "\n",
    "![img](./dogsvsfriedchicken.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let a feature function, f_i(x), take in an input, x, and return either 0 or 1, depending if the feature is present in x:\n",
    "\n",
    "f(x) = \\begin{cases} 1, & \\quad \\mbox{if the feature is present in } x\\\\ 0, & \\quad \\mbox{otherwise}\\\\ \\end{cases} \n",
    "\n",
    "Furthermore, for N features, associate each feature function f_i(x) with a weight w_i(d), which is a number that denotes how “important” f_i(x) is compared to other features for a decision, d (In this case, spam or not spam).\n",
    "\n",
    "We can “model” (in my opinion, this word could be understood as “estimate”) the score of a decision d on input x using the following procedure:\n",
    "\n",
    "- For each f_i(x) in a set of N features, determine if f_i(x) should be 1 or 0\n",
    "- Multiply each f_i(x) with the associated weight w_i(d), which depends on the decision d being evaluated.\n",
    "- Add up all of the weight*feature pairs: sum_d = \\sum_{i=1}^{N} w_i(d)*f_i(x)\n",
    "- Throw the sum up into an exponent: numerator_d = \\exp(sum_d) \n",
    "- Divide the sum by a number that will range the score between 0 and 1, and such that the sum of scores across all decisions is 1. It turns out that this is the sum of the numerators for every possible decision d: denominator = \\sum_{d} \\exp(\\sum_{i=1}^{N} w_i(d)*f_i(x))\n",
    "- The procedure above is pretty much the equation below:\n",
    "\n",
    "![img](./maxentequation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "## Install the MegaM library\n",
    "\n",
    "- Make sure the Punkt Sentence Tokenizer is installed\n",
    "   - nltk.download('punkt')\n",
    "- Install MegaM library used by NLTK for Max Entropy algorithm\n",
    "   - wget http://caml.inria.fr/pub/distrib/ocaml-4.02/ocaml-4.02.1.tar.gz\n",
    "   - tar -zxvf ocaml-4.02.1.tar.gz\n",
    "   - ./configure\n",
    "   - make world.opt\n",
    "   - sudo make install\n",
    "   - wget http://hal3.name/megam/megam_src.tgz\n",
    "   - tar -zxvf megam_src.tgz\n",
    "   - cd megam_0.92\n",
    "   - Run `ocamlc -where` and note down the path\n",
    "   - Edit the Makefile 74 line\n",
    "     - #WITHCLIBS =-I /usr/lib/ocaml/caml\n",
    "     - WITHCLIBS =-I /usr/local/lib/ocaml/caml\n",
    "   - Edit the Makefile again, change the 62 line -lstr to -lcamlstr\n",
    "     - #WITHSTR =str.cma -cclib -lstr\n",
    "     - WITHSTR =str.cma -cclib -lcamlstr\n",
    "   - Run `make`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure,\n",
    "                                          log_likelihood, approxrand)\n",
    "from nltk import precision\n",
    "import random\n",
    "from nltk import classify\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.classify.megam import call_megam, write_megam_file, parse_megam_weights\n",
    "from nltk.corpus import names\n",
    "import collections,re\n",
    "import csv\n",
    "import json,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/arclight/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "train_data = \"train_set_v2.json\"\n",
    "test_data = \"test_set_v2.json\"\n",
    "\n",
    "nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#os.environ[\"MEGAM\"] = '/usr/local/Cellar/megam/0.9.2/bin/megam'\n",
    "\n",
    "all_features = [\"words\",\"length\",\"pos\",\"interjection\",\"question\"]\n",
    "metrics = {}\n",
    "def feature_set_generator(text,length,label, include_list):\n",
    "    features = {}\n",
    "    words = text.split()\n",
    "\n",
    "    if not include_list:\n",
    "        include_list = all_features\n",
    "\n",
    "    # Bag of words\n",
    "    if(\"words\" in include_list):\n",
    "        features[\"words\"] = tuple((word,True) for word in words)\n",
    "\n",
    "    # Length\n",
    "    if(\"length\" in include_list):\n",
    "        features[\"length\"] = length\n",
    "\n",
    "    # Part of speech tagging\n",
    "    pos = nltk.word_tokenize(text)\n",
    "    if(\"pos\" in include_list):\n",
    "        set_of_pos_tags = nltk.pos_tag(pos)\n",
    "        features[\"pos\"] = tuple(t for t in set_of_pos_tags)\n",
    "\n",
    "\n",
    "    # Interjections - SUBSTANTIAL INCREASE IN ACCURACY\n",
    "    if(\"interjection\" in include_list):\n",
    "        set_of_pos_tags = nltk.pos_tag(pos)\n",
    "        interjection_tags = 0\n",
    "        for tag in set_of_pos_tags:\n",
    "            if tag == \"UH\":\n",
    "                interjection_tags += 1\n",
    "        features[\"interjection\"] = interjection_tags\n",
    "\n",
    "    if(\"question\" in include_list):\n",
    "        question_count = 0\n",
    "        for text in words:\n",
    "            if \"?\" in text:\n",
    "                question_count += 1\n",
    "        features[\"question\"] = question_count\n",
    "\n",
    "    return features\n",
    "\n",
    "def me_classifier(exclude_list):\n",
    "    me_classifier = 0\n",
    "\n",
    "    with open(train_data, 'r',encoding='utf-8', errors='ignore') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        feature_set = [(feature_set_generator(text,length,label,exclude_list),label) for text,length,label in reader]\n",
    "        #print(feature_set)\n",
    "        me_classifier = MaxentClassifier.train(feature_set,\"megam\")\n",
    "\n",
    "    accuracy = 0.0\n",
    "    with open(test_data,'r',encoding='utf-8', errors='ignore') as testcsvfile:\n",
    "        test_reader = csv.reader(testcsvfile)\n",
    "        test_feature_set = [(feature_set_generator(text,length,label,exclude_list),label) for text,length,label in test_reader]\n",
    "        accuracy = classify.accuracy(me_classifier, test_feature_set)\n",
    "\n",
    "    classified = collections.defaultdict(set)\n",
    "    observed = collections.defaultdict(set)\n",
    "    i=1\n",
    "    with open(test_data,'r',encoding='utf-8', errors='ignore') as testcsvfile:\n",
    "        test_reader = csv.reader(testcsvfile)\n",
    "        for text,length,label in test_reader:\n",
    "            observed[label].add(i)\n",
    "            classified[me_classifier.classify(feature_set_generator(text,length,label,exclude_list))].add(i)\n",
    "            i+=1\n",
    "\n",
    "    return accuracy,precision(observed[\"1\"], classified[\"1\"]),recall(observed['1'], classified['1']),\\\n",
    "           f_measure(observed['1'], classified['1']),precision(observed['0'], classified['0']),recall(observed['1'], classified['0']),f_measure(observed['1'], classified['0'])\n",
    "\n",
    "\n",
    "def print_stats(a,ps,rs,fs,pns,rns,fns):\n",
    "    print()\n",
    "    print(\"****************** MAX ENTROPY STATISTICS******************************\")\n",
    "    print('Accuracy:', a)\n",
    "    print('Sarcasm precision:', ps)\n",
    "    print('Sarcasm recall:', rs)\n",
    "    print('Sarcasm F-measure:', fs)\n",
    "    print('Not Sarcasm precision:',pns)\n",
    "    print('Not Sarcasm recall:', rns)\n",
    "    print('Not Sarcasm F-measure:', fns)\n",
    "    print(\"***********************************************************************\")\n",
    "\n",
    "\n",
    "def prepare_dict(dict,a,ps,rs,fs,pns,rns,fns):\n",
    "    dict = {}\n",
    "    dict[\"title\"] = \"Maximum Entropy with all features\"\n",
    "    dict[\"accuracy\"] = a\n",
    "    dict[\"sarcasm_precision\"] = ps\n",
    "    dict[\"sarcasm_recall\"] = rs\n",
    "    dict[\"sarcasm_f_measure\"] = fs\n",
    "    dict[\"not_sarcasm_precision\"] = pns\n",
    "    dict[\"not_sarcasm_recall\"] = rns\n",
    "    dict[\"not_sarcasm_f_measure\"] = fns\n",
    "    return dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************** MAX ENTROPY STATISTICS******************************\n",
      "Accuracy: 0.6024705221785513\n",
      "Sarcasm precision: 0.6297335203366059\n",
      "Sarcasm recall: 0.19445647466435687\n",
      "Sarcasm F-measure: 0.2971542025148908\n",
      "Not Sarcasm precision: 0.5982721382289417\n",
      "Not Sarcasm recall: 0.8055435253356431\n",
      "Not Sarcasm F-measure: 0.5361003026372676\n",
      "***********************************************************************\n"
     ]
    }
   ],
   "source": [
    "a,ps,rs,fs,pns,rns,fns = me_classifier([])\n",
    "max_ent_with_all_features = {}\n",
    "metrics[\"max_ent_with_all_features\"]=prepare_dict(max_ent_with_all_features,a,ps,rs,fs,pns,rns,fns)\n",
    "print_stats(a,ps,rs,fs,pns,rns,fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 \n",
    "\n",
    "## Try MaxEnt Classifier with just Parts of Speech words and inspect the metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 \n",
    "\n",
    "## Try MaxEnt Classifier with only interjection and inspect the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 \n",
    "\n",
    "## Inspect data and note down what could improve accuracy. Do sarcastic sentences have a \"?\" character often or are they phrased as a question? Rhetorical questions often resemble sarcastic sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
